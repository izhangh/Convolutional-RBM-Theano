\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}

\begin{document}
\section{Convolutional restricted Boltzmann machine}
In this section, we introduce a convolutional restricted
Boltzmann machine for modeling the DNA sequence my means
of a collection of motifs.\par
We introduce a set of DNA sequences $\mathbf{V}=\{\mathbf{v}^1,\cdots,\mathbf{v}^S\}$, where
$S$ denotes the number of sequences.
 $\mathbf{v}^i$ denotes a the $i^{th}$ sequence, which is represented by a $4\times N$ matrix
according to an one-hot representation with $N$ being the length of the sequence.
In the one-hot representation the nucleotides 'A', 'C', 'G' and 'T' are given
by the vectors $[1,0,0,0]$, $[0,1,0,0]$, $[0,0,1,0]$ and  $[0,0,0,1]$, respectively.
We assume that our model is composed of $K$ motifs, each of
which of length $M$.
Additionally, we introduce a set of hidden activation
maps, one per sequence  and strand, denoted by 
$\mathbf{H}=\{\mathbf{H}^{1}, \cdots, \mathbf{H}^{S}\}$ 
and ${\mathbf{H}'}=\{\mathbf{H'}^{1}, \cdots, \mathbf{H'}^{S}\}$, 
 (the apostrophe denotes the reverse strand hidden activations).
Each $\mathbf{H}^s$ and ${\mathbf{H}'}^s$ corrsponds to
a binary matrix of dimension $(N-M+1)\times K$.
The hidden activation maps $\mathbf{H}$ and $\mathbf{H}'$ indicate the locations of where 
in the sequence motif hits were obtained.
Together, $\mathbf{V}$, $\mathbf{H}$ and $\mathbf{H}'$ comprise the random variables of our model.\par
Next, we introduce the parameters of the model to be the set of motifs $\mathbf{W}$,
the bias associated with the motifs $\mathbf{b}=b_1\cdots b_K$ and the bias associated with the sequence $\mathbf{c}=[c_A,c_C,c_G,c_T]^\top$.
We denote the collection of motifs $\mathbf{W}=\{\mathbf{W}^1,\cdots, \mathbf{W}^K\}$, where each $\mathbf{W}^i$
represents a $4 \times M$ matrix for all $i$.
Each motif can be thought of a position-specific score matrix (a concept which is widely 
used in the bioinformatics community for modelling  transcription factor binding sites).
The convolutional restricted Boltzmann machine is defined by
\begin{align}
	E(\mathbf{V},\mathbf{H}, {\mathbf{H}'})=
	-\sum_{s=1}^S\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} 
	  {\mathbf{v}^s}_{n+m}^\top W_{m+1}^k \mathbf{H}_{nk}^s \nonumber\\
	-\sum_{s=1}^S\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} 
	  {\mathbf{v}^s}_{n+m}^\top C(W_{M-m}^k) {\mathbf{H}'}_{nk}^s\nonumber\\
	- \sum_{k=1}^K b_k \sum_{s=1}^S\sum_{n=1}^{N-M+1}(\mathbf{H}_{nk}^s + {\mathbf{H}'}_{nk}^s)
	-c^\top \sum_{s=1}^S\sum_{n=1}^{N}\mathbf{v}^s_n
\end{align}
 Additionally, the function $C(.)$ denotes complementary mapping
as follows: $A\rightarrow T$, 
$C\rightarrow G$, $G\rightarrow C$ and $T\rightarrow A$.
The associated probability to a state $(\mathbf{V},\mathbf{H})$ is then given
by
\begin{align}
	P(\mathbf{V},\mathbf{H}, \mathbf{H}')=\frac{e^{-E(\mathbf{V},\mathbf{H},\mathbf{H}')}}
	{\sum_{\mathbf{V}}\sum_{\mathbf{H}, \mathbf{H}'}e^{-E(\mathbf{V},\mathbf{H},\mathbf{H}')}}
\end{align}
and the marginal probability of $(\mathbf{V})$
\begin{align}
	P(\mathbf{V})=\frac{\sum_{\mathbf{H}, \mathbf{H}'}e^{-E(\mathbf{V},\mathbf{H},\mathbf{H}')}}
	{\sum_{\mathbf{V}}\sum_{\mathbf{H},\mathbf{H}'}e^{-E(\mathbf{V},\mathbf{H},\mathbf{H}')}}
\end{align}
Since, we cannot perform the marginalization and the computation of the
partition function directly, we leverage MCMC techniques in order
to carry out inference.\par
%Computation of $P(\mathbf{h}|\mathbf{v})$ can be done 
%resolves in the parallel computations
%of 
Energy contributions per position and motif can be computed massively parallel
by leveraging GPUs:
\begin{align}
	x_{nk}^s&:=\sum_{m=0}^{M-1}\mathbf{v}_{n+m}^{s\top} \mathbf{W}_{m+1}^k + \mathbf{b}_k\\
	{x'}_{nk}^s&:=\sum_{m=0}^{M-1}\mathbf{v}_{n+m}^{s\top} C(\mathbf{W}_{M-m}^k) + \mathbf{b}_k\\
\end{align}
and 
At the second step, we employ probabilistic max-pooling, 
by splitting the range of $N-M+1$ hidden activations for
each motif and its comlementary motif into $P=(N-M+1)/a$ distinct bins.
Within each bin at most one hidden unit or none 
 are turned on per a motif and its complementary representation.
That is, for each $1\leq p\leq P$ and $1\leq i\leq a$ the probabilities
\begin{align}
	P(h_{a(p-1)+i}^k=1|\mathbf{v})&:=\frac{e^{x_{a(p-1)+i}^k}}{\sum_{j=1}^a e^{x_{a(p-1)+j}^k}+ \sum_{j=1}^a e^{{x'}_{a(p-1)+j}^k}+1}\\
	P({h'}_{a(p-1)+i}^k=1|\mathbf{v})&:=\frac{e^{{x'}_{a(p-1)+i}^k}}{\sum_{j=1}^{a} e^{x_{a(p-1)+j}^k}+ \sum_{j=1}^a e^{{x'}_{a(p-1)+j}^k}+1}\\
	P({h'}_{a(p-1)}^k=\mathbf{0}|\mathbf{v})&:=\frac{1}{\sum_{j=1}^a e^{x_{a(p-1)+j}^k}+ \sum_{j=1}^a e^{{x'}_{a(p-1)+j}^k}+1}
\end{align}
(assuming that $N-M+1$ is divisible by $a$).\par
For the top-down pass, we determine $P(\mathbf{v}|\mathbf{h})$ according to
\begin{align}
	\mathbf{y}_n:=\sum_{k=1}^K\sum_{m=0}^{M-1} \mathbf{W}_{m+1}^k h_{n-m}^k + \sum_{k=1}^K\sum_{m=0}^{M-1} C(\mathbf{W}_{M-m}^k) {h'}_{n-m}^k + c
\end{align}
and 
\begin{align}
	P(\mathbf{v}_n|\mathbf{h}):=\frac{e^{y_n}}{\mathbf{1}^\top e^{y_n}}
\end{align}
\section{Inference}
Inference is used to estimate $P(\mathbf{v})$ and is carried out by repeatedly sampling
$P(\mathbf{h}|\mathbf{v})$ and $P(\mathbf{v}|\mathbf{h})$. It allows us to estimate the
marginal distribution $P(\mathbf{v})$, the partition function $Z$ and the gradient for 
maximum likelihood learning.
\section{Learning}
\subsection{Sparsity constraint}
In order to force the probabilistic system to learn meaningful interpretations
of the DNA sequence, we impose the following restriction on the model which adds to
the energy function
\begin{align}
Reg(\mathbf{V}):=\lambda\cdot \sum_{k=1}^K (ReLu(\mathbf{E}_{Data}[\mathbf{H}^k|\mathbf{V}] + \mathbf{E}_{Data}[\mathbf{H'}^k|\mathbf{V}] - \rho))
\end{align}
with $\rho=0.01$ meaning that each motif should occur at most once every one hundred positions
and 
\begin{align}
\mathbf{E}_{Data}[\mathbf{H}^k|\mathbf{V}]=\frac{1}{S(N-M+1)}\sum_s\sum_{n=1}^{N-M+1}\sigma(x_n^k) \\
\mathbf{E}_{Data}[{\mathbf{H}'}^k|\mathbf{V}]=\frac{1}{S(N-M+1)}\sum_s\sum_{n=1}^{N-M+1} \sigma({x'}_n^k)
\end{align}
The gradient with respect to the penalization term is given by 
The gradient with respect to the regularized likelihood is given by
\begin{align}
	\nabla_{W^k} \log P(\mathbf{V}) - \nabla_{W^k}Reg(\mathbf{V}).
\end{align}
Due to the fact that maximum likelihood learning is rather slow, we
employ the contrastive divergence optimization strategy, which has been proven
successful in practice
\begin{align}
	CD_{W_j^k}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto 
	\sum_{s=1}^S \sum_{i=1}^{N-M+1} v_{i}^s p(h_{i-j+1}^{sk}|\mathbf{v}^s)+
	 C(v_{i+M-j}^s) p({h'}_{i}^{sk}|\mathbf{v}^s)\nonumber\\
	 &- \sum_{s=1}^S \sum_{i=1}^{N-M+1}v_{i}^{rs} p(h_{i-j+1}^{rsk}|\mathbf{v}^{rs})
	 +C(v_{i+M-j}^{rs}) p({h'}_{i}^{rsk}|\mathbf{v}^{rs})\label{cd1w}\\
	CD_{b_k}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto 
	\sum_{s=1}^S \sum_{i=1}^{N-M+1} p(h_{i}^{k}|\mathbf{v}^s)+
	p({h'}_{i}^{k}|\mathbf{v}^s)
	- p(h_{i}^{rsk}|\mathbf{v}^{rs})
	- p({h'}_{i}^{rsk}|\mathbf{v}^{rs})\label{cd1b}\\
	CD_{c}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto 
	\sum_{s=1}^S \sum_{i=1}^{N} v_{i}^s 
	- p(v_{i}^{rs}|\mathbf{h}^{rs})
\end{align}
The superscript $r$ for $v^r$ and $h^r$ in the negative phase denote the
reconstructions after aquiring one or more bottom-up /top-down samples.
Recall the definition of the cross-correlation function which is defined as
\begin{align}
 R_{fg}(x)&=\int_{-\infty}^\infty f(u+x)g(u)du\\
 &=\int_{-\infty}^\infty f(u)g(u-x)du\\
 &=(f\star g)(x)
\end{align}
which is similar to the convolution operation
The contrastive divergence in Equation \ref{cd1w} makes 
use of the cross-correlation operation.
\section{Persistent contrastive divergence}
In the ordinary form of contrastive divergence, the negative phase is
determined by running the block Gibbs sampling for $k$ steps, each time
starting from an observed data-point $\mathbf{v}^s$. 
In contrast to that, persistent contrastive divergence uses runs
block Gibbs sampling starting from  the previously stored state, which
should be closer to the stationary state of the Markov chain, 
and hence lead to better estimates.
This is also what Hinton refers to as using persitent "fantasy particles".
\section{Monitoring of the average Free energy}
We can monitor the learning progress and prevent overfitting by comparing 
the average Free energy on the training and test set. In the beginning of 
the learning phase, in either set, the free energy should decrease.
Upon overfitting, the Free energy on the test set starts to increase again,
while it still decreases on the training set.
The advantage of this approach is, that we do not have to deal with 
the partition function, which simply cancels out.
The Free energy for our setting is defined as
\begin{align}
	F(\mathbf{v})&=&-\log \sum_{\mathbf{h},\mathbf{h}'}e^{-E(\mathbf{v},\mathbf{h},\mathbf{h}')}\\
	&=&-\log\sum_{\mathbf{h},\mathbf{h}'} 
	exp(
	\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top W_{m+1}^k h_n^k \nonumber\\
	&&+\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top C(W_{M-m}^k) {h'}_n^k\nonumber\\
	&& +\sum_{k=1}^K b_k \sum_{n=1}^{N-M+1}(h_n^k + {h'}_n^k)
	+c^\top \sum_{n=1}^{N}v_n )\\
	%&=&-c^\top\sum_{n=1}^{N}v_n -\log\sum_{\mathbf{h},\mathbf{h}'} 
	%exp(
	%\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top W_{m+1}^k h_n^k \nonumber\\
	%&&+\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top C(W_{M-m}^k) {h'}_n^k\nonumber\\
	%&& +\sum_{k=1}^K b_k \sum_{n=1}^{N-M+1}(h_n^k + {h'}_n^k))\\
	&=&-c^\top\sum_{n=1}^{N}v_n \nonumber\\
	&&-\sum_{k=1}^K\sum_{p=0}^{P-1}\log(
	1+\sum_{m=0}^{M-1}\sum_{i=1}^a e^{b_k + v^\top_{a\cdot p+i+m}W^k_{m+1}}\nonumber\\
	&&+\sum_{m=0}^{M-1}\sum_{i=1}^a e^{b_k + v^\top_{a\cdot p+i+m}C(W^k_{M-m})})\\
	%&&exp(
	%\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top W_{m+1}^k h_n^k \nonumber\\
	%&&+\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top C(W_{M-m}^k) {h'}_n^k\nonumber\\
	%&& +\sum_{k=1}^K b_k \sum_{n=1}^{N-M+1}(h_n^k + {h'}_n^k))
\end{align}
\section{Estimation of the partition function}
We might follow the AIC appoach in order to estimate the partition
function \cite{salakhutdinov2008quantitative}. Right now, it is not possible
to quantify the probability of an observed sequence. However, once we
have gained access to the partition function, we can.
%\begin{align}
%	\nabla_{W^k} \log P(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto
%	\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{[i:i+M-1]}^s* \mathbf{h}_{[i:i+M-1]}^s\nonumber\\
%	&+\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{[i+M-1:i]}^s)* \mathbf{h'}_{[i:i+M-1]}^s\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{[i:i+M-1]}^{rs}* \mathbf{h}_{[i:i+M-1]}^{rs}\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{[i+M-1:i]}^{rs})* \mathbf{h'}_{[i:i+M-1]}^{rs}\nonumber
%\end{align}
%\begin{align}
%	CD_{W^k_j}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto
%	\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{i}^s* p(h_{i-j+1}^s|\mathbf{v})\nonumber\\
%	&+\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{i-j+1}^s)* p({h'}_{i}^s|\mathbf {v})\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{i}^{rs}* h_{i-j+1}^{rs}\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{i-j+1}^{rs})* {h'}_{i}^{rs}
%\end{align}
%We use zero-padding $h_i$ where $i<1$ and $i>N-M+1$.
\bibliographystyle{plain}
\bibliography{literature}

\end{document}
