\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}

\begin{document}
Definition of the convolutional restricted Boltzmann machine
\begin{align}
	E(\mathbf{v},\mathbf{h}, \mathbf{h}')=
	-\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top W_{m+1}^k h_n^k \nonumber\\
	-\sum_{n=1}^{N-M+1}\sum_{k=1}^K\sum_{m=0}^{M-1} v_{n+m}^\top C(W_{M-m}^k) {h'}_n^k\nonumber\\
	- \sum_{k=1}^K b_k \sum_{n=1}^{N-M+1}(h_n^k + {h'}_n^k)
	-c^\top \sum_{n=1}^{N}v_n
\end{align}
where $\mathbf{v}$ denotes the DNA sequence, $\mathbf{h}$ the forward strand 
hidden states, $\mathbf{h}'$ the reverse strand
hidden states, $N$ the length of the DNA, $M$ the length of the motif, 
$K$ the number of the motif, $W^k$ the parameters for the $k^{th}$ motif,
$b_k$ the bias term for the $k^{th}$ motif and $c$ the bias for the
nucleotide. Additionally, the function $C(.)$ denotes complementary mapping
as follows: $A\rightarrow T$, 
$C\rightarrow G$, $G\rightarrow C$ and $T\rightarrow A$.
The associated probability to a state $(\mathbf{v},\mathbf{h})$ is then given
by
\begin{align}
	P(\mathbf{v},\mathbf{h}, \mathbf{h}')=\frac{e^{-E(\mathbf{v},\mathbf{h},\mathbf{h}')}}
	{\sum_{\mathbf{v}}\sum_{\mathbf{h}, \mathbf{h}'}e^{-E(\mathbf{v},\mathbf{h},\mathbf{h}')}}
\end{align}
and the marginal probability of $(\mathbf{v})$
\begin{align}
	P(\mathbf{v})=\frac{\sum_{\mathbf{h}, \mathbf{h}'}e^{-E(\mathbf{v},\mathbf{h},\mathbf{h}')}}
	{\sum_{\mathbf{v}}\sum_{\mathbf{h},\mathbf{h}'}e^{-E(\mathbf{v},\mathbf{h},\mathbf{h}')}}
\end{align}
Since, we cannot perform the marginalization and the computation of the
partition function directly, we leverage MCMC techniques in order
to carry out inference.\par
%Computation of $P(\mathbf{h}|\mathbf{v})$ can be done 
%resolves in the parallel computations
%of 
Energy contributions per position and motif can be computed massively parallel
by leveraging GPUs:
\begin{align}
	x_n^k&:=\sum_{m=0}^{M-1}v_{n+m}^\top W_{m+1}^k + b_k\\
	{x'}_n^k&:=\sum_{m=0}^{M-1}v_{n+m}^\top C(W_{M-m}^k) + b_k\\
\end{align}
At the second step, we employ probabilistic max-pooling, 
by splitting the range of $N-M+1$ hidden activations for
each motif and its comlementary motif into $P=(N-M+1)/a$ distinct bins.
Within each bin at most one hidden unit or none 
 are turned on per a motif and its complementary representation.
That is, for each $1\leq p\leq P$ and $1\leq i\leq a$ the probabilities
\begin{align}
	P(h_{a(p-1)+i}^k=1|\mathbf{v})&:=\frac{e^{x_{a(p-1)+i}^k}}{\sum_{j=1}^a e^{x_{a(p-1)+j}^k}+ \sum_{j=1}^a e^{{x'}_{a(p-1)+j}^k}+1}\\
	P({h'}_{a(p-1)+i}^k=1|\mathbf{v})&:=\frac{e^{{x'}_{a(p-1)+i}^k}}{\sum_{j=1}^{a} e^{x_{a(p-1)+j}^k}+ \sum_{j=1}^a e^{{x'}_{a(p-1)+j}^k}+1}\\
	P({h'}_{a(p-1)}^k=\mathbf{0}|\mathbf{v})&:=\frac{1}{\sum_{j=1}^a e^{x_{a(p-1)+j}^k}+ \sum_{j=1}^a e^{{x'}_{a(p-1)+j}^k}+1}
\end{align}
(assuming that $N-M+1$ is divisible by $a$).\par
For the top-down pass, we determine $P(\mathbf{v}|\mathbf{h})$ according to
\begin{align}
	y_n:=\sum_{k=1}^K\sum_{m=0}^{M-1} W_{m+1}^k h_{n-m}^k + \sum_{k=1}^K\sum_{m=0}^{M-1} C(W_{M-m}^k) {h'}_{n-m}^k + c
\end{align}
and 
\begin{align}
	P(v_n|\mathbf{h}):=\frac{e^{y_n}}{\mathbf{1}^\top e^{y_n}}
\end{align}
\section{Inference}
Inference is used to estimate $P(\mathbf{v})$ and is carried out by repeatedly sampling
$P(\mathbf{h}|\mathbf{v})$ and $P(\mathbf{v}|\mathbf{h})$. It allows us to estimate the
marginal distribution $P(\mathbf{v})$, the partition function $Z$ and the gradient for 
maximum likelihood learning.
\section{Learning}
%The gradient with respect to the likelihood
%\begin{align}
%	\nabla_{W^k} \log P(\mathbf{v}) 
%\end{align}
We adjust the parameters by employing a practical
short-cut with respect to ordinary maximum likelihood learning,
referred to as contrastive divergent which is given by
\begin{align}
	CD_{W_j^k}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto 
	\sum_{s=1}^S \sum_{i=1}^{N-M+1} v_{i}^s p(h_{i-j+1}^{sk}|\mathbf{v})+
	 C(v_{i+M-j}^s) p({h'}_{i}^{sk}|\mathbf{v})\nonumber\\
	 &- \sum_{s=1}^S \sum_{i=1}^{N-M+1}v_{i}^{rs} h_{i-j+1}^{rsk}
	 +C(v_{i+M-j}^{rs}) {h'}_{i}^{rsk}\label{cd1w}\\
	CD_{b_k}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto 
	\sum_{s=1}^S \sum_{i=1}^{N-M+1} p(h_{i}^{k}|\mathbf{v})+
	p({h'}_{i}^{k}|\mathbf{v})
	- h_{i}^{rsk}
	- {h'}_{i}^{rsk}\label{cd1b}\\
	CD_{c}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto 
	\sum_{s=1}^S \sum_{i=1}^{N} v_{i}^s 
	- v_{i}^{rs}
\end{align}
The superscript $r$ for $v^r$ and $h^r$ in the negative phase denote the
reconstructions after aquiring one or more bottom-up /top-down samples.
Recall the definition of the cross-correlation function which is defined as
\begin{align}
 R_{fg}(x)&=\int_{-\infty}^\infty f(u+x)g(u)du\\
 &=\int_{-\infty}^\infty f(u)g(u-x)du\\
 &=(f\star g)(x)
\end{align}
which is similar to the convolution operation
The contrastive divergence in Equation \ref{cd1w} makes 
use of the cross-correlation operation.
%\begin{align}
%	\nabla_{W^k} \log P(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto
%	\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{[i:i+M-1]}^s* \mathbf{h}_{[i:i+M-1]}^s\nonumber\\
%	&+\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{[i+M-1:i]}^s)* \mathbf{h'}_{[i:i+M-1]}^s\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{[i:i+M-1]}^{rs}* \mathbf{h}_{[i:i+M-1]}^{rs}\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{[i+M-1:i]}^{rs})* \mathbf{h'}_{[i:i+M-1]}^{rs}\nonumber
%\end{align}
%\begin{align}
%	CD_{W^k_j}(\mathbf{v}^1\cdots\mathbf{v}^S) &\propto
%	\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{i}^s* p(h_{i-j+1}^s|\mathbf{v})\nonumber\\
%	&+\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{i-j+1}^s)* p({h'}_{i}^s|\mathbf {v})\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}\mathbf{v}_{i}^{rs}* h_{i-j+1}^{rs}\nonumber\\
%	&-\sum_{s=1}^S \sum_{i=1}^{N-M+1}C(\mathbf{v}_{i-j+1}^{rs})* {h'}_{i}^{rs}
%\end{align}
%We use zero-padding $h_i$ where $i<1$ and $i>N-M+1$.
\end{document}
