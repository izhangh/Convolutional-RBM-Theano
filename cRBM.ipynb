{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a convolutional RBM using Theano\n",
    "\n",
    "This notebook implements an efficient cRBM with various training procedures and different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Importing theano, Biopython and all the other useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import Bio.SeqIO as sio\n",
    "import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Getting and transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFASTASequencesFromFile (filename):\n",
    "    dhsSequences = []\n",
    "    for dhs in sio.parse(open(filename), 'fasta', IUPAC.unambiguous_dna):\n",
    "        dhsSequences.append(dhs.seq)\n",
    "    return dhsSequences\n",
    "\n",
    "def readPWMsFromFile (filename):\n",
    "    matrices = []\n",
    "    for mat in motifs.parse(open(filename), 'jaspar'):\n",
    "        matrices.append(mat.pwm)\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in FASTA sequences\n",
    "These sequences are DNase1-hypersensitivity sites where each sample represents one DHS.\n",
    "They were all already brought to the same dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allSeqs = readFASTASequencesFromFile('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwms = readPWMsFromFile('../data/jaspar_matrices.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test set\n",
    "Since working with these huge amounts of sequences is very time consuming, just start with a selection of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set: 1000\n"
     ]
    }
   ],
   "source": [
    "test_set = [allSeqs[random.randrange(0,len(allSeqs))] for i in range(1000)]\n",
    "print \"Length of test set: \"  + str(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert test set sequences to matrices we can work with\n",
    "Each sequence is converted to a matrix of dimensionality (2 x 4 x N_v), representing:\n",
    "* the two strands (first dimension)\n",
    "* the four letters of the DNA alphabet (second dimension)\n",
    "* the sequence represented by 1 (letter is present at this position) or 0 (letter is not present) (third dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    m = len(seq.alphabet.letters)\n",
    "    n = len(seq)\n",
    "    result = np.zeros((2, m, n))\n",
    "    revSeq = seq.reverse_complement()\n",
    "    for i in range(len(seq)):\n",
    "        result[0,getIntToLetter(seq[i]),i] = 1\n",
    "        result[1,getIntToLetter(revSeq[i]),i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of sequence to matrix for the test set in (in ms): 298.466920853\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataMat = np.array([getMatrixFromSeq(t) for t in test_set])\n",
    "print \"Conversion of sequence to matrix for the test set in (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing the cRBM\n",
    "We now have all the tools we need to actually implement the cRBM.\n",
    "Some notes on constraints and behavior of our learning algorithm:\n",
    "* It uses Theano to do most (if not all) of the work. That means it's ways faster on a GPU\n",
    "* It expects kernels (filters/motifs) to be of the same length. While this choice was made due to performance issues, it's not a problem because we expect the algorithm to combine multiple motifs if nessessary.\n",
    "* The sequences (DHSes) are also expected to be of the same size\n",
    "\n",
    "Now, finally some hints on usage:\n",
    "\n",
    "Blah, blah, blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class implements a cRBM for sequence analysis.\n",
    "It does perform efficient forward and backward pass of any given DNA sequence.\n",
    "It implements softmax and sigmoid functions as activation and performs probabilistic max pooling\n",
    "after the convolution step.\n",
    "So this class is basically a two layer network with a convolution layer and a pooling layer on top\n",
    "of that.\n",
    "The learning procedure uses contrastive divergence (CD) with a variable amount of steps.\n",
    "\"\"\"\n",
    "class ConvRBM:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the cRBM. The parameters here are global params that should not change\n",
    "    during the execution of training or testing and characterize the network.\n",
    "    \n",
    "    Parameters:\n",
    "    _motifLength:    How long are the motifs (position weight matrices PWM). This\n",
    "                     This is equivalent to ask what the number of k-mers is.\n",
    "                     The current approach only deals with one fixed motif length.\n",
    "                     \n",
    "    _numMotifs:      How many motifs are applied to the sequence, that is how many\n",
    "                     hidden units does the network have. Each hidden unit consists\n",
    "                     of a vector of size (sequenceLength-motifLength+1)\n",
    "                     \n",
    "    _poolingFactor:  How many units from the hidden layer are pooled together.\n",
    "                     Note that the number has to divide evenly to the length of\n",
    "                     the hidden units, that is:\n",
    "                     mod(sequenceLength-motifLength+1, poolingFactor) == 0\n",
    "                     (1 = equivalent to sigmoid activation)\n",
    "                     \n",
    "    _alphabet:       Biopython uses alphabets for sequences to do sanity checks.\n",
    "                     However, all of the code is written for DNA sequences and even\n",
    "                     though in theory there should be no difference between that\n",
    "                     and other alphabets, Biopython may have trouble with the convolution.\n",
    "    \"\"\"\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.motifs = np.random.rand(_numMotifs, 2, 4, _motifLength)\n",
    "        self.alphabet = _alphabet\n",
    "        \n",
    "        # cRBM parameters\n",
    "        self.bias = np.random.rand(self.numMotifs)\n",
    "        self.c = random.random()\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Calculate the forward pass for any given set of sequences, that is calculate P(H | V).\n",
    "    This method applies convolution of all filters to all sequences in the set, using theano.\n",
    "    It also looks on both strands for matches and returns the hidden activation layer.\n",
    "    \n",
    "    Parameters:\n",
    "    data:            The DNA sequences to calculate the forward pass on. The data is a 4D matrix\n",
    "                     with dimensionality: (N_batch x numOfStrands(2) x numOfLetters(4) x N_v)\n",
    "                     \n",
    "    Return:\n",
    "    The function returns the hidden activation layer as numpy matrix.\n",
    "    That matrix has dimensionality N_batch x K x 1 x N_h where\n",
    "    K is the number of kernels (motifs/PWMs) that are applied and\n",
    "    N_h is the length of the hidden layer (convolution is of type 'valid')\n",
    "    \n",
    "    Note that the strandness is lost during this process because it's added up in the hidden layer.\n",
    "    \"\"\"\n",
    "    def forwardBatch (self, data):\n",
    "        # create 4D tensor for theano (BatchSize x K x 2*numOfLetters x lenOfSeqs)\n",
    "        D = T.tensor4('data')\n",
    "        K = T.tensor4('kernels')\n",
    "        out = conv.conv2d(D,K)\n",
    "        f = theano.function([D,K], out, allow_input_downcast=True)\n",
    "        return f(data, self.motifs)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the backward pass on the data, that is P(V | H).\n",
    "    It does so by performing convolution of the hidden layer and the motifs for each kernel using theano.\n",
    "    \n",
    "    Parameters:\n",
    "    hidden layer:   The layer that was previously computed by the forward pass.\n",
    "    \n",
    "    Return:\n",
    "    A matrix of dimensionality: N_batch x K x numOfLetters(4) x N_v\n",
    "    The result can be interpreted as the probability for each position to have a specific letter\n",
    "    once it is summed over all K (sum over second dimension).\n",
    "    That means, the combination of summing over K and applying softmax can be interpreted as P(V | H).\n",
    "    \"\"\"\n",
    "    def backwardBatch (self, hiddenActivation):\n",
    "        # theano convolution call\n",
    "        H = T.tensor4('hidden')\n",
    "        K = T.tensor4('kernels')\n",
    "        K_star = K.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "        C = conv.conv2d(H, K_star, border_mode='full')\n",
    "        out = T.sum(C, axis=1) # sum over all K\n",
    "        f = theano.function([H,K], out, allow_input_downcast=True)\n",
    "\n",
    "        return f(hiddenActivation, self.motifs)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Set new kernels when you don't wish to start with random ones. Can be used if prior knowledge about\n",
    "    the structure of the sequences is present.\n",
    "    \n",
    "    Parameters:\n",
    "    customKernels:  A matrix of shape (K x numOfStrands(2) x numOfLetters(4) x num-of-k-mers) containing the\n",
    "                    new kernels to work with.\n",
    "                    \n",
    "    Return:\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "    def setCustomKernels (self, customKernels):\n",
    "        if len(customKernels.shape) != 4:\n",
    "            print \"New motifs must be a 4D matrix with dims: (K x numOfStrands(2) x numOfLetters(4) x numOfKMers)\"\n",
    "            return\n",
    "        \n",
    "        self.numMotifs = customKernels.shape[0]\n",
    "        self.motifLength = customKernels.shape[3]\n",
    "        print \"New motifs set. # Motifs: \" + str(self.numMotifs) + \" K-mer-Length: \" + str(self.motifLength)\n",
    "        self.motifs = customKernels\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the gradient for a given data matrix and samples from the hidden layer.\n",
    "    The gradient is computed using theano and convolution.\n",
    "    \n",
    "    Parameters:\n",
    "    hiddenSample:   A matrix of shape (N_batch x K x numOfLetters(4) x N_v) containing samples\n",
    "                    from the prob. distribution of the hidden layer.\n",
    "                    Note that this matrix has the same structure as the result from forwardBatch().\n",
    "                    \n",
    "    data:           A matrix of shape (N_batch x numOfStrands(2) x numOfLetters(4) x N_v) containing\n",
    "                    the data. Note that this matrix is usually the same as the matrix passed to forwardBatch().\n",
    "                    \n",
    "    Return:\n",
    "    A matrix containing the gradient for all kernels/motifs/pwms.\n",
    "    This matrix is of shape (N_batch x K x 1 x num-of-k-mers)\n",
    "    \"\"\"\n",
    "    def gradient (self, hiddenSample, data):\n",
    "        H = T.tensor4('hidden')\n",
    "        S = T.tensor4('sample')\n",
    "        H_reshaped = H.dimshuffle(1, 0, 2, 3)\n",
    "        out = conv.conv2d(S, H_reshaped)\n",
    "        f = theano.function([H,S], out, allow_input_downcast=True)\n",
    "\n",
    "        return f(np.tile(np.mean(hiddenSample, axis=0), [2,1,1,1]), np.tile(data, [1,1,1,1]))\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The training algorithm for cRBMs. This uses the forward and backward pass\n",
    "    to collect the statistics for them.\n",
    "    \"\"\"\n",
    "    def miniBatchTraining (self, sequences, batchSize, cd_value):\n",
    "        \n",
    "        # first, some vars that we need all the time\n",
    "        sequenceLength = len(sequences[0])\n",
    "        hiddenUnitLength = sequenceLength - self.motifLength + 1\n",
    "        \n",
    "        # first, compute expected value of the data\n",
    "        # that means computing forward pass for all datapoints and computing the gradient\n",
    "        hiddenActivation = np.zeros((2*self.numMotifs, hiddenUnitLength))\n",
    "        derivatives = np.zeros((self.motifLength, self.numMotifs))\n",
    "        for seq in sequences:\n",
    "            hiddenActivation = self.forwardPass(seq)\n",
    "            \n",
    "            # calculate the gradients of the data\n",
    "            # In order to do that, we have to convolve the DNA and hidden layer\n",
    "            # This can be done by expressing DNA as four different convolutions (for each letter)\n",
    "            # We would need to represent the DNA as matrix of 4 x lengthOfDHS\n",
    "            # Example:\n",
    "            # -------\n",
    "            # ACGTGGGG\n",
    "            # 10000000\n",
    "            # 01000000\n",
    "            # 00101111\n",
    "            # 00010000\n",
    "            # Then, we can apply 4 convolutions to the sequence\n",
    "            for k in range(self.numMotifs):\n",
    "                derivatives[k,:] += np.convolve( hiddenActivation[k,:])\n",
    "            \n",
    "        print derivatives\n",
    "        # Now, sample from this distribution and collect the model's statistics\n",
    "        # Do that by sampling from the data statistics, recompute the model's statistics\n",
    "        # and so on until reaching the stationary distribution (in case of CD, do it once)\n",
    "        hidden_model = hiddenActivation\n",
    "        for it in range(cd_value):\n",
    "            sample = self._sampleFromHidden(hidden_model)\n",
    "            visible = self.backwardPass(sample)\n",
    "            hidden_model = self.forwardPass(visible)\n",
    "\n",
    "        print hidden_model.shape\n",
    "        \n",
    "        return self.learningRate * (hiddenActivation - hidden_model)\n",
    "   \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    For sure deprecated.\n",
    "    \"\"\"\n",
    "    def _probMaxPooling (self, h_k):\n",
    "\n",
    "        # first of all some easy definitions\n",
    "        l = h_k.shape[1]\n",
    "        numOfGroups = l/self.poolingFactor\n",
    "        P = np.zeros((2, l))\n",
    "\n",
    "        # exponent of everything\n",
    "        ex = np.exp(h_k)\n",
    "        \n",
    "        # reshape s.t. each group forms one row\n",
    "        newDim = (numOfGroups, -1)\n",
    "        reordered = np.append(ex[0].reshape(newDim), ex[1].reshape(newDim), 1)\n",
    "        #print \"Shape of reordered: \" + str(reordered.shape)\n",
    "        # calculate denominators (sum of all rows)\n",
    "        denoms = np.sum(reordered, 1) + 1 # denoms for all groups (add 1 to have log. unit)\n",
    "        res = np.argmax(reordered.T / denoms, 0)\n",
    "\n",
    "        # calculate the actual values of the pooling layer P\n",
    "        for group in range(numOfGroups):\n",
    "            if reordered[group,res[group]] > 1: # check if really any element from P should be active\n",
    "                # we don't care about strands so just set res = res/2 for the index\n",
    "                idx = group * self.poolingFactor + int(res[group]/2)\n",
    "                P[res[group] % 2, idx] = reordered[group,res[group]] / denoms[group]\n",
    "        return P\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Maybe not working...\n",
    "    \"\"\"\n",
    "    def _sampleFromHidden (self, hiddenActivation):\n",
    "        boolean_mat = hiddenActivation > self.rng.random_sample(hiddenActivation.shape)\n",
    "        return boolean_mat.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the sigmoid activation for the hidden layer.\n",
    "    \"\"\"\n",
    "    def sigmoid(self, _H):\n",
    "        return (1.0 / (1.0 + np.exp(-_H)))\n",
    "\n",
    "\n",
    "\n",
    "    def softmax (self, V):\n",
    "        # insert ones (that is the 5th component 1/1+e^x)\n",
    "        #withOnes = np.insert(V, 4, np.ones(V.shape[2]), 1)\n",
    "        exp = np.exp(V) # exponentiate it all\n",
    "        denominator = np.sum(exp, axis=1) # this the sum of all the exp(-x)\n",
    "        # now expand the denominator such that the division works\n",
    "        denominator = denominator[:,np.newaxis,:].repeat(exp.shape[1], axis=1)\n",
    "        div = exp / denominator\n",
    "        return div\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Maybe deprecated.\n",
    "    \"\"\"\n",
    "    def _getLetterToInt (self, num):\n",
    "        if num == 0:\n",
    "            return 'A'\n",
    "        elif num == 1:\n",
    "            return 'C'\n",
    "        elif num == 2:\n",
    "            return 'G'\n",
    "        elif num == 3:\n",
    "            return 'T'\n",
    "        else:\n",
    "            print 'ERROR: Num ' + str(num) + \" not a valid char in DNA alphabet\"\n",
    "            return -1\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Maybe deprecated.\n",
    "    \"\"\"\n",
    "    def _getDNASeqFromNumericals (self, seq):\n",
    "        dna_seq = []\n",
    "        for num in range(seq.shape[0]):\n",
    "            dna_seq.append(self._getLetterToInt(seq[num]))\n",
    "        return Seq(\"\".join(dna_seq), alphabet=self.alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the algorithm is doing something meaningful\n",
    "We can now apply the learning algorithm on a toy sequence to see what happens.\n",
    "Important questions to ask:\n",
    "* Does the hidden layer somehow make sense?\n",
    "* What does the sigmoid function do with it?\n",
    "* How does the *reconstruction* look like?\n",
    "* Is the softmax doing the right thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 1 K-mer-Length: 4\n",
      "Result from forward pass (that is: P(H | V))\n",
      "Shape: -> (1, 1, 1, 5)\n",
      "Result from backward pass (that is: P(V | H))\n",
      "Shape: -> (1, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "# Construct a kernel matrix, looking for ACGT and GGGG\n",
    "kernel1 = np.tile(np.eye(4), [2,1,1]) # the kernel will only look for ACGT\n",
    "kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1]) # this one looks for GGGG\n",
    "kernel = np.array([kernel1])\n",
    "\n",
    "# create toy sequences where we might be able to see what happens\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "#data = np.array([randSeq1, randSeq2])\n",
    "data = np.array([randSeq1])\n",
    "# initialize the learner and insert our predefined kernels\n",
    "cRBM = ConvRBM (4, 2)\n",
    "cRBM.setCustomKernels(kernel)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "res = cRBM.sigmoid(cRBM.forwardBatch(data))\n",
    "print \"Result from forward pass (that is: P(H | V))\"\n",
    "print \"Shape: -> \" + str(res.shape)\n",
    "#print res\n",
    "vis = cRBM.softmax(cRBM.backwardBatch(res))\n",
    "print \"Result from backward pass (that is: P(V | H))\"\n",
    "print \"Shape: -> \" + str(vis.shape)\n",
    "#print vis\n",
    "grad = cRBM.gradient(res, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.71,  0.32,  0.32,  0.2 ,  0.34,  0.05,  0.07,  0.1 ],\n",
       "        [ 0.1 ,  0.54,  0.23,  0.27,  0.2 ,  0.4 ,  0.07,  0.1 ],\n",
       "        [ 0.1 ,  0.07,  0.4 ,  0.2 ,  0.27,  0.23,  0.54,  0.1 ],\n",
       "        [ 0.1 ,  0.07,  0.05,  0.34,  0.2 ,  0.32,  0.32,  0.71]]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some performance tests on the test set\n",
    "Now, let's test it on the whole test set. The performance obtained here, should tell us a lot on how fast we can actually train the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 100 K-mer-Length: 6\n",
      "Forward pass: (1000, 100, 1, 145)\n",
      "Backward pass: (1000, 4, 150)\n",
      "Gradient: (1000, 100, 4, 6)\n",
      "Time for processing (for, back, grad) of 1000 sequences: 1.9392850399\n"
     ]
    }
   ],
   "source": [
    "# let's have some more motifs (10)\n",
    "kernel = np.tile(np.random.rand(4,6), [100, 2,1,1])\n",
    "cRBM.setCustomKernels(kernel)\n",
    "\n",
    "# perform forward and backward pass and calculate the gradient\n",
    "start = time.time()\n",
    "res = cRBM.sigmoid(cRBM.forwardBatch(dataMat))\n",
    "vis = cRBM.softmax(cRBM.backwardBatch(res))\n",
    "grad = cRBM.gradient(res, dataMat)\n",
    "print \"Forward pass: \" + str(res.shape)\n",
    "print \"Backward pass: \" + str(vis.shape)\n",
    "print \"Gradient: \" + str(grad.shape)\n",
    "print \"Time for processing (for, back, grad) of \" + str(dataMat.shape[0]) + \" sequences: \" + str((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del cRBM, test_set, allSeqs, pwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
