{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a convolutional RBM using Theano\n",
    "\n",
    "This notebook implements an efficient cRBM with various training procedures and different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Importing theano, Biopython and all the other useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import Bio.SeqIO as sio\n",
    "import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Getting and transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFASTASequencesFromFile (filename):\n",
    "    dhsSequences = []\n",
    "    for dhs in sio.parse(open(filename), 'fasta', IUPAC.unambiguous_dna):\n",
    "        dhsSequences.append(dhs.seq)\n",
    "    return dhsSequences\n",
    "\n",
    "def readPWMsFromFile (filename):\n",
    "    matrices = []\n",
    "    for mat in motifs.parse(open(filename), 'jaspar'):\n",
    "        matrices.append(mat.pwm)\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in FASTA sequences\n",
    "These sequences are DNase1-hypersensitivity sites where each sample represents one DHS.\n",
    "They were all already brought to the same dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allSeqs = readFASTASequencesFromFile('data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwms = readPWMsFromFile('data/jaspar_matrices.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test set\n",
    "Since working with these huge amounts of sequences is very time consuming, just start with a selection of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set: 1000\n"
     ]
    }
   ],
   "source": [
    "test_set = [allSeqs[random.randrange(0,len(allSeqs))] for i in range(1000)]\n",
    "print \"Length of test set: \"  + str(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert test set sequences to matrices we can work with\n",
    "Each sequence is converted to a matrix of dimensionality (2 x 4 x N_v), representing:\n",
    "* the two strands (first dimension)\n",
    "* the four letters of the DNA alphabet (second dimension)\n",
    "* the sequence represented by 1 (letter is present at this position) or 0 (letter is not present) (third dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    m = len(seq.alphabet.letters)\n",
    "    n = len(seq)\n",
    "    result = np.zeros((2, m, n))\n",
    "    for i in range(len(seq)):\n",
    "        result[0,getIntToLetter(seq[i]),i] = 1\n",
    "        result[1,getIntToLetter(seq[i]),i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of sequence to matrix for the test set in (in ms): 308.15911293\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataMat = np.array([getMatrixFromSeq(t) for t in test_set])\n",
    "print \"Conversion of sequence to matrix for the test set in (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Borrowing Ian Goodfellow's implementation of the prob. max pooling layer\n",
    "This implementation is now part of the pylearn2 library which is licensed under the 3-claused BSD license.\n",
    "It should not be a big issue using this as long as we cite the pylearn2 guys properly.\n",
    "\n",
    "Source code is available here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/expr/probabilistic_max_pooling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.gof.op import get_debug_values\n",
    "\n",
    "def max_pool(z, pool_shape, top_down=None, theano_rng=None):\n",
    "    \"\"\"\n",
    "    Probabilistic max-pooling\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : theano 4-tensor\n",
    "        a theano 4-tensor representing input from below\n",
    "    pool_shape : tuple\n",
    "        tuple of ints. the shape of regions to be pooled\n",
    "    top_down : theano 4-tensor, optional\n",
    "        a theano 4-tensor representing input from above\n",
    "        if None, assumes top-down input is 0\n",
    "    theano_rng : MRG_RandomStreams, optional\n",
    "        Used for random numbers for sampling\n",
    "    Returns\n",
    "    -------\n",
    "    p : theano 4-tensor\n",
    "        the expected value of the pooling layer p\n",
    "    h : theano 4-tensor\n",
    "        the expected value of the detector layer h\n",
    "    p_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the pooling layer\n",
    "    h_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the detector layer\n",
    "    Notes\n",
    "    ------\n",
    "    all 4-tensors are formatted with axes ('b', 'c', 0, 1).\n",
    "    This is for maximum speed when using theano's conv2d\n",
    "    to generate z and top_down, or when using it to infer conditionals of\n",
    "    other layers using the return values.\n",
    "    Detailed description:\n",
    "    Suppose you have a variable h that lives in a Conv2DSpace h_space and\n",
    "    you want to pool it down to a variable p that lives in a smaller\n",
    "    Conv2DSpace p.\n",
    "    This function does that, using non-overlapping pools.\n",
    "    Specifically, consider one channel of h. h must have a height that is a\n",
    "    multiple of pool_shape[0] and a width that is a multiple of pool_shape[1].\n",
    "    A channel of h can thus be broken down into non-overlapping rectangles\n",
    "    of shape pool_shape.\n",
    "    Now consider one rectangular pooled region within one channel of h.\n",
    "    I now use 'h' to refer just to this rectangle, and 'p' to refer to\n",
    "    just the one pooling unit associated with that rectangle.\n",
    "    We assume that the space that h and p live in is constrained such\n",
    "    that h and p are both binary and p = max(h). To reduce the state-space\n",
    "    in order to make probabilistic computations cheaper we also\n",
    "    constrain sum(h) <= 1.\n",
    "    Suppose h contains k different units. Suppose that the only term\n",
    "    in the model's energy function involving h is -(z*h).sum()\n",
    "    (elemwise multiplication) and the only term in\n",
    "    the model's energy function involving p is -(top_down*p).sum().\n",
    "    Then P(h[i] = 1) = softmax( [ z[1], z[2], ..., z[k], -top_down] )[i]\n",
    "    and P(p = 1) = 1-softmax( [z[1], z[2], ..., z[k], -top_down])[k]\n",
    "    This variation of the function assumes that z, top_down, and all\n",
    "    return values use Conv2D axes ('b', 'c', 0, 1).\n",
    "    This variation of the function implements the softmax using a\n",
    "    theano graph of exp, maximum, sub, and div operations.\n",
    "    Performance notes:\n",
    "    It might be possible to make a faster implementation with different\n",
    "    theano ops. rather than using set_subtensor, it might be possible\n",
    "    to use the stuff in theano.sandbox.neighbours. Probably not possible,\n",
    "    or at least nasty, because that code isn't written with multiple\n",
    "    channels in mind, and I don't think just a reshape can fix it.\n",
    "    Some work on this in galatea.cond.neighbs.py\n",
    "    At some point images2neighbs' gradient was broken so check that\n",
    "    it has been fixed before sinking too much time into this.\n",
    "    Stabilizing the softmax is also another source of slowness.\n",
    "    Here it is stabilized with several calls to maximum and sub.\n",
    "    It might also be possible to stabilize it with\n",
    "    T.maximum(-top_down,T.signal.downsample.max_pool(z)).\n",
    "    Don't know if that would be faster or slower.\n",
    "    Elsewhere in this file I implemented the softmax with a reshape\n",
    "    and call to Softmax / SoftmaxWithBias.\n",
    "    This is slower, even though Softmax is faster on the GPU than the\n",
    "    equivalent max/sub/exp/div graph. Maybe the reshape is too expensive.\n",
    "    Benchmarks show that most of the time is spent in GpuIncSubtensor\n",
    "    when running on gpu. So it is mostly that which needs a faster\n",
    "    implementation. One other way to implement this would be with\n",
    "    a linear.Conv2D.lmul_T, where the convolution stride is equal to\n",
    "    the pool width, and the thing to multiply with is the hparts stacked\n",
    "    along the channel axis. Unfortunately, conv2D doesn't work right\n",
    "    with stride > 2 and is pretty slow for stride 2. Conv3D is used to\n",
    "    mitigate some of this, but only has CPU code.\n",
    "    \"\"\"\n",
    "\n",
    "    z_name = z.name\n",
    "    if z_name is None:\n",
    "        z_name = 'anon_z'\n",
    "\n",
    "    batch_size, ch, zr, zc = z.shape\n",
    "\n",
    "    r, c = pool_shape\n",
    "\n",
    "    zpart = []\n",
    "\n",
    "    mx = None\n",
    "\n",
    "    if top_down is None:\n",
    "        t = 0.\n",
    "    else:\n",
    "        t = - top_down\n",
    "        t.name = 'neg_top_down'\n",
    "\n",
    "    for i in xrange(r):\n",
    "        zpart.append([])\n",
    "        for j in xrange(c):\n",
    "            cur_part = z[:, :, i:zr:r, j:zc:c]\n",
    "            if z_name is not None:\n",
    "                cur_part.name = z_name + '[%d,%d]' % (i, j)\n",
    "            zpart[i].append(cur_part)\n",
    "            if mx is None:\n",
    "                mx = T.maximum(t, cur_part)\n",
    "                if cur_part.name is not None:\n",
    "                    mx.name = 'max(-top_down,' + cur_part.name + ')'\n",
    "            else:\n",
    "                max_name = None\n",
    "                if cur_part.name is not None:\n",
    "                    mx_name = 'max(' + cur_part.name + ',' + mx.name + ')'\n",
    "                mx = T.maximum(mx, cur_part)\n",
    "                mx.name = mx_name\n",
    "    mx.name = 'local_max(' + z_name + ')'\n",
    "\n",
    "    pt = []\n",
    "\n",
    "    for i in xrange(r):\n",
    "        pt.append([])\n",
    "        for j in xrange(c):\n",
    "            z_ij = zpart[i][j]\n",
    "            safe = z_ij - mx\n",
    "            safe.name = 'safe_z(%s)' % z_ij.name\n",
    "            cur_pt = T.exp(safe)\n",
    "            cur_pt.name = 'pt(%s)' % z_ij.name\n",
    "            pt[-1].append(cur_pt)\n",
    "\n",
    "    off_pt = T.exp(t - mx)\n",
    "    off_pt.name = 'p_tilde_off(%s)' % z_name\n",
    "    denom = off_pt\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            denom = denom + pt[i][j]\n",
    "    denom.name = 'denom(%s)' % z_name\n",
    "\n",
    "    off_prob = off_pt / denom\n",
    "    p = 1. - off_prob\n",
    "    p.name = 'p(%s)' % z_name\n",
    "\n",
    "    hpart = []\n",
    "    for i in xrange(r):\n",
    "        hpart.append([pt_ij / denom for pt_ij in pt[i]])\n",
    "\n",
    "    h = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            h.name = 'h_interm'\n",
    "            h = T.set_subtensor(h[:, :, i:zr:r, j:zc:c], hpart[i][j])\n",
    "\n",
    "    h.name = 'h(%s)' % z_name\n",
    "\n",
    "    if theano_rng is None:\n",
    "        return p, h\n",
    "    \n",
    "    ### --------------------- DONE IF NO SAMPLES ARE GENERATED ---------------------------###\n",
    "    else:\n",
    "        events = []\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                events.append(hpart[i][j])\n",
    "        events.append(off_prob)\n",
    "\n",
    "        events = [event.dimshuffle(0, 1, 2, 3, 'x') for event in events]\n",
    "\n",
    "        events = tuple(events)\n",
    "\n",
    "        stacked_events = T.concatenate(events, axis=4)\n",
    "\n",
    "        rows = zr // pool_shape[0]\n",
    "        cols = zc // pool_shape[1]\n",
    "        outcomes = pool_shape[0] * pool_shape[1] + 1\n",
    "        assert stacked_events.ndim == 5\n",
    "        for se, bs, r, c, chv in get_debug_values(stacked_events, batch_size,\n",
    "                                                  rows, cols, ch):\n",
    "            assert se.shape[0] == bs\n",
    "            assert se.shape[1] == r\n",
    "            assert se.shape[2] == c\n",
    "            assert se.shape[3] == chv\n",
    "            assert se.shape[4] == outcomes\n",
    "        reshaped_events = stacked_events.reshape((\n",
    "            batch_size * rows * cols * ch, outcomes))\n",
    "\n",
    "        multinomial = theano_rng.multinomial(pvals=reshaped_events,\n",
    "                                             dtype=p.dtype)\n",
    "\n",
    "        reshaped_multinomial = multinomial.reshape((batch_size, ch, rows,\n",
    "                                                    cols, outcomes))\n",
    "\n",
    "        h_sample = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "        idx = 0\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                h_sample = T.set_subtensor(h_sample[:, :, i:zr:r, j:zc:c],\n",
    "                                           reshaped_multinomial[:, :, :, :,\n",
    "                                           idx])\n",
    "                idx += 1\n",
    "\n",
    "        p_sample = 1 - reshaped_multinomial[:, :, :, :, -1]\n",
    "\n",
    "        return p, h, p_sample, h_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizing theano to do it all on the GPU\n",
    "We don't want only the convolutions to happen on the GPU because that means that every time we perform a convolution, the whole information is transferred to the GPU and back after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "\n",
    "class CRBM:\n",
    "\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.alphabet = _alphabet\n",
    "        self.initializeMotifs()\n",
    "        \n",
    "        # cRBM parameters\n",
    "        self.bias = theano.shared(value=np.random.rand(self.numMotifs), name='bias', borrow=True)\n",
    "        self.c = theano.shared(value=np.array([random.random()]), name='c', borrow=True)\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.theano_rng = RandomStreams(self.rng.randint(2**30))\n",
    "    \n",
    "    \n",
    "    def initializeMotifs (self):\n",
    "        # create random kernel\n",
    "        x = np.random.rand(self.numMotifs, 2, 4, self.motifLength).astype(np.float32)\n",
    "        \n",
    "        # create reverse complement\n",
    "        for i in range(self.numMotifs):\n",
    "            x[i,1] = np.flipud(np.fliplr(x[i,0]))\n",
    "            \n",
    "        self.motifs = theano.shared(value=x, name='W', borrow=True)\n",
    "        \n",
    "        \n",
    "    def setCustomKernels (self, customKernels):\n",
    "        if len(customKernels.shape) != 4:\n",
    "            print \"New motifs must be a 4D matrix with dims: (K x numOfStrands(2) x numOfLetters(4) x numOfKMers)\"\n",
    "            return\n",
    "        \n",
    "        self.numMotifs = customKernels.shape[0]\n",
    "        self.motifLength = customKernels.shape[3]\n",
    "        self.bias = theano.shared(value=np.random.rand(self.numMotifs), name='bias', borrow=True)\n",
    "        self.motifs = theano.shared(value=customKernels.astype(np.float32))\n",
    "        print \"New motifs set. # Motifs: \" + str(self.numMotifs) + \" K-mer-Length: \" + str(self.motifLength)\n",
    "\n",
    "        \n",
    "### ------------------------------THE TOUGH STUFF-------------------------------- ###\n",
    "### ----------------------------------------------------------------------------- ###\n",
    "\n",
    "    def forwardBatch (self, data):\n",
    "        out = conv.conv2d(data, self.motifs)[:,:,::-1,::-1] # flip, because conv reverts H\n",
    "        bMod = self.bias.dimshuffle('x', 0, 'x', 'x') # add dims to the bias until it works\n",
    "        pooled = max_pool(out + bMod, pool_shape=(1, self.poolingFactor), theano_rng=self.theano_rng)\n",
    "        return [pooled[1], pooled[3]] #only return pooled layer and probs\n",
    "\n",
    "\n",
    "    def backwardBatch (self, H_sample):\n",
    "        # theano convolution call\n",
    "        #K_star = self.motifs.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "        C = conv.conv2d(H_sample, self.motifs[:,:,:,::-1], border_mode='full')\n",
    "        out = T.sum(C, axis=1) # sum over all K\n",
    "        #out = out + self.c\n",
    "        \n",
    "        # add fourth dimension (the strands) that was lost during forward pass\n",
    "        res = T.tile(self.softmax(out).dimshuffle(0,'x',1,2),[1,2,1,1])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def gradient (self, hiddenProbs, data):\n",
    "        mean = T.mean(hiddenProbs, axis=0, keepdims=True) # sum over all samples to get avg (but keep dim)\n",
    "        mean = T.tile(mean, [2,1,1,1])\n",
    "        H_reshaped = mean.dimshuffle(1, 0, 2, 3)\n",
    "        out = conv.conv2d(data, H_reshaped)\n",
    "        return T.mean(out, axis=0)\n",
    "\n",
    "    \n",
    "    def batchTraining (self, data, epochs, batchSize, numOfCDs):\n",
    "        \n",
    "        # calculate the data gradient for weights (motifs) and bias\n",
    "        D_batch = T.tensor4('data')\n",
    "        [H_data, S_data] = self.forwardBatch(D_batch)\n",
    "        G_data = self.gradient(H_data, D_batch)\n",
    "        bias_data = T.mean(T.sum(H_data, axis=3), axis=0)\n",
    "        \n",
    "        # calculate the model gradient (only CD-1 for now)\n",
    "        V_model = self.backwardBatch(S_data)\n",
    "        S_v = self.sampleVisibleLayer(V_model)\n",
    "        [H_model, S_model] = self.forwardBatch(S_v)\n",
    "        G_model = self.gradient(H_model, D_batch)\n",
    "        bias_model = T.mean(T.sum(H_model, axis=3), axis=0)\n",
    "        \n",
    "        self.motifs = self.motifs + self.learningRate * (G_data - G_model)\n",
    "        self.bias = self.bias + self.learningRate * (bias_data - bias_model)\n",
    "        \n",
    "        training = theano.function([D_batch], [self.motifs,self.bias], allow_input_downcast=True)\n",
    "        \n",
    "        print \"START\"\n",
    "        \n",
    "        # after we got the theano function for everything, let's apply it!\n",
    "        itPerEpoch = data.shape[0] / batchSize\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(itPerEpoch):\n",
    "                print batch*batchSize, (batch+1)*batchSize\n",
    "                training = training(data[batch*batchSize:(batch+1)*batchSize])\n",
    "            print \"Epoch done: \" + str(epoch)\n",
    "        \n",
    "        return training\n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "    def sampleVisibleLayer (self, V):\n",
    "        S = self.theano_rng.multinomial(n=1,pvals=V.dimshuffle(0,1,3,2)).dimshuffle(0,1,3,2)\n",
    "        S = S.astype('float32')\n",
    "        return S\n",
    "    \n",
    "    def softmax (self, x):\n",
    "        return T.exp(x) / T.exp(x).sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2, 2, 4, 8)\n",
      "Data mat shape: (1000, 2, 4, 150)\n",
      "START\n",
      "0 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[0].shape[3] = 2, input[1].shape[3] = 1)\nApply node that caused the error: Elemwise{maximum,no_inplace}(max(anon_z[0,1],max(-top_down,anon_z[0,0])), anon_z[0,2])\nInputs types: [TensorType(float64, 4D), TensorType(float64, 4D)]\nInputs shapes: [(1, 2, 1, 2), (1, 2, 1, 1)]\nInputs strides: [(32, 16, 16, 8), (80, 40, 40, 24)]\nInputs values: [array([[[[ 3.39141388,  3.36073847]],\n\n        [[ 5.65961594,  6.90420098]]]]), array([[[[ 3.77698393]],\n\n        [[ 5.4668097 ]]]])]\n\nBacktrace when the node is created:\n  File \"<ipython-input-7-93a9be5a9de2>\", line 120, in max_pool\n    mx = T.maximum(mx, cur_part)\n\nDebugprint of the apply node: \nElemwise{maximum,no_inplace} [@A] <TensorType(float64, 4D)> 'local_max(anon_z)'   \n\nStorage map footprint:\n - W, Shape: (2, 2, 4, 4), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{3}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{2}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - data, Shape: (1, 2, 4, 8), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0.10000000149}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - max(anon_z[0,1],max(-top_down,anon_z[0,0])), Shape: (1, 2, 1, 2), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - TensorConstant{0.10000000149}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - anon_z[0,0], Shape: (1, 2, 1, 2), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - anon_z[0,1], Shape: (1, 2, 1, 2), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - h_interm, Shape: (1, 2, 1, 5), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - anon_z[0,2], Shape: (1, 2, 1, 1), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{4}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - <RandomStateType>, ElemSize: 64 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0.0}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - bias, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - <RandomStateType>, ElemSize: 64 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - DimShuffle{x,x,x}.0, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-df941c96f124>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Data mat shape: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataMat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Result from training: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-dc831cdeca56>\u001b[0m in \u001b[0;36mbatchTraining\u001b[1;34m(self, data, epochs, batchSize, numOfCDs)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitPerEpoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Epoch done: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input dimension mis-match. (input[0].shape[3] = 2, input[1].shape[3] = 1)\nApply node that caused the error: Elemwise{maximum,no_inplace}(max(anon_z[0,1],max(-top_down,anon_z[0,0])), anon_z[0,2])\nInputs types: [TensorType(float64, 4D), TensorType(float64, 4D)]\nInputs shapes: [(1, 2, 1, 2), (1, 2, 1, 1)]\nInputs strides: [(32, 16, 16, 8), (80, 40, 40, 24)]\nInputs values: [array([[[[ 3.39141388,  3.36073847]],\n\n        [[ 5.65961594,  6.90420098]]]]), array([[[[ 3.77698393]],\n\n        [[ 5.4668097 ]]]])]\n\nBacktrace when the node is created:\n  File \"<ipython-input-7-93a9be5a9de2>\", line 120, in max_pool\n    mx = T.maximum(mx, cur_part)\n\nDebugprint of the apply node: \nElemwise{maximum,no_inplace} [@A] <TensorType(float64, 4D)> 'local_max(anon_z)'   \n\nStorage map footprint:\n - W, Shape: (2, 2, 4, 4), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{3}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{2}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - data, Shape: (1, 2, 4, 8), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0.10000000149}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - max(anon_z[0,1],max(-top_down,anon_z[0,0])), Shape: (1, 2, 1, 2), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - TensorConstant{0.10000000149}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - anon_z[0,0], Shape: (1, 2, 1, 2), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - anon_z[0,1], Shape: (1, 2, 1, 2), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - h_interm, Shape: (1, 2, 1, 5), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - anon_z[0,2], Shape: (1, 2, 1, 1), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{4}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - <RandomStateType>, ElemSize: 64 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (1,), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0.0}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Subtensor{int64}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - bias, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - <RandomStateType>, ElemSize: 64 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{3}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{2}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - DimShuffle{x,x,x}.0, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{-1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n\n"
     ]
    }
   ],
   "source": [
    "theano.config.exception_verbosity='high'\n",
    "theano.config.optimizer='None'\n",
    "theano.config.compute_test_value='ignore'\n",
    "\n",
    "x = CRBM(4,2,0.1, 3)\n",
    "kernel1 = np.tile(np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]]), [2,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0],[0,0,0],[1,1,1],[0,0,0]]), [2,1,1])\n",
    "#kernel1 = np.tile(np.eye(4), [2,1,1])\n",
    "#kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1])\n",
    "kernel = np.array([kernel1, kernel2])\n",
    "filter_shape = kernel.shape\n",
    "\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2])\n",
    "print \"Data shape: \" + str(data.shape)\n",
    "#x.setCustomKernels(kernel)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print \"Data mat shape: \" + str(dataMat.shape)\n",
    "res = x.batchTraining(data, 1, 1, 10)\n",
    "print \"Result from training: \"\n",
    "print res[0]\n",
    "print res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct a kernel matrix, looking for ACGT and GGGG\n",
    "kernel1 = np.tile(np.eye(4), [2,1,1]) # the kernel will only look for ACGT\n",
    "kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1]) # this one looks for GGGG\n",
    "kernel2[1,:,:] = np.array([[0,0,0,0],[1,1,1,1],[0,0,0,0],[0,0,0,0]])\n",
    "kernel3 = np.tile(np.zeros((4,4)), [2,1,1])\n",
    "kernel = np.array([kernel1, kernel2, kernel3])\n",
    "print kernel.shape\n",
    "print x.motifs.shape\n",
    "# create toy sequences where we might be able to see what happens\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2])\n",
    "\n",
    "print data.shape\n",
    "print dataMat.shape\n",
    "# initialize the learner, insert our predefined kernels and train\n",
    "cRBM = ConvRBM_slow (4, 3, 0.1, 1)\n",
    "cRBM.setCustomKernels(kernel)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "cRBM.batchTraining(data, 10, 2, 1)\n",
    "print cRBM.motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the algorithm is doing something meaningful\n",
    "We can now apply the learning algorithm on a toy sequence to see what happens.\n",
    "Important questions to ask:\n",
    "\n",
    "* Does the hidden layer somehow make sense?\n",
    "* What does the sigmoid function do with it?\n",
    "* How does the *reconstruction* look like?\n",
    "* Is the softmax doing the right thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n",
      "(2, 4, 4)\n",
      "New motifs set. # Motifs: 3 K-mer-Length: 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the filter stack size (2) and image stack size (1) differ\nApply node that caused the error: ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern', None),('bsize', None),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', None),('unroll_kern', None),('unroll_patch', True),('imshp_logical', (None, None, None)),('kshp_logical', (None, None)),('kshp_logical_top_aligned', True)}(data, kernels)\nInputs types: [TensorType(float32, 4D), TensorType(float32, 4D)]\nInputs shapes: [(2, 1, 4, 8), (3, 2, 4, 4)]\nInputs strides: [(128, 128, 32, 4), (128, 64, 16, 4)]\nInputs values: ['not shown', 'not shown']\n\nBacktrace when the node is created:\n  File \"<ipython-input-189-8b88baef28e9>\", line 82, in forwardBatch\n    out = conv.conv2d(D,K)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-83c195f39f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobMaxPooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Result from forward pass (that is: P(H | V))\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Shape: -> \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-189-8b88baef28e9>\u001b[0m in \u001b[0;36mforwardBatch\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mbMod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# add dims to the bias until it works\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotifs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbMod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: the filter stack size (2) and image stack size (1) differ\nApply node that caused the error: ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern', None),('bsize', None),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', None),('unroll_kern', None),('unroll_patch', True),('imshp_logical', (None, None, None)),('kshp_logical', (None, None)),('kshp_logical_top_aligned', True)}(data, kernels)\nInputs types: [TensorType(float32, 4D), TensorType(float32, 4D)]\nInputs shapes: [(2, 1, 4, 8), (3, 2, 4, 4)]\nInputs strides: [(128, 128, 32, 4), (128, 64, 16, 4)]\nInputs values: ['not shown', 'not shown']\n\nBacktrace when the node is created:\n  File \"<ipython-input-189-8b88baef28e9>\", line 82, in forwardBatch\n    out = conv.conv2d(D,K)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# Construct a kernel matrix, looking for ACGT and GGGG\n",
    "kernel1 = np.tile(np.eye(4), [2,1,1]) # the kernel will only look for ACGT\n",
    "kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1]) # this one looks for GGGG\n",
    "kernel2[1,:,:] = np.array([[0,0,0,0],[1,1,1,1],[0,0,0,0],[0,0,0,0]])\n",
    "kernel3 = np.tile(np.zeros((4,4)), [2,1,1])\n",
    "print kernel3.shape\n",
    "print kernel2.shape\n",
    "kernel = np.array([kernel1, kernel2, kernel3])\n",
    "\n",
    "# create toy sequences where we might be able to see what happens\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2])\n",
    "\n",
    "# initialize the learner and insert our predefined kernels\n",
    "cRBM = ConvRBM_slow (4, 3, 0.1, 5)\n",
    "cRBM.setCustomKernels(kernel)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "res = cRBM.probMaxPooling(cRBM.forwardBatch(data))\n",
    "print \"Result from forward pass (that is: P(H | V))\"\n",
    "print \"Shape: -> \" + str(res.shape)\n",
    "print res\n",
    "\n",
    "vis = cRBM.softmax(cRBM.backwardBatch(res))\n",
    "print \"Result from backward pass (that is: P(V | H))\"\n",
    "print \"Shape: -> \" + str(vis.shape)\n",
    "print vis\n",
    "grad = cRBM.gradient(res, data)\n",
    "print \"Result from gradient calculation\"\n",
    "print \"Shape: -> \" + str(grad.shape)\n",
    "print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some performance tests on the test set\n",
    "Now, let's test it on the whole test set. The performance obtained here, should tell us a lot on how fast we can actually train the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the filter stack size (2) and image stack size (1) differ\nApply node that caused the error: ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern', None),('bsize', None),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', None),('unroll_kern', None),('unroll_patch', True),('imshp_logical', (None, None, None)),('kshp_logical', (None, None)),('kshp_logical_top_aligned', True)}(data, kernels)\nInputs types: [TensorType(float32, 4D), TensorType(float32, 4D)]\nInputs shapes: [(1000, 1, 4, 150), (10, 2, 4, 7)]\nInputs strides: [(2400, 2400, 600, 4), (224, 112, 28, 4)]\nInputs values: ['not shown', 'not shown']\n\nBacktrace when the node is created:\n  File \"<ipython-input-189-8b88baef28e9>\", line 82, in forwardBatch\n    out = conv.conv2d(D,K)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-f0232ba2c5c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# perform forward and backward pass and calculate the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobMaxPooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataMat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackwardBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataMat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-189-8b88baef28e9>\u001b[0m in \u001b[0;36mforwardBatch\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mbMod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# add dims to the bias until it works\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotifs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbMod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: the filter stack size (2) and image stack size (1) differ\nApply node that caused the error: ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern', None),('bsize', None),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', None),('unroll_kern', None),('unroll_patch', True),('imshp_logical', (None, None, None)),('kshp_logical', (None, None)),('kshp_logical_top_aligned', True)}(data, kernels)\nInputs types: [TensorType(float32, 4D), TensorType(float32, 4D)]\nInputs shapes: [(1000, 1, 4, 150), (10, 2, 4, 7)]\nInputs strides: [(2400, 2400, 600, 4), (224, 112, 28, 4)]\nInputs values: ['not shown', 'not shown']\n\nBacktrace when the node is created:\n  File \"<ipython-input-189-8b88baef28e9>\", line 82, in forwardBatch\n    out = conv.conv2d(D,K)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "cRBM = ConvRBM_slow (7, 10, 0.1, 3)\n",
    "# perform forward and backward pass and calculate the gradient\n",
    "start = time.time()\n",
    "res = cRBM.probMaxPooling(cRBM.forwardBatch(dataMat))\n",
    "vis = cRBM.softmax(cRBM.backwardBatch(res))\n",
    "grad = cRBM.gradient(res, dataMat)\n",
    "\n",
    "print res[0,0,:,:]\n",
    "print \"Forward pass: \" + str(res.shape)\n",
    "print \"Backward pass: \" + str(vis.shape)\n",
    "print \"Gradient: \" + str(grad.shape)\n",
    "print \"Time for processing (for, back, grad) of \" + str(dataMat.shape[0]) + \" sequences: \" + str((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del cRBM, test_set, allSeqs, pwms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2b: Implementing the cRBM\n",
    "We now have all the tools we need to actually implement the cRBM.\n",
    "Some notes on constraints and behavior of our learning algorithm:\n",
    "\n",
    "* It uses Theano to do most (if not all) of the work. That means it's ways faster on a GPU (but not as fast as possible)\n",
    "* It expects kernels (filters/motifs) to be of the same length. While this choice was made due to performance issues, it's not a problem because we expect the algorithm to combine multiple motifs if nessessary.\n",
    "* The sequences (DHSes) are also expected to be of the same size\n",
    "\n",
    "Now, finally some hints on usage:\n",
    "\n",
    "* The pooling factor has to be evenly dividable by the length of the hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class implements a cRBM for sequence analysis.\n",
    "It does perform efficient forward and backward pass of any given DNA sequence.\n",
    "It implements softmax and sigmoid functions as activation and performs probabilistic max pooling\n",
    "after the convolution step.\n",
    "So this class is basically a two layer network with a convolution layer and a pooling layer on top\n",
    "of that.\n",
    "The learning procedure uses contrastive divergence (CD) with a variable amount of steps.\n",
    "\"\"\"\n",
    "class ConvRBM_slow:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the cRBM. The parameters here are global params that should not change\n",
    "    during the execution of training or testing and characterize the network.\n",
    "    \n",
    "    Parameters:\n",
    "    _motifLength:    How long are the motifs (position weight matrices PWM). This\n",
    "                     This is equivalent to ask what the number of k-mers is.\n",
    "                     The current approach only deals with one fixed motif length.\n",
    "                     \n",
    "    _numMotifs:      How many motifs are applied to the sequence, that is how many\n",
    "                     hidden units does the network have. Each hidden unit consists\n",
    "                     of a vector of size (sequenceLength-motifLength+1)\n",
    "                     \n",
    "    _poolingFactor:  How many units from the hidden layer are pooled together.\n",
    "                     Note that the number has to divide evenly to the length of\n",
    "                     the hidden units, that is:\n",
    "                     mod(sequenceLength-motifLength+1, poolingFactor) == 0\n",
    "                     (1 = equivalent to sigmoid activation)\n",
    "                     \n",
    "    _alphabet:       Biopython uses alphabets for sequences to do sanity checks.\n",
    "                     However, all of the code is written for DNA sequences and even\n",
    "                     though in theory there should be no difference between that\n",
    "                     and other alphabets, Biopython may have trouble with the convolution.\n",
    "    \"\"\"\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.alphabet = _alphabet\n",
    "        self.initializeMotifs()\n",
    "        \n",
    "        # cRBM parameters\n",
    "        self.bias = np.random.rand(self.numMotifs)\n",
    "        self.c = random.random()\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def initializeMotifs (self):\n",
    "        x = np.random.rand(self.numMotifs, 2, 4, self.motifLength)\n",
    "        for i in range(self.numMotifs):\n",
    "            x[i,1] = np.flipud(np.fliplr(x[i,0]))\n",
    "        self.motifs = x\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Calculate the forward pass for any given set of sequences, that is calculate P(H | V).\n",
    "    This method applies convolution of all filters to all sequences in the set, using theano.\n",
    "    It also looks on both strands for matches and returns the hidden activation layer.\n",
    "    \n",
    "    Parameters:\n",
    "    data:            The DNA sequences to calculate the forward pass on. The data is a 4D matrix\n",
    "                     with dimensionality: (N_batch x numOfStrands(2) x numOfLetters(4) x N_v)\n",
    "                     \n",
    "    Return:\n",
    "    The function returns the hidden activation layer as numpy matrix.\n",
    "    That matrix has dimensionality N_batch x K x 1 x N_h where\n",
    "    K is the number of kernels (motifs/PWMs) that are applied and\n",
    "    N_h is the length of the hidden layer (convolution is of type 'valid')\n",
    "    \n",
    "    Note that the strandness is lost during this process because it's added up in the hidden layer.\n",
    "    \"\"\"\n",
    "    def forwardBatch (self, data):\n",
    "        # create 4D tensor for theano (BatchSize x K x 2*numOfLetters x lenOfSeqs)\n",
    "        D = T.tensor4('data')\n",
    "        K = T.tensor4('kernels')\n",
    "        out = conv.conv2d(D,K)\n",
    "        f = theano.function([D,K], out, allow_input_downcast=True)\n",
    "        \n",
    "        bMod = self.bias[np.newaxis,:,np.newaxis,np.newaxis] # add dims to the bias until it works\n",
    "        \n",
    "        return f(data, self.motifs) + bMod\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the backward pass on the data, that is P(V | H).\n",
    "    It does so by performing convolution of the hidden layer and the motifs for each kernel using theano.\n",
    "    \n",
    "    Parameters:\n",
    "    hidden layer:   The layer that was previously computed by the forward pass.\n",
    "    \n",
    "    Return:\n",
    "    A matrix of dimensionality: N_batch x K x numOfLetters(4) x N_v\n",
    "    The result can be interpreted as the probability for each position to have a specific letter\n",
    "    once it is summed over all K (sum over second dimension).\n",
    "    That means, the combination of summing over K and applying softmax can be interpreted as P(V | H).\n",
    "    \"\"\"\n",
    "    def backwardBatch (self, hiddenActivation):\n",
    "        # theano convolution call\n",
    "        H = T.tensor4('hidden')\n",
    "        K = T.tensor4('kernels')\n",
    "        K_star = K.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "        C = conv.conv2d(H, K_star, border_mode='full')\n",
    "        out = T.sum(C, axis=1) # sum over all K\n",
    "        f = theano.function([H,K], out, allow_input_downcast=True)\n",
    "\n",
    "        res = f(hiddenActivation, self.motifs) + self.c\n",
    "        \n",
    "        # add fourth dimension (the strands) that was lost during forward pass (max pooling)\n",
    "        res = np.tile(res[:,np.newaxis,:,:], [1,2,1,1])\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Set new kernels when you don't wish to start with random ones. Can be used if prior knowledge about\n",
    "    the structure of the sequences is present.\n",
    "    \n",
    "    Parameters:\n",
    "    customKernels:  A matrix of shape (K x numOfStrands(2) x numOfLetters(4) x num-of-k-mers) containing the\n",
    "                    new kernels to work with.\n",
    "                    \n",
    "    Return:\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "    def setCustomKernels (self, customKernels):\n",
    "        if len(customKernels.shape) != 4:\n",
    "            print \"New motifs must be a 4D matrix with dims: (K x numOfStrands(2) x numOfLetters(4) x numOfKMers)\"\n",
    "            return\n",
    "        \n",
    "        self.numMotifs = customKernels.shape[0]\n",
    "        self.motifLength = customKernels.shape[3]\n",
    "        self.bias = np.random.rand(self.numMotifs)\n",
    "        self.motifs = customKernels\n",
    "        print \"New motifs set. # Motifs: \" + str(self.numMotifs) + \" K-mer-Length: \" + str(self.motifLength)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the gradient for a given data matrix and samples from the hidden layer.\n",
    "    The gradient is computed using theano and convolution.\n",
    "    \n",
    "    Parameters:\n",
    "    hiddenProbs:    A matrix of shape (N_batch x K x numOfLetters(4) x N_v) containing probabilities\n",
    "                    from the prob. distribution of the hidden layer.\n",
    "                    Note that this matrix is obtained as result from forwardBatch().\n",
    "                    \n",
    "    data:           A matrix of shape (N_batch x numOfStrands(2) x numOfLetters(4) x N_v) containing\n",
    "                    the data. Note that this matrix is usually the same as the matrix passed to forwardBatch().\n",
    "                    \n",
    "    Return:\n",
    "    A matrix containing the gradient for all kernels/motifs/pwms.\n",
    "    This matrix is of shape (N_batch x K x 1 x num-of-k-mers)\n",
    "    \"\"\"\n",
    "    def gradient (self, hiddenProbs, data):\n",
    "        H = T.tensor4('hidden')\n",
    "        S = T.tensor4('sample')\n",
    "        H_reshaped = H.dimshuffle(1, 0, 2, 3)\n",
    "        out = conv.conv2d(S, H_reshaped)\n",
    "        f = theano.function([H,S], out, allow_input_downcast=True)\n",
    "\n",
    "        res = f(np.tile(np.mean(hiddenProbs, axis=0), [2,1,1,1]), np.tile(data, [1,1,1,1]))\n",
    "        return np.mean(res, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    def batchTraining (self, data, epochs, batchSize, numOfCDs):\n",
    "        itPerEpoch = data.shape[0] / batchSize\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(itPerEpoch):\n",
    "                D_batch = data[batch*batchSize:(batch+1)*batchSize]\n",
    "                H = self.probMaxPooling(self.forwardBatch(D_batch))\n",
    "                G_data = self.gradient(H, D_batch) # gradient for W for the data\n",
    "                bias_data = np.mean(np.sum(H, axis=3), axis=0) # gradient for bias\n",
    "                c_data = np.mean(np.sum(np.sum(np.sum(D_batch, axis=3), axis=2), axis=1), axis=0)\n",
    "                print \"C_data: \" + str(c_data)\n",
    "                for cd in range(numOfCDs):\n",
    "                    S = self.sampleFromMatrix(H)\n",
    "                    V = self.softmax(self.backwardBatch(S))\n",
    "                    S_v = self.sampleFromMatrix(V)\n",
    "                    H = self.probMaxPooling(self.forwardBatch(S_v))\n",
    "                \n",
    "                G_model = self.gradient(H, D_batch)\n",
    "                bias_model = np.sum(np.sum(H, axis=3), axis=0)\n",
    "                c_model = np.mean(np.sum(np.sum(np.sum(V, axis=3), axis=2), axis=1), axis=0)\n",
    "                print \"C_model: \" + str(c_model)\n",
    "                #print \"Gradient of model: \" + str((G_data-G_model).shape)\n",
    "                #print \"For data: \" + str((batch*batchSize, (batch+1)*batchSize))\n",
    "                #print G_data-G_model\n",
    "                \n",
    "                self.motifs = self.motifs + self.learningRate * (G_data - G_model)\n",
    "                self.bias = self.bias + self.learningRate * (bias_data - bias_model).sum(axis=1)\n",
    "                self.c = self.c + self.learningRate * (c_data - c_model)\n",
    "                \n",
    "            print \"Epoch done: \" + str(epoch)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Calculates the probabilistic max pooling layer (P) from a hidden layer H.\n",
    "    P is sparse because only every poolingFactor'th unit can be active while all the other\n",
    "    units are forced to zero. The one that becomes active is the one with the highest softmax\n",
    "    activation from H.\n",
    "    Parameters:\n",
    "    H:              Hidden layer obtained by forwardBatch function.\n",
    "    \n",
    "    Return:\n",
    "    A layer of the same shape as H, but sparsed out.\n",
    "    \"\"\"\n",
    "    def probMaxPooling (self, H):\n",
    "\n",
    "        # first of all some easy definitions\n",
    "        n_h = H.shape[3]\n",
    "        numOfGroups = n_h/self.poolingFactor\n",
    "        #print \"N_H = \" + str(n_h) + \" numOfGroups: \" + str(numOfGroups) + \" poolingFactor: \" + str(self.poolingFactor)\n",
    "\n",
    "        # exponentiate it all\n",
    "        exp = np.exp(H)\n",
    "        \n",
    "        # get the right dimensions for the matrix\n",
    "        dims = (exp.shape[0], exp.shape[1], self.poolingFactor, numOfGroups)\n",
    "        reshaped = exp.reshape(dims)\n",
    "        \n",
    "        # append ones (to have the 5th, or non-active state), get the denominator and divide by it\n",
    "        withOnes = np.insert(reshaped, self.poolingFactor, np.ones(numOfGroups), 2)\n",
    "        denom = np.sum(withOnes, axis=2)\n",
    "        div = withOnes / denom[:,:,np.newaxis,:]\n",
    "        \n",
    "        # now calculate argmax & max and insert into P\n",
    "        P = np.zeros(div.shape)\n",
    "        idx = np.argmax(div, axis=2)\n",
    "        maxes = np.max(div, axis=2)\n",
    "        \n",
    "        # TODO: this is not performant!!!\n",
    "        for sample in range(idx.shape[0]):\n",
    "            for kernel in range(idx.shape[1]):\n",
    "                for seqPos in range(idx.shape[2]):\n",
    "                    P[sample,kernel,idx[sample,kernel,seqPos],seqPos] = maxes[sample, kernel, seqPos]\n",
    "\n",
    "        # cut out the 5th extra dimension\n",
    "        P = P[:,:,0:reshaped.shape[2],:]\n",
    "        return np.reshape(P,(H.shape[0],H.shape[1],1,-1), 'F')\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Maybe not working...\n",
    "    \"\"\"\n",
    "    def sampleFromMatrix (self, M):\n",
    "        boolean_mat = M > self.rng.random_sample(M.shape)\n",
    "        return boolean_mat.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the sigmoid activation for the hidden layer.\n",
    "    \"\"\"\n",
    "    def sigmoid(self, _H):\n",
    "        return (1.0 / (1.0 + np.exp(-_H)))\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the softmax activation using numpy. This function was designed to work with the\n",
    "    backward pass well.\n",
    "    Using it for the probabilistic max pooling might not be recommended.\n",
    "    Parameters:\n",
    "    V:              The hidden layer calculated so far. That is, the result from convolution\n",
    "                    when calling backwardBatch().\n",
    "                    The matrix is of shape N_batch x K x numOfLetters(4) x N_v\n",
    "    Return:\n",
    "    A matrix of the same shape and size, but each column for the letters (3rd dim) has been softmaxed.\n",
    "    \"\"\"\n",
    "    def softmax (self, V):\n",
    "        exp = np.exp(-V) # exponentiate it all\n",
    "        denominator = np.sum(exp, axis=1) # this the sum of all the exp(-x)\n",
    "        # now expand the denominator such that the division works\n",
    "        denominator = denominator[:,np.newaxis,:].repeat(exp.shape[1], axis=1)\n",
    "        div = exp / denominator\n",
    "        return div\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
