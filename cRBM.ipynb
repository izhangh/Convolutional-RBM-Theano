{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a convolutional RBM using Theano\n",
    "\n",
    "This notebook implements an efficient cRBM with various training procedures and different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Importing theano, Biopython and all the other useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import Bio.SeqIO as sio\n",
    "import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Getting and transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFASTASequencesFromFile (filename):\n",
    "    dhsSequences = []\n",
    "    for dhs in sio.parse(open(filename), 'fasta', IUPAC.unambiguous_dna):\n",
    "        dhsSequences.append(dhs.seq)\n",
    "    return dhsSequences\n",
    "\n",
    "def readPWMsFromFile (filename):\n",
    "    matrices = []\n",
    "    for mat in motifs.parse(open(filename), 'jaspar'):\n",
    "        matrices.append(mat.pwm)\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in FASTA sequences\n",
    "These sequences are DNase1-hypersensitivity sites where each sample represents one DHS.\n",
    "They were all already brought to the same dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allSeqs = readFASTASequencesFromFile('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwms = readPWMsFromFile('../data/jaspar_matrices.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test set\n",
    "Since working with these huge amounts of sequences is very time consuming, just start with a selection of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test set: 1000\n"
     ]
    }
   ],
   "source": [
    "test_set = [allSeqs[random.randrange(0,len(allSeqs))] for i in range(1000)]\n",
    "print \"Length of test set: \"  + str(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert test set sequences to matrices we can work with\n",
    "Each sequence is converted to a matrix of dimensionality (2 x 4 x N_v), representing:\n",
    "* the two strands (first dimension)\n",
    "* the four letters of the DNA alphabet (second dimension)\n",
    "* the sequence represented by 1 (letter is present at this position) or 0 (letter is not present) (third dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    m = len(seq.alphabet.letters)\n",
    "    n = len(seq)\n",
    "    result = np.zeros((2, m, n))\n",
    "    for i in range(len(seq)):\n",
    "        result[0,getIntToLetter(seq[i]),i] = 1\n",
    "        result[1,getIntToLetter(seq[i]),i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of sequence to matrix for the test set in (in ms): 283.47492218\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataMat = np.array([getMatrixFromSeq(t) for t in test_set])\n",
    "print \"Conversion of sequence to matrix for the test set in (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing the cRBM\n",
    "We now have all the tools we need to actually implement the cRBM.\n",
    "Some notes on constraints and behavior of our learning algorithm:\n",
    "\n",
    "* It uses Theano to do most (if not all) of the work. That means it's ways faster on a GPU\n",
    "* It expects kernels (filters/motifs) to be of the same length. While this choice was made due to performance issues, it's not a problem because we expect the algorithm to combine multiple motifs if nessessary.\n",
    "* The sequences (DHSes) are also expected to be of the same size\n",
    "\n",
    "Now, finally some hints on usage:\n",
    "\n",
    "* The pooling factor has to be evenly dividable by the length of the hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class implements a cRBM for sequence analysis.\n",
    "It does perform efficient forward and backward pass of any given DNA sequence.\n",
    "It implements softmax and sigmoid functions as activation and performs probabilistic max pooling\n",
    "after the convolution step.\n",
    "So this class is basically a two layer network with a convolution layer and a pooling layer on top\n",
    "of that.\n",
    "The learning procedure uses contrastive divergence (CD) with a variable amount of steps.\n",
    "\"\"\"\n",
    "class ConvRBM:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the cRBM. The parameters here are global params that should not change\n",
    "    during the execution of training or testing and characterize the network.\n",
    "    \n",
    "    Parameters:\n",
    "    _motifLength:    How long are the motifs (position weight matrices PWM). This\n",
    "                     This is equivalent to ask what the number of k-mers is.\n",
    "                     The current approach only deals with one fixed motif length.\n",
    "                     \n",
    "    _numMotifs:      How many motifs are applied to the sequence, that is how many\n",
    "                     hidden units does the network have. Each hidden unit consists\n",
    "                     of a vector of size (sequenceLength-motifLength+1)\n",
    "                     \n",
    "    _poolingFactor:  How many units from the hidden layer are pooled together.\n",
    "                     Note that the number has to divide evenly to the length of\n",
    "                     the hidden units, that is:\n",
    "                     mod(sequenceLength-motifLength+1, poolingFactor) == 0\n",
    "                     (1 = equivalent to sigmoid activation)\n",
    "                     \n",
    "    _alphabet:       Biopython uses alphabets for sequences to do sanity checks.\n",
    "                     However, all of the code is written for DNA sequences and even\n",
    "                     though in theory there should be no difference between that\n",
    "                     and other alphabets, Biopython may have trouble with the convolution.\n",
    "    \"\"\"\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.alphabet = _alphabet\n",
    "        self.initializeMotifs()\n",
    "        \n",
    "        # cRBM parameters\n",
    "        self.bias = np.random.rand(self.numMotifs)\n",
    "        self.c = random.random()\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def initializeMotifs (self):\n",
    "        x = np.random.rand(self.numMotifs, 2, 4, self.motifLength)\n",
    "        for i in range(self.numMotifs):\n",
    "            x[i,1] = np.flipud(np.fliplr(x[i,0]))\n",
    "        self.motifs = x\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Calculate the forward pass for any given set of sequences, that is calculate P(H | V).\n",
    "    This method applies convolution of all filters to all sequences in the set, using theano.\n",
    "    It also looks on both strands for matches and returns the hidden activation layer.\n",
    "    \n",
    "    Parameters:\n",
    "    data:            The DNA sequences to calculate the forward pass on. The data is a 4D matrix\n",
    "                     with dimensionality: (N_batch x numOfStrands(2) x numOfLetters(4) x N_v)\n",
    "                     \n",
    "    Return:\n",
    "    The function returns the hidden activation layer as numpy matrix.\n",
    "    That matrix has dimensionality N_batch x K x 1 x N_h where\n",
    "    K is the number of kernels (motifs/PWMs) that are applied and\n",
    "    N_h is the length of the hidden layer (convolution is of type 'valid')\n",
    "    \n",
    "    Note that the strandness is lost during this process because it's added up in the hidden layer.\n",
    "    \"\"\"\n",
    "    def forwardBatch (self, data):\n",
    "        # create 4D tensor for theano (BatchSize x K x 2*numOfLetters x lenOfSeqs)\n",
    "        D = T.tensor4('data')\n",
    "        K = T.tensor4('kernels')\n",
    "        out = conv.conv2d(D,K)\n",
    "        f = theano.function([D,K], out, allow_input_downcast=True)\n",
    "        \n",
    "        bMod = self.bias[np.newaxis,:,np.newaxis,np.newaxis] # add dims to the bias until it works\n",
    "        \n",
    "        return f(data, self.motifs) + bMod\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the backward pass on the data, that is P(V | H).\n",
    "    It does so by performing convolution of the hidden layer and the motifs for each kernel using theano.\n",
    "    \n",
    "    Parameters:\n",
    "    hidden layer:   The layer that was previously computed by the forward pass.\n",
    "    \n",
    "    Return:\n",
    "    A matrix of dimensionality: N_batch x K x numOfLetters(4) x N_v\n",
    "    The result can be interpreted as the probability for each position to have a specific letter\n",
    "    once it is summed over all K (sum over second dimension).\n",
    "    That means, the combination of summing over K and applying softmax can be interpreted as P(V | H).\n",
    "    \"\"\"\n",
    "    def backwardBatch (self, hiddenActivation):\n",
    "        # theano convolution call\n",
    "        H = T.tensor4('hidden')\n",
    "        K = T.tensor4('kernels')\n",
    "        K_star = K.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "        C = conv.conv2d(H, K_star, border_mode='full')\n",
    "        out = T.sum(C, axis=1) # sum over all K\n",
    "        f = theano.function([H,K], out, allow_input_downcast=True)\n",
    "\n",
    "        res = f(hiddenActivation, self.motifs) + self.c\n",
    "        \n",
    "        # add fourth dimension (the strands) that was lost during forward pass (max pooling)\n",
    "        res = np.tile(res[:,np.newaxis,:,:], [1,2,1,1])\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Set new kernels when you don't wish to start with random ones. Can be used if prior knowledge about\n",
    "    the structure of the sequences is present.\n",
    "    \n",
    "    Parameters:\n",
    "    customKernels:  A matrix of shape (K x numOfStrands(2) x numOfLetters(4) x num-of-k-mers) containing the\n",
    "                    new kernels to work with.\n",
    "                    \n",
    "    Return:\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "    def setCustomKernels (self, customKernels):\n",
    "        if len(customKernels.shape) != 4:\n",
    "            print \"New motifs must be a 4D matrix with dims: (K x numOfStrands(2) x numOfLetters(4) x numOfKMers)\"\n",
    "            return\n",
    "        \n",
    "        self.numMotifs = customKernels.shape[0]\n",
    "        self.motifLength = customKernels.shape[3]\n",
    "        self.bias = np.random.rand(self.numMotifs)\n",
    "        self.motifs = customKernels\n",
    "        print \"New motifs set. # Motifs: \" + str(self.numMotifs) + \" K-mer-Length: \" + str(self.motifLength)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the gradient for a given data matrix and samples from the hidden layer.\n",
    "    The gradient is computed using theano and convolution.\n",
    "    \n",
    "    Parameters:\n",
    "    hiddenProbs:    A matrix of shape (N_batch x K x numOfLetters(4) x N_v) containing probabilities\n",
    "                    from the prob. distribution of the hidden layer.\n",
    "                    Note that this matrix is obtained as result from forwardBatch().\n",
    "                    \n",
    "    data:           A matrix of shape (N_batch x numOfStrands(2) x numOfLetters(4) x N_v) containing\n",
    "                    the data. Note that this matrix is usually the same as the matrix passed to forwardBatch().\n",
    "                    \n",
    "    Return:\n",
    "    A matrix containing the gradient for all kernels/motifs/pwms.\n",
    "    This matrix is of shape (N_batch x K x 1 x num-of-k-mers)\n",
    "    \"\"\"\n",
    "    def gradient (self, hiddenProbs, data):\n",
    "        H = T.tensor4('hidden')\n",
    "        S = T.tensor4('sample')\n",
    "        H_reshaped = H.dimshuffle(1, 0, 2, 3)\n",
    "        out = conv.conv2d(S, H_reshaped)\n",
    "        f = theano.function([H,S], out, allow_input_downcast=True)\n",
    "\n",
    "        res = f(np.tile(np.mean(hiddenProbs, axis=0), [2,1,1,1]), np.tile(data, [1,1,1,1]))\n",
    "        return np.mean(res, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    def batchTraining (self, data, epochs, batchSize, numOfCDs):\n",
    "        itPerEpoch = data.shape[0] / batchSize\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(itPerEpoch):\n",
    "                D_batch = data[batch*batchSize:(batch+1)*batchSize]\n",
    "                H = self.probMaxPooling(self.forwardBatch(D_batch))\n",
    "                G_data = self.gradient(H, D_batch)\n",
    "                for cd in range(numOfCDs):\n",
    "                    S = self.sampleFromMatrix(H)\n",
    "                    V = self.softmax(self.backwardBatch(S))\n",
    "                    S_v = self.sampleFromMatrix(V)\n",
    "                    H = self.probMaxPooling(self.forwardBatch(S_v))\n",
    "                \n",
    "                G_model = self.gradient(H, D_batch)\n",
    "\n",
    "                #print \"Gradient of model: \" + str((G_data-G_model).shape)\n",
    "                #print \"For data: \" + str((batch*batchSize, (batch+1)*batchSize))\n",
    "                #print G_data-G_model\n",
    "                self.motifs = self.motifs + self.learningRate * (G_data - G_model)\n",
    "                \n",
    "            print \"Epoch done: \" + str(epoch)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Calculates the probabilistic max pooling layer (P) from a hidden layer H.\n",
    "    P is sparse because only every poolingFactor'th unit can be active while all the other\n",
    "    units are forced to zero. The one that becomes active is the one with the highest softmax\n",
    "    activation from H.\n",
    "    Parameters:\n",
    "    H:              Hidden layer obtained by forwardBatch function.\n",
    "    \n",
    "    Return:\n",
    "    A layer of the same shape as H, but sparsed out.\n",
    "    \"\"\"\n",
    "    def probMaxPooling (self, H):\n",
    "\n",
    "        # first of all some easy definitions\n",
    "        n_h = H.shape[3]\n",
    "        numOfGroups = n_h/self.poolingFactor\n",
    "        #print \"N_H = \" + str(n_h) + \" numOfGroups: \" + str(numOfGroups) + \" poolingFactor: \" + str(self.poolingFactor)\n",
    "\n",
    "        # exponentiate it all\n",
    "        exp = np.exp(H)\n",
    "        \n",
    "        # get the right dimensions for the matrix\n",
    "        dims = (exp.shape[0], exp.shape[1], self.poolingFactor, numOfGroups)\n",
    "        reshaped = exp.reshape(dims)\n",
    "        \n",
    "        # append ones (to have the 5th, or non-active state), get the denominator and divide by it\n",
    "        withOnes = np.insert(reshaped, self.poolingFactor, np.ones(numOfGroups), 2)\n",
    "        denom = np.sum(withOnes, axis=2)\n",
    "        div = withOnes / denom[:,:,np.newaxis,:]\n",
    "        \n",
    "        # now calculate argmax & max and insert into P\n",
    "        P = np.zeros(div.shape)\n",
    "        idx = np.argmax(div, axis=2)\n",
    "        maxes = np.max(div, axis=2)\n",
    "        \n",
    "        # TODO: this is not performant!!!\n",
    "        for sample in range(idx.shape[0]):\n",
    "            for kernel in range(idx.shape[1]):\n",
    "                for seqPos in range(idx.shape[2]):\n",
    "                    P[sample,kernel,idx[sample,kernel,seqPos],seqPos] = maxes[sample, kernel, seqPos]\n",
    "\n",
    "        # cut out the 5th extra dimension\n",
    "        P = P[:,:,0:reshaped.shape[2],:]\n",
    "        return np.reshape(P,(H.shape[0],H.shape[1],1,-1), 'F')\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Maybe not working...\n",
    "    \"\"\"\n",
    "    def sampleFromMatrix (self, M):\n",
    "        boolean_mat = M > self.rng.random_sample(M.shape)\n",
    "        return boolean_mat.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the sigmoid activation for the hidden layer.\n",
    "    \"\"\"\n",
    "    def sigmoid(self, _H):\n",
    "        return (1.0 / (1.0 + np.exp(-_H)))\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the softmax activation using numpy. This function was designed to work with the\n",
    "    backward pass well.\n",
    "    Using it for the probabilistic max pooling might not be recommended.\n",
    "    Parameters:\n",
    "    V:              The hidden layer calculated so far. That is, the result from convolution\n",
    "                    when calling backwardBatch().\n",
    "                    The matrix is of shape N_batch x K x numOfLetters(4) x N_v\n",
    "    Return:\n",
    "    A matrix of the same shape and size, but each column for the letters (3rd dim) has been softmaxed.\n",
    "    \"\"\"\n",
    "    def softmax (self, V):\n",
    "        exp = np.exp(-V) # exponentiate it all\n",
    "        denominator = np.sum(exp, axis=1) # this the sum of all the exp(-x)\n",
    "        # now expand the denominator such that the division works\n",
    "        denominator = denominator[:,np.newaxis,:].repeat(exp.shape[1], axis=1)\n",
    "        div = exp / denominator\n",
    "        return div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mat shape: (1000, 2, 4, 150)\n",
      "Epoch done: 0\n",
      "Epoch done: 1\n",
      "Epoch done: 2\n",
      "Epoch done: 3\n",
      "Epoch done: 4\n",
      "Epoch done: 5\n",
      "Epoch done: 6\n",
      "Epoch done: 7\n",
      "Epoch done: 8\n",
      "Epoch done: 9\n",
      "Result from training: \n",
      "[[[[-0.10123912  0.55427602  0.41431642 -0.03443159]\n",
      "   [ 0.2602303   0.69098109  0.12820424  0.24371101]\n",
      "   [ 0.34385368 -0.14884399  0.23140761  0.46342825]\n",
      "   [-0.06803943 -0.17771981 -0.01285306 -0.39939569]]\n",
      "\n",
      "  [[-0.70479356 -0.52250442 -0.75576243 -0.48251517]\n",
      "   [-0.09675839 -0.23586488 -0.43017437 -0.14925616]\n",
      "   [-0.04847198 -0.38548182  0.17918553 -0.05271527]\n",
      "   [-0.66703762  0.11454077  0.13507012 -0.67108625]]]\n",
      "\n",
      "\n",
      " [[[ 0.3243244   0.12182661 -0.07499545 -0.17341926]\n",
      "   [ 0.18539755  0.57782378 -0.12611428  0.46086844]\n",
      "   [-0.22307444  0.27865996  0.62987008  0.27234713]\n",
      "   [-0.1545353  -0.10274256  0.30628093 -0.26107212]]\n",
      "\n",
      "  [[-0.56646999 -0.20337043 -0.68078518 -0.56901103]\n",
      "   [-0.28783951  0.16259759 -0.00267042 -0.71618429]\n",
      "   [ 0.16868545 -0.63980034  0.06602822 -0.12754802]\n",
      "   [-0.80602529 -0.3747711  -0.29737929 -0.24552273]]]]\n"
     ]
    }
   ],
   "source": [
    "x = ConvRBM(4,2,0.1, 3)\n",
    "print \"Data mat shape: \" + str(dataMat.shape)\n",
    "x.batchTraining(dataMat, 10, 100, 10)\n",
    "print \"Result from training: \"\n",
    "print x.motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 1 K-mer-Length: 4\n",
      "H: (1, 1, 1, 5)\n",
      "[[[[ 0.99211325  0.          0.          0.          0.        ]]]]\n",
      "Sample: [[[[1 0 0 0 0]]]]\n"
     ]
    }
   ],
   "source": [
    "k = np.array([np.tile(np.eye(4), [2,1,1])]) # the kernel will only look for ACGT\n",
    "V = np.array([getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))])\n",
    "x.setCustomKernels(k)\n",
    "H = x.probMaxPooling(x.forwardBatch(V))\n",
    "print \"H: \" + str(H.shape)\n",
    "print H\n",
    "S = x.sampleFromMatrix(H)\n",
    "print \"Sample: \" + str(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the algorithm is doing something meaningful\n",
    "We can now apply the learning algorithm on a toy sequence to see what happens.\n",
    "Important questions to ask:\n",
    "\n",
    "* Does the hidden layer somehow make sense?\n",
    "* What does the sigmoid function do with it?\n",
    "* How does the *reconstruction* look like?\n",
    "* Is the softmax doing the right thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 4)\n",
      "(2, 4, 4)\n",
      "New motifs set. # Motifs: 3 K-mer-Length: 4\n",
      "Result from forward pass (that is: P(H | V))\n",
      "Shape: -> (2, 3, 1, 5)\n",
      "[[[[ 0.99  0.    0.    0.    0.  ]]\n",
      "\n",
      "  [[ 0.    0.    0.    0.    0.44]]\n",
      "\n",
      "  [[ 0.18  0.    0.    0.    0.  ]]]\n",
      "\n",
      "\n",
      " [[[ 0.5   0.    0.    0.    0.  ]]\n",
      "\n",
      "  [[ 0.2   0.    0.    0.    0.  ]]\n",
      "\n",
      "  [[ 0.18  0.    0.    0.    0.  ]]]]\n",
      "Result from backward pass (that is: P(V | H))\n",
      "Shape: -> (2, 2, 4, 8)\n",
      "[[[[ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]]\n",
      "\n",
      "  [[ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]]]\n",
      "\n",
      "\n",
      " [[[ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]]\n",
      "\n",
      "  [[ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]\n",
      "   [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5]]]]\n",
      "Result from gradient calculation\n",
      "Shape: -> (3, 4, 4)\n",
      "[[[ 0.75  0.    0.    0.  ]\n",
      "  [ 0.    0.75  0.    0.  ]\n",
      "  [ 0.75  0.75  1.49  0.75]\n",
      "  [ 0.    0.    0.    0.75]]\n",
      "\n",
      " [[ 0.54  0.    0.    0.  ]\n",
      "  [ 0.    0.54  0.    0.  ]\n",
      "  [ 0.1   0.1   0.64  0.1 ]\n",
      "  [ 0.    0.    0.    0.54]]\n",
      "\n",
      " [[ 0.18  0.    0.    0.  ]\n",
      "  [ 0.    0.18  0.    0.  ]\n",
      "  [ 0.18  0.18  0.36  0.18]\n",
      "  [ 0.    0.    0.    0.18]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2,4,4) (3,4,4) (3,2,4,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-6aa28b79aaa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Shape: -> \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mcRBM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmotifs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2,4,4) (3,4,4) (3,2,4,4) "
     ]
    }
   ],
   "source": [
    "# Construct a kernel matrix, looking for ACGT and GGGG\n",
    "kernel1 = np.tile(np.eye(4), [2,1,1]) # the kernel will only look for ACGT\n",
    "kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1]) # this one looks for GGGG\n",
    "kernel2[1,:,:] = np.array([[0,0,0,0],[1,1,1,1],[0,0,0,0],[0,0,0,0]])\n",
    "kernel3 = np.tile(np.zeros((4,4)), [2,1,1])\n",
    "print kernel3.shape\n",
    "print kernel2.shape\n",
    "kernel = np.array([kernel1, kernel2, kernel3])\n",
    "\n",
    "# create toy sequences where we might be able to see what happens\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2])\n",
    "\n",
    "# initialize the learner and insert our predefined kernels\n",
    "cRBM = ConvRBM (4, 3, 0.1, 5)\n",
    "cRBM.setCustomKernels(kernel)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "for i in range(3):\n",
    "    res = cRBM.probMaxPooling(cRBM.forwardBatch(data))\n",
    "    print \"Result from forward pass (that is: P(H | V))\"\n",
    "    print \"Shape: -> \" + str(res.shape)\n",
    "    print res\n",
    "\n",
    "    vis = cRBM.softmax(cRBM.backwardBatch(res))\n",
    "    print \"Result from backward pass (that is: P(V | H))\"\n",
    "    print \"Shape: -> \" + str(vis.shape)\n",
    "    print vis\n",
    "    grad = cRBM.gradient(res, data)\n",
    "    print \"Result from gradient calculation\"\n",
    "    print \"Shape: -> \" + str(grad.shape)\n",
    "    print grad\n",
    "    cRBM.motifs += grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some performance tests on the test set\n",
    "Now, let's test it on the whole test set. The performance obtained here, should tell us a lot on how fast we can actually train the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_H = 144 numOfGroups: 48 poolingFactor: 3\n",
      "[[ 0.    0.66  0.    0.    0.81  0.    0.    0.    0.73  0.    0.59  0.    0.\n",
      "   0.72  0.    0.    0.75  0.    0.    0.45  0.    0.    0.46  0.    0.    0.\n",
      "   0.49  0.    0.    0.59  0.    0.    0.56  0.    0.    0.69  0.    0.\n",
      "   0.82  0.    0.    0.79  0.    0.    0.49  0.    0.    0.47  0.    0.\n",
      "   0.43  0.    0.    0.59  0.54  0.    0.    0.    0.4   0.    0.53  0.    0.\n",
      "   0.    0.    0.78  0.    0.    0.85  0.    0.    0.36  0.    0.    0.8\n",
      "   0.    0.    0.64  0.    0.    0.78  0.    0.    0.79  0.    0.84  0.    0.\n",
      "   0.57  0.    0.    0.57  0.    0.    0.    0.56  0.    0.62  0.    0.\n",
      "   0.91  0.    0.    0.67  0.    0.    0.    0.39  0.    0.71  0.    0.\n",
      "   0.75  0.    0.    0.47  0.    0.52  0.    0.    0.52  0.    0.    0.    0.\n",
      "   0.59  0.    0.    0.49  0.    0.    0.56  0.56  0.    0.    0.55  0.    0.\n",
      "   0.37  0.    0.    0.    0.    0.53]]\n",
      "Forward pass: (1000, 10, 1, 144)\n",
      "Backward pass: (1000, 2, 4, 150)\n",
      "Gradient: (10, 4, 7)\n",
      "Time for processing (for, back, grad) of 1000 sequences: 0.555987119675\n"
     ]
    }
   ],
   "source": [
    "cRBM = ConvRBM (7, 10, 0.1, 3)\n",
    "# perform forward and backward pass and calculate the gradient\n",
    "start = time.time()\n",
    "res = cRBM.probMaxPooling(cRBM.forwardBatch(dataMat))\n",
    "vis = cRBM.softmax(cRBM.backwardBatch(res))\n",
    "grad = cRBM.gradient(res, dataMat)\n",
    "\n",
    "print res[0,0,:,:]\n",
    "print \"Forward pass: \" + str(res.shape)\n",
    "print \"Backward pass: \" + str(vis.shape)\n",
    "print \"Gradient: \" + str(grad.shape)\n",
    "print \"Time for processing (for, back, grad) of \" + str(dataMat.shape[0]) + \" sequences: \" + str((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del cRBM, test_set, allSeqs, pwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
