{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional RBM (cRBM)\n",
    "\n",
    "This notebook takes care of implementing the basic functionality for cRBMs.\n",
    "Or maybe it's just for the preliminaries, that is some simple stuff before it actually comes to the Boltzmann Machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading the data and converting it to various forms of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Theano imports\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RS\n",
    "from theano import pp\n",
    "\n",
    "# numpy and python classics\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "# biopython stuff\n",
    "import Bio.SeqIO as sio\n",
    "import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes to read biological files (such as FASTA or JASPAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class reads sequences from fasta files.\n",
    "To use it, create an instance of that object and use\n",
    "the function readSequencesFromFile.\n",
    "\"\"\"\n",
    "class FASTAReader:\n",
    "    \n",
    "    def __init__(self, _path):\n",
    "        self.path = _path\n",
    "        \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        dhsSequences = []\n",
    "        for dhs in sio.parse(open(filename), 'fasta', IUPAC.unambiguous_dna):\n",
    "            dhsSequences.append(dhs.seq)\n",
    "        return dhsSequences\n",
    "    \n",
    "    \n",
    "class JASPARReader:\n",
    "    \n",
    "    def __init__ (self):\n",
    "        pass\n",
    "    \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        matrices = []\n",
    "        for mat in motifs.parse(open(filename), 'jaspar'):\n",
    "            matrices.append(mat.pwm)\n",
    "        return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matReader = JASPARReader()\n",
    "pwms = matReader.readSequencesFromFile('data/jaspar_matrices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the two classes to calculate a forward pass in our algorithm\n",
    "seqReader = FASTAReader('.')\n",
    "allSeqs = seqReader.readSequencesFromFile('data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 15000\n",
      "Test set size: 5000\n"
     ]
    }
   ],
   "source": [
    "data = [allSeqs[random.randrange(0,len(allSeqs))] for i in range(20000)]\n",
    "train_set, test_set = train_test_split(data, test_size=0.25)\n",
    "print \"Training set size: \" + str(len(train_set))\n",
    "print \"Test set size: \" + str(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of test set in (in ms): 2945.70684433\n"
     ]
    }
   ],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    m = len(seq.alphabet.letters)\n",
    "    n = len(seq)\n",
    "    result = np.zeros((1, m, n))\n",
    "    revSeq = seq.reverse_complement()\n",
    "    for i in range(len(seq)):\n",
    "        result[0,getIntToLetter(seq[i]),i] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "trainingData = np.array([getMatrixFromSeq(t) for t in train_set])\n",
    "testingData = np.array([getMatrixFromSeq(t) for t in test_set])\n",
    "print \"Conversion of test set in (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Borrowing Ian Goodfellow's implementation of the probabilistic max pooling layer\n",
    "This implementation is now part of the pylearn2 library which is licensed under the 3-claused BSD license.\n",
    "Source code is available here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/expr/probabilistic_max_pooling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from theano.gof.op import get_debug_values\n",
    "\n",
    "def max_pool(z, pool_shape, top_down=None, theano_rng=None):\n",
    "    \"\"\"\n",
    "    Probabilistic max-pooling\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : theano 4-tensor\n",
    "        a theano 4-tensor representing input from below\n",
    "    pool_shape : tuple\n",
    "        tuple of ints. the shape of regions to be pooled\n",
    "    top_down : theano 4-tensor, optional\n",
    "        a theano 4-tensor representing input from above\n",
    "        if None, assumes top-down input is 0\n",
    "    theano_rng : MRG_RandomStreams, optional\n",
    "        Used for random numbers for sampling\n",
    "    Returns\n",
    "    -------\n",
    "    p : theano 4-tensor\n",
    "        the expected value of the pooling layer p\n",
    "    h : theano 4-tensor\n",
    "        the expected value of the detector layer h\n",
    "    p_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the pooling layer\n",
    "    h_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the detector layer\n",
    "    Notes\n",
    "    ------\n",
    "    all 4-tensors are formatted with axes ('b', 'c', 0, 1).\n",
    "    This is for maximum speed when using theano's conv2d\n",
    "    to generate z and top_down, or when using it to infer conditionals of\n",
    "    other layers using the return values.\n",
    "    Detailed description:\n",
    "    Suppose you have a variable h that lives in a Conv2DSpace h_space and\n",
    "    you want to pool it down to a variable p that lives in a smaller\n",
    "    Conv2DSpace p.\n",
    "    This function does that, using non-overlapping pools.\n",
    "    Specifically, consider one channel of h. h must have a height that is a\n",
    "    multiple of pool_shape[0] and a width that is a multiple of pool_shape[1].\n",
    "    A channel of h can thus be broken down into non-overlapping rectangles\n",
    "    of shape pool_shape.\n",
    "    Now consider one rectangular pooled region within one channel of h.\n",
    "    I now use 'h' to refer just to this rectangle, and 'p' to refer to\n",
    "    just the one pooling unit associated with that rectangle.\n",
    "    We assume that the space that h and p live in is constrained such\n",
    "    that h and p are both binary and p = max(h). To reduce the state-space\n",
    "    in order to make probabilistic computations cheaper we also\n",
    "    constrain sum(h) <= 1.\n",
    "    Suppose h contains k different units. Suppose that the only term\n",
    "    in the model's energy function involving h is -(z*h).sum()\n",
    "    (elemwise multiplication) and the only term in\n",
    "    the model's energy function involving p is -(top_down*p).sum().\n",
    "    Then P(h[i] = 1) = softmax( [ z[1], z[2], ..., z[k], -top_down] )[i]\n",
    "    and P(p = 1) = 1-softmax( [z[1], z[2], ..., z[k], -top_down])[k]\n",
    "    This variation of the function assumes that z, top_down, and all\n",
    "    return values use Conv2D axes ('b', 'c', 0, 1).\n",
    "    This variation of the function implements the softmax using a\n",
    "    theano graph of exp, maximum, sub, and div operations.\n",
    "    Performance notes:\n",
    "    It might be possible to make a faster implementation with different\n",
    "    theano ops. rather than using set_subtensor, it might be possible\n",
    "    to use the stuff in theano.sandbox.neighbours. Probably not possible,\n",
    "    or at least nasty, because that code isn't written with multiple\n",
    "    channels in mind, and I don't think just a reshape can fix it.\n",
    "    Some work on this in galatea.cond.neighbs.py\n",
    "    At some point images2neighbs' gradient was broken so check that\n",
    "    it has been fixed before sinking too much time into this.\n",
    "    Stabilizing the softmax is also another source of slowness.\n",
    "    Here it is stabilized with several calls to maximum and sub.\n",
    "    It might also be possible to stabilize it with\n",
    "    T.maximum(-top_down,T.signal.downsample.max_pool(z)).\n",
    "    Don't know if that would be faster or slower.\n",
    "    Elsewhere in this file I implemented the softmax with a reshape\n",
    "    and call to Softmax / SoftmaxWithBias.\n",
    "    This is slower, even though Softmax is faster on the GPU than the\n",
    "    equivalent max/sub/exp/div graph. Maybe the reshape is too expensive.\n",
    "    Benchmarks show that most of the time is spent in GpuIncSubtensor\n",
    "    when running on gpu. So it is mostly that which needs a faster\n",
    "    implementation. One other way to implement this would be with\n",
    "    a linear.Conv2D.lmul_T, where the convolution stride is equal to\n",
    "    the pool width, and the thing to multiply with is the hparts stacked\n",
    "    along the channel axis. Unfortunately, conv2D doesn't work right\n",
    "    with stride > 2 and is pretty slow for stride 2. Conv3D is used to\n",
    "    mitigate some of this, but only has CPU code.\n",
    "    \"\"\"\n",
    "\n",
    "    z_name = z.name\n",
    "    if z_name is None:\n",
    "        z_name = 'anon_z'\n",
    "\n",
    "    batch_size, ch, zr, zc = z.shape\n",
    "\n",
    "    r, c = pool_shape\n",
    "\n",
    "    zpart = []\n",
    "\n",
    "    mx = None\n",
    "\n",
    "    if top_down is None:\n",
    "        t = 0.\n",
    "    else:\n",
    "        t = - top_down\n",
    "        t.name = 'neg_top_down'\n",
    "\n",
    "    for i in xrange(r):\n",
    "        zpart.append([])\n",
    "        for j in xrange(c):\n",
    "            cur_part = z[:, :, i:zr:r, j:zc:c]\n",
    "            if z_name is not None:\n",
    "                cur_part.name = z_name + '[%d,%d]' % (i, j)\n",
    "            zpart[i].append(cur_part)\n",
    "            if mx is None:\n",
    "                mx = T.maximum(t, cur_part)\n",
    "                if cur_part.name is not None:\n",
    "                    mx.name = 'max(-top_down,' + cur_part.name + ')'\n",
    "            else:\n",
    "                max_name = None\n",
    "                if cur_part.name is not None:\n",
    "                    mx_name = 'max(' + cur_part.name + ',' + mx.name + ')'\n",
    "                mx = T.maximum(mx, cur_part)\n",
    "                mx.name = mx_name\n",
    "    mx.name = 'local_max(' + z_name + ')'\n",
    "\n",
    "    pt = []\n",
    "\n",
    "    for i in xrange(r):\n",
    "        pt.append([])\n",
    "        for j in xrange(c):\n",
    "            z_ij = zpart[i][j]\n",
    "            safe = z_ij - mx\n",
    "            safe.name = 'safe_z(%s)' % z_ij.name\n",
    "            cur_pt = T.exp(safe)\n",
    "            cur_pt.name = 'pt(%s)' % z_ij.name\n",
    "            pt[-1].append(cur_pt)\n",
    "\n",
    "    off_pt = T.exp(t - mx)\n",
    "    off_pt.name = 'p_tilde_off(%s)' % z_name\n",
    "    denom = off_pt\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            denom = denom + pt[i][j]\n",
    "    denom.name = 'denom(%s)' % z_name\n",
    "\n",
    "    off_prob = off_pt / denom\n",
    "    p = 1. - off_prob\n",
    "    p.name = 'p(%s)' % z_name\n",
    "\n",
    "    hpart = []\n",
    "    for i in xrange(r):\n",
    "        hpart.append([pt_ij / denom for pt_ij in pt[i]])\n",
    "\n",
    "    h = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            h.name = 'h_interm'\n",
    "            h = T.set_subtensor(h[:, :, i:zr:r, j:zc:c], hpart[i][j])\n",
    "\n",
    "    h.name = 'h(%s)' % z_name\n",
    "\n",
    "    if theano_rng is None:\n",
    "        return p, h\n",
    "    \n",
    "    ### --------------------- DONE IF NO SAMPLES ARE GENERATED ---------------------------###\n",
    "    else:\n",
    "        events = []\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                events.append(hpart[i][j])\n",
    "        events.append(off_prob)\n",
    "\n",
    "        events = [event.dimshuffle(0, 1, 2, 3, 'x') for event in events]\n",
    "\n",
    "        events = tuple(events)\n",
    "\n",
    "        stacked_events = T.concatenate(events, axis=4)\n",
    "\n",
    "        rows = zr // pool_shape[0]\n",
    "        cols = zc // pool_shape[1]\n",
    "        outcomes = pool_shape[0] * pool_shape[1] + 1\n",
    "        assert stacked_events.ndim == 5\n",
    "        for se, bs, r, c, chv in get_debug_values(stacked_events, batch_size,\n",
    "                                                  rows, cols, ch):\n",
    "            assert se.shape[0] == bs\n",
    "            assert se.shape[1] == r\n",
    "            assert se.shape[2] == c\n",
    "            assert se.shape[3] == chv\n",
    "            assert se.shape[4] == outcomes\n",
    "        reshaped_events = stacked_events.reshape((\n",
    "            batch_size * rows * cols * ch, outcomes))\n",
    "\n",
    "        multinomial = theano_rng.multinomial(pvals=reshaped_events,\n",
    "                                             dtype=p.dtype)\n",
    "\n",
    "        reshaped_multinomial = multinomial.reshape((batch_size, ch, rows,\n",
    "                                                    cols, outcomes))\n",
    "\n",
    "        h_sample = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "        idx = 0\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                h_sample = T.set_subtensor(h_sample[:, :, i:zr:r, j:zc:c],\n",
    "                                           reshaped_multinomial[:, :, :, :,\n",
    "                                           idx])\n",
    "                idx += 1\n",
    "\n",
    "        p_sample = 1 - reshaped_multinomial[:, :, :, :, -1]\n",
    "\n",
    "        return p, h, p_sample, h_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PART 3: Optimizing theano to do it all on the GPU\n",
    "\n",
    "class CRBM:\n",
    "\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.alphabet = _alphabet\n",
    "        self.initializeMotifs()\n",
    "        \n",
    "        # cRBM parameters (2*x to respect both strands of the DNA)\n",
    "        b = np.random.rand(1, 2*self.numMotifs).astype(np.float32)\n",
    "        c = np.random.rand(1, 4).astype(np.float32)\n",
    "        self.bias = theano.shared(value=b, name='bias', borrow=True)\n",
    "        self.c = theano.shared(value=c, name='c', borrow=True)\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.theano_rng = RS(seed=1234)\n",
    "        self.params = [self.motifs, self.bias, self.c]\n",
    "    \n",
    "    \n",
    "    def initializeMotifs (self):\n",
    "        # create random motifs (2*self.numMotifs to respect both strands)\n",
    "        x = np.random.rand(2 * self.numMotifs, 1, 4, self.motifLength).astype(np.float32)\n",
    "        \n",
    "        # create reverse complement\n",
    "        for i in range(self.numMotifs):\n",
    "            x[self.numMotifs+i] = np.flipud(np.fliplr(x[self.numMotifs+i]))\n",
    "            \n",
    "        self.motifs = theano.shared(value=x, name='W', borrow=True)\n",
    "        \n",
    "        \n",
    "    def setCustomKernels (self, customKernels):\n",
    "        if len(customKernels.shape) != 4 or customKernels.shape[1] != 1:\n",
    "            print \"New motifs must be a 4D matrix with dims: (K x 1 x numOfLetters(4) x numOfKMers)\"\n",
    "            return\n",
    "        \n",
    "        self.numMotifs = customKernels.shape[0]\n",
    "        self.motifLength = customKernels.shape[3]\n",
    "        b = np.random.rand(1, self.numMotifs).astype(np.float32)\n",
    "        self.bias = theano.shared(value=b, name='bias', borrow=True)\n",
    "        \n",
    "        self.motifs = theano.shared(value=customKernels.astype(np.float32))\n",
    "        self.params = [self.motifs, self.bias, self.c]\n",
    "        print \"New motifs set. # Motifs: \" + str(self.numMotifs) + \" K-mer-Length: \" + str(self.motifLength)\n",
    "\n",
    "        \n",
    "### ------------------------------THE TOUGH STUFF-------------------------------- ###\n",
    "### ----------------------------------------------------------------------------- ###\n",
    "\n",
    "    def forwardBatch (self, data):\n",
    "        out = conv.conv2d(data, self.motifs)[:,:,::-1,::-1] # flip, because conv reverts H\n",
    "        out = out.mean(axis=2, keepdims=True)\n",
    "        bMod = self.bias\n",
    "        bMod = bMod.dimshuffle('x', 1, 0, 'x') # add dims to the bias until it works\n",
    "        out = out + bMod\n",
    "        pooled = max_pool(out.dimshuffle(0,2,1,3), pool_shape=(2, self.poolingFactor), theano_rng=self.theano_rng)\n",
    "        H = pooled[1]\n",
    "        S = pooled[3]\n",
    "        return [H,S] #only return pooled layer and probs\n",
    "\n",
    "\n",
    "    def backwardBatch (self, H_sample):\n",
    "        K = self.motifs.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "        H_shuffled = H_sample.dimshuffle(0, 2, 1, 3) # interpret the kernels as channels (will be summed automatically)\n",
    "        C = conv.conv2d(H_shuffled, K, border_mode='full')\n",
    "        out = T.sum(C, axis=1, keepdims=True) # sum over all K\n",
    "        c_bc = self.c\n",
    "        c_bc = c_bc.dimshuffle('x', 0, 1, 'x')\n",
    "        out = out + c_bc\n",
    "        \n",
    "        # add fourth dimension (the strands) that was lost during forward pass\n",
    "        res = self.softmax(out)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def gradient (self, hiddenProbs, data):\n",
    "        mean = T.mean(hiddenProbs, axis=0, keepdims=True) # sum over all training data to get avg (but keep dim)\n",
    "        H_reshaped = mean.dimshuffle(2, 0, 1, 3)\n",
    "        out = conv.conv2d(data, H_reshaped)\n",
    "        return T.mean(out, axis=0, keepdims=True).dimshuffle(1, 0, 2, 3) #mean over all training samples\n",
    "\n",
    "    \n",
    "    def performGibbsSampling (self, Sample_H):\n",
    "        #theano.printing.Print('Sample')(Sample_H)\n",
    "        V_model = self.backwardBatch(Sample_H)\n",
    "        S_v = self.sampleVisibleLayer(V_model)\n",
    "        [H_model, S_model] = self.forwardBatch(S_v)\n",
    "        return [H_model, S_model]\n",
    "        \n",
    "        \n",
    "    def getTrainingFunction (self, numOfCDs):\n",
    "        D = T.tensor4('data')\n",
    "        # calculate the data gradient for weights (motifs) and bias\n",
    "        [H_data, S_data] = self.forwardBatch(D)\n",
    "        \n",
    "        # calculate data gradients\n",
    "        G_motif_data = self.gradient(H_data, D)\n",
    "        G_bias_data = T.mean(T.sum(H_data, axis=3), axis=0)\n",
    "        G_c_data = T.mean(T.sum(D, axis=3), axis=0)\n",
    "        \n",
    "        # UNFORTUNATELY, THE SAMPLE FROM V CANNOT BE USED DURING SCAN...\n",
    "        # calculate the model gradient scan does that numOfCDs times)\n",
    "        #result, updates = theano.scan(fn = self.performGibbsSampling,\n",
    "        #                              outputs_info = [None, T.unbroadcast(S_data, 1)],\n",
    "        #                              n_steps = numOfCDs)\n",
    "        #H_model = result[-1][0] # we only want the last value here. Theano is smart and respects that while optimizing\n",
    "        #S_V_model = result[-1][1]\n",
    "        \n",
    "        # calculate model probs\n",
    "        S_H = S_data\n",
    "        for i in range(numOfCDs):\n",
    "            V_model = self.backwardBatch(S_H)\n",
    "            S_V_model = self.sampleVisibleLayer(V_model)\n",
    "            [H_model, S_H] = self.forwardBatch(S_V_model)\n",
    "        \n",
    "        # compute the model gradients\n",
    "        G_motif_model = self.gradient(H_model, D)\n",
    "        G_bias_model = T.mean(T.sum(H_model, axis=3), axis=0)\n",
    "        G_c_model = T.mean(T.sum(S_V_model, axis=3), axis=0)\n",
    "        \n",
    "        # update the parameters\n",
    "        new_motifs = self.motifs + self.learningRate * (G_motif_data - G_motif_model)\n",
    "        new_bias = self.bias + self.learningRate * (G_bias_data - G_bias_model)\n",
    "        new_c = self.c + self.learningRate * (G_c_data - G_c_model)\n",
    "        \n",
    "        #score = self.getDataReconstruction(D)\n",
    "        updates = [(self.motifs, new_motifs), (self.bias, new_bias), (self.c, new_c)]\n",
    "        fun = theano.function([D],\n",
    "                              [],\n",
    "                              updates = updates,\n",
    "                              allow_input_downcast=True,\n",
    "                              on_unused_input='ignore')\n",
    "        return fun\n",
    "    \n",
    "    \n",
    "    def trainMinibatch (self, trainData, testData, epochs, batchSize, numOfCDs):\n",
    "        \n",
    "        # assert that pooling can be done without rest to the division\n",
    "        assert (((trainData.shape[3] - self.motifLength + 1) % self.poolingFactor) == 0)\n",
    "        assert (((testData.shape[3] - self.motifLength + 1) % self.poolingFactor) == 0)\n",
    "        \n",
    "        itPerEpoch = trainData.shape[0] / batchSize\n",
    "        print \"BatchSize: \" + str(batchSize)\n",
    "        print \"Num of iterations per epoch: \" + str(itPerEpoch)\n",
    "        start = time.time()\n",
    "        trainingFun = self.getTrainingFunction(numOfCDs)\n",
    "        reconFun = self.getReconFun()\n",
    "        #reconstructionFun = self.getDataReconstructionFunction()\n",
    "        print \"Compilation of theano function finished in \" + str(time.time()-start) + \" seconds\"\n",
    "        print \"Start training...\"\n",
    "        start = time.time()\n",
    "        allScores = []\n",
    "        for epoch in range(epochs):\n",
    "            smallScores = []\n",
    "            for batch in range(itPerEpoch):\n",
    "                trainingFun(trainData[batch*batchSize:(batch+1)*batchSize])\n",
    "            allScores.append(reconFun(testData))\n",
    "        print \"Training finished after: \" + str(time.time()-start) + \" seconds!\"\n",
    "        return allScores\n",
    "\n",
    "    def getReconFun (self):\n",
    "        D = T.tensor4('data')\n",
    "        score = self.getDataReconstruction(D)\n",
    "        return theano.function([D], score, allow_input_downcast=True)\n",
    "    \n",
    "    \n",
    "    def getDataReconstruction (self, D):\n",
    "        [H, S_H] = self.forwardBatch(D)\n",
    "        V = self.backwardBatch(S_H)\n",
    "        S_V = self.sampleVisibleLayer(V)\n",
    "        diff = (T.argmax(D, axis=3) - T.argmax(S_V, axis=3))\n",
    "        diff = abs(diff)\n",
    "        score = diff.mean(axis=0) # mean over all training samples\n",
    "        score = score.sum(axis=2)\n",
    "        return score\n",
    "    \n",
    " \n",
    "    def sampleVisibleLayer (self, V):\n",
    "        reshaped = V.dimshuffle(0, 1, 3, 2).reshape((V.shape[0]*V.shape[3], V.shape[2]))\n",
    "        S_reshaped = self.theano_rng.multinomial(n=1,pvals=reshaped)\n",
    "        S = S_reshaped.reshape((V.shape[0], 1, V.shape[3], V.shape[2])).dimshuffle(0, 1, 3, 2)\n",
    "        S = S.astype('float32')\n",
    "        return S\n",
    "    \n",
    "    def softmax (self, x):\n",
    "        return T.exp(x) / T.exp(x).sum(axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4b: Create the theano functions neccessary for training (compile to C)\n",
    "Theano optimizes the function graph heavily when aiming for maximum performance. Thus, the package spends a long time setting up the whole system and not so much time with the actual training of the data.\n",
    "\n",
    "By generating the function in it's own cell, we can do training seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set some config parameters to make debugging simpler\n",
    "debug = True\n",
    "if debug:\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    theano.config.exception_verbosity='high'\n",
    "    theano.config.optimizer='None'\n",
    "    theano.config.compute_test_value='ignore'\n",
    "    theano.config.profile=True\n",
    "else:\n",
    "    theano.config.exception_verbosity='low'\n",
    "    theano.config.mode='FAST_RUN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel shape: (6, 1, 4, 3)\n",
      "Data shape: (2, 1, 4, 8)\n",
      "BatchSize: 2\n",
      "Num of iterations per epoch: 1\n",
      "Compilation of theano function finished in 3.17563605309 seconds\n",
      "Start training...\n",
      "Training finished after: 0.0766191482544 seconds!\n",
      "Training (with compilation) performed in: 3.26057887077 seconds.\n",
      "Result from training: \n",
      "----------------------\n",
      "Learned Motif: \n",
      "[[[[ 0.78  0.89  0.5 ]\n",
      "   [ 0.55  0.87  0.4 ]\n",
      "   [ 0.7   0.53  0.36]\n",
      "   [-0.    0.11  0.73]]]\n",
      "\n",
      "\n",
      " [[[-0.13  0.89  1.  ]\n",
      "   [ 0.53  0.48  0.3 ]\n",
      "   [ 0.74  0.56  0.8 ]\n",
      "   [ 0.56  0.57 -0.01]]]\n",
      "\n",
      "\n",
      " [[[ 0.51  0.12  0.86]\n",
      "   [ 0.14  0.03  0.25]\n",
      "   [ 0.37  0.74  0.37]\n",
      "   [ 0.32  0.24  0.14]]]\n",
      "\n",
      "\n",
      " [[[ 0.39  0.42  0.53]\n",
      "   [ 0.44  0.08  0.79]\n",
      "   [ 0.51  0.27  0.36]\n",
      "   [ 0.99  0.93  0.31]]]]\n",
      "Learned Bias (b): \n",
      "[[ 0.5  -0.09  0.93  0.37]]\n",
      "Learned Constant (c): \n",
      "[[ 0.7   0.38  0.93  0.19]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHxtJREFUeJzt3XmUVOWd//H3t4UIuPLDuIK44RJA1NFEXJI2LtFkEomZ\nk8RlNBjNiT+NxBijEM9IJokYz4yJGs3Eo8EtLqPjDBKXuHYUFdzZJIiSEZWIQUR0kB6gn/njqdKm\noatuVd1bz3NvfV7n9OmFWr6nu/n0re99nvs15xwiIpJPbaELEBGR+inERURyTCEuIpJjCnERkRxT\niIuI5JhCXEQkx6qGuJldZ2ZLzGxWt69dambzzOxFM/sPM9s82zJFRGRDkhyJTwa+0ONrDwDDnXP7\nAAuA8WkXJiIi1VUNcefcNODdHl97yDnXVfp0OjA4g9pERKSKNHripwL3pfA4IiJSo4ZC3Mx+DKx2\nzt2SUj0iIlKDPvXe0cy+BXwR+HyV2+niLCIidXDOWbXbJD0St9Kb/8TsaOA84CvOuc4EhUT1dtFF\nFwWvIfaafv1rBzi+/OW46orxe6Wa8l1TrHUllWSJ4S3Ak8DuZrbIzMYCVwKbAg+a2fNmdnXiZ5Rc\nePxxGDECliwJXYmIVFK1neKcO2EDX56cQS0SCed8iF94IVx6aehqRKSSltyx2d7eHrqE9cRU01/+\n4oP82GNh2bJ2anhl1xQxfa/KVFMyMdYE8daVhNXSe6nrCcxc1s8h6brhBrj3XrjtNthqK5g7F7bd\nNnRVIq3FzHApntiUFvL443DooWAGI0fC7NmhKxKR3ijEZT3lEAeFuEjsFOKyjiVL4O23/coUUIiL\nxE4hLuuYNg0OOgg22sh/rhAXiZtCXNbRvZUC/oh83jxYuzZcTSLSO4W4rKNniG+2GWy9NSxcGK4m\nEemdQlw+smIFzJ8P+++/7tdHjFBLRSRWCnH5yFNPwd/9HWy88bpfV19cJF4KcflIz1ZKmUJcJF4K\ncfmIQlwkf7TtXgDo7IRBg2DxYti8x9jr1av915Ytg/79w9Qn0mq07V5q8uyzsMce6wc4QN++MGwY\nvPRS8+sSkcoU4gL4Vsohh/T+72qpiMRJIS5A7/3wMoW4SJwU4sLatfDkkwpxkTxSiAtz5sAnPwnb\nbNP7bRTiInFSiEvVVgrAkCGwciUsXdqcmkQkGYW4MG1a9RA389vv58xpTk0ikoxCvMWVhyJXC3FQ\nS0UkRgrxFlceirzLLtVvqxAXiY9CvMV1n6dZjUJcJD4K8RaXtJUCPsTnzoWurmxrEpHkFOItrpYQ\nHzjQb8t/7bVsaxKR5BTiLaznUOQk1FIRiYtCvIX1HIqchEJcJC4K8RZWSyulTCEuEheFeAtTiIvk\nn4ZCtKgVK2D77eGdd9afqVlJZydsuSUsX17b/USkNhoKIRX1NhS5mo03hp13hvnzs6lLRGqjEG9R\n9bRSykaMUEtFJBYK8RbVSIirLy4SD4V4C+rshOeeg9Gj67u/QlwkHgrxFlRpKHISCnGReCjEW1Aj\nrRTwJzaXLfMrVEQkrKohbmbXmdkSM5vV7WsDzewBM5tvZn80sy2yLVPSVG2yfTVtbTB8uAZEiMQg\nyZH4ZOALPb52AfCQc24P4BFgfNqFSTaSDEVOQi0VkThUDXHn3DTg3R5fPha4ofTxDcCYlOuSjCQZ\nipyEQlwkDvX2xLd2zi0BcM69BWydXkmSpUb74WUKcZE49EnpcSruq584ceJHH7e3t9Pe3p7S00qt\nHn8cjjmm8ccph7hzyaYCiUhlHR0ddHR01Hy/RNdOMbOhwFTn3N6lz+cB7c65JWa2LfCoc26vXu6r\na6dEwjnYYQcf5Lvu2vjjbbutX644eHDjjyUi60r72ilWeiu7G/hW6eNTgCk1VSdB/OUv/n2SochJ\naPu9SHhJlhjeAjwJ7G5mi8xsLHAJcKSZzQcOL30ukatlKHIS6ouLhFe1J+6cO6GXfzoi5VokY2md\n1CwbORIefTS9xxOR2mnHZgvJIsR1JC4SloZCtIglS2DPPWHp0tpmalayciUMGuQHTPTtm85jioin\noRCyjnqGIlczYIBfmbJgQXqPKSK1UYi3iLRbKWVqqYiEpRBvEQpxkWJSiLeAFSv8TMz990//sRXi\nImEpxFtAvUORk1CIi4SlEG8BWbVSAHbbDd56Cz74IJvHF5HKFOItIMsQ32gjv3Rx7txsHl9EKlOI\nF1yjQ5GTUEtFJByFeME1OhQ5CYW4SDgK8YLLspVSphAXCUchXnCNDkVOovuACBFpLoV4gaU1FLma\n7baDri5/fRYRaS6FeIGlNRS5GjO1VERCUYgXWDP64WUKcZEwFOIFphAXKT6FeEE5pxAXaQUK8YJa\nuNC/T2socjUjRsC8ef5kqog0j0K8oNIeilzNZpvB1lt//MdDRJpDIV5QzWyllI0YoZaKSLMpxAtq\n2rTmh7j64iLNpxAvoCVL4O23/ZFxMynERZpPIV5AWQxFTkIhLtJ8CvECCtEPB3+1xEWL4MMPm//c\nIq1KIV5AoUK8b18YNgxeeqn5zy3SqhTiBZPlUOQk1FIRaS6FeMFkORQ5CYW4SHMpxAsmVCulTCEe\nh/vvh1NPhTffDF2JZE0hXjAKcVm2zAd4WxuMGgX/8i+wenXoqiQrCvECacZQ5GqGDIGVK2Hp0nA1\ntLqzz4avfx2uvda31x5+GPbZBx59NHRlkgWFeIE0YyhyNWZ+k9GcOeFqaGVTpsD06fDzn/vPhw2D\ne+/1n48dC8cfrxZL0SjECyR0K6VMLZUwli2DM86AyZNhk00+/roZjBnjl37uuqtaLEWjEC8QhXhr\nK7dRevsdGDAAfvYztViKRiFeEOWhyFlPtk9CId58PdsolajFUiwNhbiZjTezuWY2y8x+b2afSKsw\nqU2zhiInMXIkzJ0LXV2hK2kNvbVRKlGLpTjqDnEzGwqcDuzrnNsb6AN8M63CpDaxtFIABg70J1df\ney10Ja2hWhulErVY8q+RI/EVwP8Cm5hZH2AAsDiVqqRmMYU4qKXSLLW0USpRiyW/6g5x59y7wL8C\ni4A3geXOuYfSKkySa/ZQ5CQU4tmrp41SSfcWy267qcWSF420U3YBzgGGAtsDm5rZCWkVJsk1eyhy\nEgrx7DXSRqlkwAD46U/VYsmLPg3cd3/gCefcMgAzuws4CLil5w0nTpz40cft7e20t7c38LTSU7OH\nIicxciRccknoKoqr3EaZOTO75yi3WKZM8S2W0aP9kfkOO2T3nK2so6ODjo6Omu9nzrm6ntDMRgE3\nAwcAncBk4Bnn3FU9bufqfQ5J5tvfhn33hbPOCl3Jx1at8ic4ly8Pd0XFolq2zO+Kvf325rXQVq6E\nSZPgN7+BCy6AceP89eMlO2aGc67qoVkjPfGZwI3Ac8BMwIBr6n08qV9s/XCAfv1gp538tc0lXVm1\nUSrp2WIZNQoeeaR5zy+9q/tIPPET6Eg8U2+9BXvt5S841eyZmtV8/etw7LFw4omhKymOKVPg3HN9\nGyWNk5n1cM7X8f3vq8WSpcyPxCUOTzwRZihyEjq5ma60V6PUS6tY4qIQz7kYWyllCvF0hWijVKIW\nSxwU4jmnEG8NaW3qyUJ5FcvFF/thFN/8pjYKNZNCPMdCD0WuZuedfQtg+fLQleRbLG2USrq3WIYN\nU4ulmRTiORZ6KHI1bW0wfLgGRDQqtjZKJWqxNJ9CPMdibqWUqaXSmJjbKJWoxdI8CvEcU4gXWx7a\nKJVsqMXym9+Erqp4tE48pzo7YdAgWLw47EzNah55BC66yP/BkdqcdBJstRX86lehK0nHggV+Oez0\n6f4a5lKZ1okXXAxDkZMoH4nr73ht8tpGqWTYMP/K4tJLQ1dSLArxnMpDKwX8tKF+/dQPrUXe2yiV\nnH023HGHfh/SpBDPqbyEOPiLNakvnlyeVqPUaqut4JRT4LLLQldSHArxHIppKHISOrmZXBHbKD2d\ne65/lfHOO6ErKQaFeA7FNBQ5CYV4MkVuo3Q3eDB87Wtw5ZWhKykGhXgO5amVAgrxpIrcRunpRz+C\nq66C998PXUn+KcRzKG8hPny4vzyAtmD3rhXaKN0NGwZHHAG//W3oSvJP68Rzxjl/7ebHH8/XWtth\nw3xQfepToSuJT4hJPTGYNQuOPtrPiO3XL3Q18dE68YKKcShyEmqp9K6V2ijd7b23v/bP9deHriTf\nFOI5E+NQ5CQU4hvWam2UniZMgF/8AtasCV1JfinEcyZv/fAyhfj6WmU1SiWjR/tZrLfdFrqS/FKI\n54xCvDhatY3S04QJMGkSdHWFriSfFOI58tZb8Le/+ZNgebPbbr7+Dz4IXUkcWr2N0t0RR0D//nD3\n3aErySeFeI5MmxbvUORqNtoI9twT5s4NXUl4aqOsy8wfjV98sS6UVg+FeI7ktZVSppaKpzbK+saM\n8Rt/Hn44dCX5oxDPEYV4/qmNsmFtbTB+vD8al9ooxHNixQp4+eV4hyIn0eohrjZKZccf7/dBTJ8e\nupJ8UYjnROxDkZNo9QERaqNU1revv6bKpEmhK8kXhXhO5L2VArDddn4Z2ZIloStpPrVRkhk7Fp5+\nurVfsdVKIZ4TRQhxs9ZsqaiNklz//nDOOXDJJaEryQ+FeA50dsJzz/ndbXnXiiGuNkptvvtdeOAB\nePXV0JXkg0I8B/IyFDmJVgtxtVFqt/nmPsg1UDkZhXgOFKGVUtZKIa42Sv3GjdNA5aQU4jlQpBAf\nPhzmzfNzQotObZT6aaBychoKEbm1a/0v9J//nJ+ZmtXstBM8+KAfFFFUU6b4gcAzZ+oovF5vvOGv\nOb5gAQwaFLqa5tNQiIKYMwe23ro4AQ7Fb6mojZKO8kDlK64IXUncFOKRe/xxOOSQ0FWkq+ghrjZK\nen70I7j6ag1UrkQhHrki9cPLihziWo2SLg1Urq6hEDezLczsDjObZ2ZzzewzaRUmfnu6Qjw/1EbJ\nxvjx/gTnqlWhK4lTo0filwP3Ouf2AkYB8xovScryOhS5mj32gEWL4MMPQ1eSLrVRsqGBypXVHeJm\ntjlwqHNuMoBzbo1zbkVqlUluhyJX07evf5n80kuhK0mP2ijZ0kDl3jVyJL4zsNTMJpvZ82Z2jZn1\nT6swKWYrpaxILRW1UbI3ejQMHaqByhvSSIj3AfYDrnLO7QesBC7Y0A3POw8ee0x/RWulEM+HH/8Y\n/uEfivuzioUGKm9Ynwbu+wbwunPu2dLndwLnb+iGTz89kZtugvfeg0MOaee009r5whdgyy0bePaC\ny/NQ5CRGjoTLLw9dReMWL4bbb/cDOyRbRx758UDlMWNCV5O+jo4OOjo6ar5fQzs2zexPwOnOuZfN\n7CJggHPu/B63+WjH5uuvwx/+4N8eewwOOAC+/GX/tttudZdRSHfe6V+e33NP6EqysWgRfOYz8Ne/\nhq6kMT/8od9V+8tfhq6kNdx1l79M7YwZxTtX1FPSHZuNhvgo4FqgL7AQGOuce6/HbTa47f5//gce\negimTvVBteWW8Pd/7wP9oIOgTyOvEQpg3Dg/ROGCDTao8s85/zN/9VV/WYE8eucdf4J21iy/u1Cy\n19Xlr79z5ZV+/XiRNWXbvXNupnPuAOfcPs6543oGeCWbbALHHgvXXuuvVHbjjf6l0rhxfov5SSf5\nl6nLlzdSYX4VuR8O/ihqxAh/WYG8uvJKvy1cAd48Gqi8vigvgNXqbZcVK2D77f2RXp5nalbz3e/6\no6rvfS90JbV7/32/fv/JJ4t9Ia8YrV7tv+e33QYHHhi6muzk+gJYQ4b4JVv33ONP8I0bB3Pn+iPT\nvfYq/mqXJ5/M/1DkJPK8QuW3v4XDD1eAh6CByuuK8ki8N11dfkzZ1Kn+bdEiOOYYf4RepNUuF17o\n3//sZ2HryNpjj8H558NTT4WupDarVvmj8Pvug1GjQlfTmj780P8MHnjAHwwUUa6PxHvT1uZbK//8\nz/DCC/Dii/4Kfzfe6I/eP/95v0rglVdCV9qYovfDy0aO9K+w8rbu9/rrYb/9FOAhaaDyx3J1JF5J\nUVa7dHb6C+AvXlyMmZrVDB7s/2jtvHPoSpJZswZ23x1uvtn/Xkk4K1b4o/EZM2DXXUNXk75CHolX\nUpTVLkUaipxE3vrit98OO+6oAI/B5pv7c2dFHKhcy3FvYUK8uzy3XVqllVKWpxDv6vIn0yZMCF2J\nlBVxoPJ77/lzfEkVMsR7GjLEL2fLw2oXhXi8pk6Ffv389m+JQ9EGKi9eDJ/9rG/ZJVWYnng9Ylvt\nUsShyNW8+CKceKL/oxoz5/ya5PPPh+OOC12NdFeUgcrz58PRR8Ppp/sNTW1tTdh2n0TMId7T66/7\no/WpU8NsMpo50w8VmD8/++eKxapVMHCgP1cR87r4hx+Gs87yf2zaWuL1a76cfrrfIPeTn4SupD4z\nZvhzepMmwdix/mtNuXZKEnkK8e5CrHb59a99D/+669J/7JjttZc/Ybj33qEr6d3hh/uX7SefHLoS\n2ZAFC/z/y4ULYbPNQldTm3vvhW99y1/w7ktf+vjrLbc6JW0hVru0Wj+8LPa++PTp/kJdxx8fuhLp\nTXmg8r/9W+hKanP99XDqqf7yut0DvBY6Eq9DFm0X52CHHXyQF3HNayU//al/5RPrxo1jj4WjjoIz\nzwxdiVQya5bvKS9c6E9Ax8w5//t+zTV+5++ee65/Gx2JZyiL1S5FHYqcRMxH4rNnw9NP+6Mlidve\ne/udtLEPVF671mfGrbfCE09sOMBroSPxFDWy2uX66/1f5Ntvb1q50Xj1VTjsMP/9is2JJ/pwOH+D\nM6skNk8+6X9mCxbEuUu7sxP+8R/h7bfhv/6rciboxGYEamm7fPvbsO++fgVEq+nq8rvv3ngjrouY\nLVwIn/60f98qO2iLoL0dTjvNn7eKyXvv+bFyW20FN91UveWjdkoEamm7tOpJTfBL9oYPj29AxKWX\n+m3dCvB8iXGgcnkTz/Dh/jroafbsdSQewIbaLl1dsHQpbLRR6OrCOO00fw31M84IXYm3eLGfPPTy\ny/kdH9eqnPOvei+8MI6Byj038SSdDap2So68/ro/Uj/ggNCVhHP55f6X/eqrQ1fiaQByvsUyUHlD\nm3iSUjslR4YMae0Ah7hWqLzzDvzud3DuuaErkXqNGeNH6D38cLga7r3Xn/+67rraA7wWCnGJwsiR\nvicew4s2DUDOv9ADldPYxJOU2ikSjW228ecKQoanBiAXR3mg8q23wujRzXnOJJt4klI7RXInhpaK\nBiAXR7MHKqe9iScphbhEI3SIr1rlr0s9fny4GiRdY8fCM89k/3vV2emvrTNrll82vP322T5fdwpx\niUboENcA5OLp3x++//1sr8vz3nt+CaFzcP/9zd+wphCXaIQM8TVr/OYejV4rnjPOgD/+0V/eIW1Z\nbuJJSiEu0Rg+3K8VX726+c+tAcjFldVA5fnz4eCD4Rvf8CuaQm3U0+oUicqwYTBlCnzqU817zq4u\nf5Gryy7zl5yV4lm61M+tnD3bX/K5UY1s4klKq1Mkl0K0VDQAufjSHKjcrE08SSnEJSrNDnHn/IaQ\nCRPCbs+W7J17rh+B9s479T9GMzfxJKUQl6g0O8QfeQRWrIjjQkmSrcGD/U7cK66o/b7O+dbJT34C\nf/oTHHhg+vXVSz1xicr8+X6QRnnSUdYOP9wPPz7llOY8n4RVz0DltWvhnHN8eN93X/PWgKsnLrm0\n667+io4ffJD9c02fDq+8AieckP1zSRxqHahc3sQze3bzN/EkpRCXqPTp47crz52b/XNNmuS3Zfft\nm/1zSTwuuMCf4Fy1qvLtum/iue8+2GKL5tRXK4W4RKcZfXENQG5do0b5ASSTJ/d+m/ImnhEjwm3i\nSUohLtFpRohfconfjt2/f7bPI3GaMMFv/lmzZv1/676J54or4p+21XCIm1mbmT1vZnenUZBI1iG+\ncKHfhh3LKDhpvoMOgqFD/VF2dzNmwOc+B//0T/lZdprGkfg44KUUHkcE+DjEs1rUpAHIAusPVI5t\nE09SfRq5s5kNBr4I/Bz4QSoVScvbbjv/H2vJEth223Qfe/Fi+Pd/9wOQpbUdeaRvp919Nyxf7k94\n3n13XGvAk2goxIFfAucBkZ63lTwy+/hoPO0Qv+wyvyZcE+zFzB+Nf+c7sMkmfh34HnuErqp2dYe4\nmX0JWOKce9HM2oFeu0cTJ0786OP29nba29vrfVppEeUQT/N6JuUByLNmpfeYkm9jxviRgGeeGX4N\neEdHBx0dHTXfr+4dm2Z2MXASsAboD2wG3OWcO7nH7bRjU2p2zTXw1FOVl4HVauJEeOMNuPba9B5T\nJCtJd2ymsu3ezD4HnOuc+8oG/k0hLjV76in43vfg2WfTeTwNQJa80bZ7ybXhw2HePH/dijRoALIU\nlS6AJdHaaSd48MHGg3fVKn8Uft99mp8p+aEjccm9tDb9aACyFJlCXKKVRohrALIUnUJcopVGiGsA\nshSdQlyi1WiId3X5bdU6CpciU4hLtPbYAxYtgg8/rO/+GoAsrUAhLtHq29evTHmpjsurlQcgjx+f\njyvRidRLIS5Rq7elUh6A/NWvpl+TSEwU4hK1ekP84ov9Vena9BsuBadfcYlaPSGuAcjSShTiErWR\nI2HOnNruowHI0kq07V6i5hxsuaUfqTZoUPXbz54NRx3lb6/5mZJn2nYvhWDmJ44nbaloALK0GoW4\nRC9pX1wDkKUVKcQleklDXAOQpRU1OmNTJHMjR8LNN1e+jQYgS6vSiU2J3rvvwtChfiJ5b+u+f/hD\nf8XCX/2qubWJZCXpiU0diUv0Bg70LZLXXoOdd17/38sDkGfObH5tIqGpJy65UKkvfuWVcNxxMGRI\nc2sSiYFCXHKhtxB//3246io4//zm1yQSA4W45EJvIa4ByNLqFOKSCxsK8VWr4LLL/OVmRVqVQlxy\nYc89/Waezs6Pv6YByCIKccmJfv1gp51g/nz/uQYgi3gKccmN7i0VDUAW8bROXHKjHOLlAciXXRa6\nIpHwdCQuuVEO8alTYeONNQBZBLTtXnLk1VfhsMNgu+380IevfS10RSLZSbrtXiEuudHV5bffDxkC\nc+dqfqYUm66dIoXT1gb77w+nnaYAFynTkbjkSmen74eLFJ3Gs0khKcBF1qUQFxHJMYW4iEiOKcRF\nRHJMIS4ikmN1h7iZDTazR8xsrpnNNrOz0yxMRESqa+RIfA3wA+fccGA0cKaZ7ZlOWdnq6OgIXcJ6\nYqwJ4qxLNSWjmpKLta4k6g5x59xbzrkXSx9/AMwDdkirsCzF+AOLsSaIsy7VlIxqSi7WupJIpSdu\nZjsB+wAz0ng8ERFJpuEQN7NNgTuBcaUjchERaZKGtt2bWR/gD8B9zrnLe7mN9tyLiNQh86sYmtmN\nwFLn3A/qfhAREalb3SFuZgcDjwGzAVd6m+Ccuz+98kREpJLMr2IoIiLZyWzHppkdbWZ/NrOXzez8\nrJ6nFmZ2nZktMbNZoWspi3HTlJltbGYzzOyFUl0Xh66pzMzazOx5M7s7dC1lZvbfZjaz9P16OnQ9\nAGa2hZndYWbzSj/DzwSuZ/fS9+f50vv3IvldH1/6/swys9+b2SciqGlcKQuS5YFzLvU3/B+HV4Ch\nQF/gRWDPLJ6rxroOwS+FnBW6lm41bQvsU/p4U2B+JN+rAaX3GwHTgYND11Sq5xzgZuDu0LV0q2kh\nMDB0HT1quh4YW/q4D7B56Jq61dYGLAaGBK5jaOln94nS57cDJweuaTgwC9i49H/vAWCXSvfJ6kj8\n08AC59xrzrnVwG3AsRk9V2LOuWnAu6Hr6M5FumnKObey9OHG+P90wb9vZjYY+CJwbehaejAiug6R\nmW0OHOqcmwzgnFvjnFsRuKzujgBedc69HriOFcD/ApuUVtoNwP9xCWkvYIZzrtM5txZ/3vG4SnfI\n6hdvB6D7D+gNIgim2MW0aarUtngBeAvocM69FLom4JfAefiT6DFxwINm9oyZnR66GGBnYKmZTS61\nL64xs/6hi+rmG8CtoYtwzr0L/CuwCHgTWO6ceyhsVcwBDjWzgWY2AH/QMqTSHaI5emh1sW2acs51\nOef2BQYDnzWzz4Wsx8y+BCwpvWqx0lssDnbO7Yf/D3emmR0SuJ4+wH7AVaW6VgIXhC3JM7O+wFeA\nOyKoZRd8e24osD2wqZmdELIm59yfgV8ADwL3Ai8AayvdJ6sQfxPYsdvng0tfkw0ovZS7E7jJOTcl\ndD3dlV6G3wPsH7iUg4GvmNlC/FHcYaV9CsE55/5aev834D/x7cSQ3gBed849W/r8Tnyox+AY4LnS\n9yq0/YEnnHPLSq2Lu4CDAteEc26yc25/51w7sBx4udLtswrxZ4DdzGxo6WzvN4FYVhPEdhQH8Dvg\nJdfLrtdmM7OtzGyL0sf9gSPxJ6eDcc5NcM7t6JzbBf/79Ihz7uSQNQGY2YDSqyjMbBPgKPxL4mCc\nc0uA181s99KXDgdiaIcBHE8ErZSS+cCBZtbPzAz/fZoXuCbM7JOl9zsCXwVuqXT7PlkU4Zxba2Zn\n4c+stgHXOedi+ObcArQDg8xsEXBR+eRPwJoOBk4EZpd60DFsmtoOuKH0i92Gf4XwcMB6YrYN8J+l\ny0v0AX7vnHsgcE0AZwO/L7UvFgJjA9dDqcd7BPCd0LUAOOdmll7NPYdvWbwAXBO2KgD+w8z+H7Aa\n+P/VTkprs4+ISI7pxKaISI4pxEVEckwhLiKSYwpxEZEcU4iLiOSYQlxEJMcU4iIiOaYQFxHJsf8D\ny7bVq/2y8pAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5db9b19f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theano.config.profile=True\n",
    "\n",
    "#initialize the learner and set custom kernels\n",
    "x = CRBM(3, 2, 0.2, 3)\n",
    "kernel1 = np.tile(np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]]), [1,1,1])\n",
    "kernel1_ = np.tile(np.flipud(np.fliplr(kernel1[0])),[1,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0],[0,0,0],[1,1,1],[0,0,0]]), [1,1,1])\n",
    "kernel2_ = np.tile(np.flipud(np.fliplr(kernel2[0])), [1,1,1])\n",
    "kernel3 = np.random.rand(1,4,3)\n",
    "kernel3_ = np.tile(np.flipud(np.fliplr(kernel3[0])), [1,1,1])\n",
    "kernel = np.array([kernel1, kernel1_, kernel2, kernel2_, kernel3, kernel3_])\n",
    "#kernel = np.array([kernel3, kernel3_])\n",
    "print \"Kernel shape: \" + str(kernel.shape)\n",
    "\n",
    "# initialize the data\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2], dtype=np.float32)\n",
    "print \"Data shape: \" + str(data.shape)\n",
    "#x.setCustomKernels(kernel)\n",
    "#print x.motifs.shape\n",
    "\n",
    "# perform training on our test data\n",
    "start = time.time()\n",
    "scores = x.trainMinibatch(data, data, 10, 2, 10)\n",
    "print \"Training (with compilation) performed in: \" + str(time.time()-start) + \" seconds.\"\n",
    "print \"Result from training: \"\n",
    "print \"----------------------\"\n",
    "print \"Learned Motif: \"\n",
    "print x.motifs.get_value()\n",
    "print \"Learned Bias (b): \"\n",
    "print x.bias.get_value()\n",
    "print \"Learned Constant (c): \"\n",
    "print x.c.get_value()\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4: Test our GPU solution on the \"real\" training set\n",
    "It's time now to test on some real data to see how good the training is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mat shape: (15000, 1, 4, 150)\n",
      "BatchSize: 100\n",
      "Num of iterations per epoch: 150\n",
      "Compilation of theano function finished in 0.972440958023 seconds\n",
      "Start training...\n",
      "Training finished after: 57.7489359379 seconds!\n",
      "Training of 15000 performed in: 58.7246339321 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwmFmZlVx4TAzs6r8f1t1yAcp4kevAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5dc53da310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theano.config.mode='FAST_RUN'\n",
    "theano.config.profile=True\n",
    "\n",
    "learner = CRBM(4, 10, 0.3, 3)\n",
    "print \"Data mat shape: \" + str(trainingData.shape)\n",
    "start = time.time()\n",
    "scores = learner.trainMinibatch(trainingData, trainingData, 5, 100, 1)\n",
    "print \"Training of \" + str(trainingData.shape[0]) + \" performed in: \" + str(time.time()-start) + \" seconds.\"\n",
    "plt.plot(scores)\n",
    "plt.ylabel('Error done in reconstruction')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
