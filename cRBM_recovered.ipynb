{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional RBM (cRBM)\n",
    "\n",
    "This notebook takes care of implementing the basic functionality for cRBMs.\n",
    "Or maybe it's just for the preliminaries, that is some simple stuff before it actually comes to the Boltzmann Machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading the data and converting it to various forms of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Theano imports\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RS\n",
    "from theano import pp\n",
    "\n",
    "# numpy and python classics\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "# biopython stuff\n",
    "import Bio.SeqIO as sio\n",
    "import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes to read biological files (such as FASTA or JASPAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class reads sequences from fasta files.\n",
    "To use it, create an instance of that object and use\n",
    "the function readSequencesFromFile.\n",
    "\"\"\"\n",
    "class FASTAReader:\n",
    "    \n",
    "    def __init__(self, _path):\n",
    "        self.path = _path\n",
    "        \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        dhsSequences = []\n",
    "        for dhs in sio.parse(open(filename), 'fasta', IUPAC.unambiguous_dna):\n",
    "            dhsSequences.append(dhs.seq)\n",
    "        return dhsSequences\n",
    "    \n",
    "    \n",
    "class JASPARReader:\n",
    "    \n",
    "    def __init__ (self):\n",
    "        pass\n",
    "    \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        matrices = []\n",
    "        for mat in motifs.parse(open(filename), 'jaspar'):\n",
    "            matrices.append(mat.pwm)\n",
    "        return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matReader = JASPARReader()\n",
    "pwms = matReader.readSequencesFromFile('data/jaspar_matrices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the two classes to calculate a forward pass in our algorithm\n",
    "seqReader = FASTAReader('.')\n",
    "allSeqs = seqReader.readSequencesFromFile('data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "test_set = [allSeqs[random.randrange(0,len(allSeqs))] for i in range(1000)]\n",
    "print len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of test set in (in ms): 108.583927155\n"
     ]
    }
   ],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    m = len(seq.alphabet.letters)\n",
    "    n = len(seq)\n",
    "    result = np.zeros((1, m, n))\n",
    "    revSeq = seq.reverse_complement()\n",
    "    for i in range(len(seq)):\n",
    "        result[0,getIntToLetter(seq[i]),i] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "dataMat = np.array([getMatrixFromSeq(t) for t in test_set])\n",
    "print \"Conversion of test set in (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Borrowing Ian Goodfellow's implementation of the probabilistic max pooling layer\n",
    "This implementation is now part of the pylearn2 library which is licensed under the 3-claused BSD license.\n",
    "Source code is available here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/expr/probabilistic_max_pooling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from theano.gof.op import get_debug_values\n",
    "\n",
    "def max_pool(z, pool_shape, top_down=None, theano_rng=None):\n",
    "    \"\"\"\n",
    "    Probabilistic max-pooling\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : theano 4-tensor\n",
    "        a theano 4-tensor representing input from below\n",
    "    pool_shape : tuple\n",
    "        tuple of ints. the shape of regions to be pooled\n",
    "    top_down : theano 4-tensor, optional\n",
    "        a theano 4-tensor representing input from above\n",
    "        if None, assumes top-down input is 0\n",
    "    theano_rng : MRG_RandomStreams, optional\n",
    "        Used for random numbers for sampling\n",
    "    Returns\n",
    "    -------\n",
    "    p : theano 4-tensor\n",
    "        the expected value of the pooling layer p\n",
    "    h : theano 4-tensor\n",
    "        the expected value of the detector layer h\n",
    "    p_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the pooling layer\n",
    "    h_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the detector layer\n",
    "    Notes\n",
    "    ------\n",
    "    all 4-tensors are formatted with axes ('b', 'c', 0, 1).\n",
    "    This is for maximum speed when using theano's conv2d\n",
    "    to generate z and top_down, or when using it to infer conditionals of\n",
    "    other layers using the return values.\n",
    "    Detailed description:\n",
    "    Suppose you have a variable h that lives in a Conv2DSpace h_space and\n",
    "    you want to pool it down to a variable p that lives in a smaller\n",
    "    Conv2DSpace p.\n",
    "    This function does that, using non-overlapping pools.\n",
    "    Specifically, consider one channel of h. h must have a height that is a\n",
    "    multiple of pool_shape[0] and a width that is a multiple of pool_shape[1].\n",
    "    A channel of h can thus be broken down into non-overlapping rectangles\n",
    "    of shape pool_shape.\n",
    "    Now consider one rectangular pooled region within one channel of h.\n",
    "    I now use 'h' to refer just to this rectangle, and 'p' to refer to\n",
    "    just the one pooling unit associated with that rectangle.\n",
    "    We assume that the space that h and p live in is constrained such\n",
    "    that h and p are both binary and p = max(h). To reduce the state-space\n",
    "    in order to make probabilistic computations cheaper we also\n",
    "    constrain sum(h) <= 1.\n",
    "    Suppose h contains k different units. Suppose that the only term\n",
    "    in the model's energy function involving h is -(z*h).sum()\n",
    "    (elemwise multiplication) and the only term in\n",
    "    the model's energy function involving p is -(top_down*p).sum().\n",
    "    Then P(h[i] = 1) = softmax( [ z[1], z[2], ..., z[k], -top_down] )[i]\n",
    "    and P(p = 1) = 1-softmax( [z[1], z[2], ..., z[k], -top_down])[k]\n",
    "    This variation of the function assumes that z, top_down, and all\n",
    "    return values use Conv2D axes ('b', 'c', 0, 1).\n",
    "    This variation of the function implements the softmax using a\n",
    "    theano graph of exp, maximum, sub, and div operations.\n",
    "    Performance notes:\n",
    "    It might be possible to make a faster implementation with different\n",
    "    theano ops. rather than using set_subtensor, it might be possible\n",
    "    to use the stuff in theano.sandbox.neighbours. Probably not possible,\n",
    "    or at least nasty, because that code isn't written with multiple\n",
    "    channels in mind, and I don't think just a reshape can fix it.\n",
    "    Some work on this in galatea.cond.neighbs.py\n",
    "    At some point images2neighbs' gradient was broken so check that\n",
    "    it has been fixed before sinking too much time into this.\n",
    "    Stabilizing the softmax is also another source of slowness.\n",
    "    Here it is stabilized with several calls to maximum and sub.\n",
    "    It might also be possible to stabilize it with\n",
    "    T.maximum(-top_down,T.signal.downsample.max_pool(z)).\n",
    "    Don't know if that would be faster or slower.\n",
    "    Elsewhere in this file I implemented the softmax with a reshape\n",
    "    and call to Softmax / SoftmaxWithBias.\n",
    "    This is slower, even though Softmax is faster on the GPU than the\n",
    "    equivalent max/sub/exp/div graph. Maybe the reshape is too expensive.\n",
    "    Benchmarks show that most of the time is spent in GpuIncSubtensor\n",
    "    when running on gpu. So it is mostly that which needs a faster\n",
    "    implementation. One other way to implement this would be with\n",
    "    a linear.Conv2D.lmul_T, where the convolution stride is equal to\n",
    "    the pool width, and the thing to multiply with is the hparts stacked\n",
    "    along the channel axis. Unfortunately, conv2D doesn't work right\n",
    "    with stride > 2 and is pretty slow for stride 2. Conv3D is used to\n",
    "    mitigate some of this, but only has CPU code.\n",
    "    \"\"\"\n",
    "\n",
    "    z_name = z.name\n",
    "    if z_name is None:\n",
    "        z_name = 'anon_z'\n",
    "\n",
    "    batch_size, ch, zr, zc = z.shape\n",
    "\n",
    "    r, c = pool_shape\n",
    "\n",
    "    zpart = []\n",
    "\n",
    "    mx = None\n",
    "\n",
    "    if top_down is None:\n",
    "        t = 0.\n",
    "    else:\n",
    "        t = - top_down\n",
    "        t.name = 'neg_top_down'\n",
    "\n",
    "    for i in xrange(r):\n",
    "        zpart.append([])\n",
    "        for j in xrange(c):\n",
    "            cur_part = z[:, :, i:zr:r, j:zc:c]\n",
    "            if z_name is not None:\n",
    "                cur_part.name = z_name + '[%d,%d]' % (i, j)\n",
    "            zpart[i].append(cur_part)\n",
    "            if mx is None:\n",
    "                mx = T.maximum(t, cur_part)\n",
    "                if cur_part.name is not None:\n",
    "                    mx.name = 'max(-top_down,' + cur_part.name + ')'\n",
    "            else:\n",
    "                max_name = None\n",
    "                if cur_part.name is not None:\n",
    "                    mx_name = 'max(' + cur_part.name + ',' + mx.name + ')'\n",
    "                mx = T.maximum(mx, cur_part)\n",
    "                mx.name = mx_name\n",
    "    mx.name = 'local_max(' + z_name + ')'\n",
    "\n",
    "    pt = []\n",
    "\n",
    "    for i in xrange(r):\n",
    "        pt.append([])\n",
    "        for j in xrange(c):\n",
    "            z_ij = zpart[i][j]\n",
    "            safe = z_ij - mx\n",
    "            safe.name = 'safe_z(%s)' % z_ij.name\n",
    "            cur_pt = T.exp(safe)\n",
    "            cur_pt.name = 'pt(%s)' % z_ij.name\n",
    "            pt[-1].append(cur_pt)\n",
    "\n",
    "    off_pt = T.exp(t - mx)\n",
    "    off_pt.name = 'p_tilde_off(%s)' % z_name\n",
    "    denom = off_pt\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            denom = denom + pt[i][j]\n",
    "    denom.name = 'denom(%s)' % z_name\n",
    "\n",
    "    off_prob = off_pt / denom\n",
    "    p = 1. - off_prob\n",
    "    p.name = 'p(%s)' % z_name\n",
    "\n",
    "    hpart = []\n",
    "    for i in xrange(r):\n",
    "        hpart.append([pt_ij / denom for pt_ij in pt[i]])\n",
    "\n",
    "    h = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            h.name = 'h_interm'\n",
    "            h = T.set_subtensor(h[:, :, i:zr:r, j:zc:c], hpart[i][j])\n",
    "\n",
    "    h.name = 'h(%s)' % z_name\n",
    "\n",
    "    if theano_rng is None:\n",
    "        return p, h\n",
    "    \n",
    "    ### --------------------- DONE IF NO SAMPLES ARE GENERATED ---------------------------###\n",
    "    else:\n",
    "        events = []\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                events.append(hpart[i][j])\n",
    "        events.append(off_prob)\n",
    "\n",
    "        events = [event.dimshuffle(0, 1, 2, 3, 'x') for event in events]\n",
    "\n",
    "        events = tuple(events)\n",
    "\n",
    "        stacked_events = T.concatenate(events, axis=4)\n",
    "\n",
    "        rows = zr // pool_shape[0]\n",
    "        cols = zc // pool_shape[1]\n",
    "        outcomes = pool_shape[0] * pool_shape[1] + 1\n",
    "        assert stacked_events.ndim == 5\n",
    "        for se, bs, r, c, chv in get_debug_values(stacked_events, batch_size,\n",
    "                                                  rows, cols, ch):\n",
    "            assert se.shape[0] == bs\n",
    "            assert se.shape[1] == r\n",
    "            assert se.shape[2] == c\n",
    "            assert se.shape[3] == chv\n",
    "            assert se.shape[4] == outcomes\n",
    "        reshaped_events = stacked_events.reshape((\n",
    "            batch_size * rows * cols * ch, outcomes))\n",
    "\n",
    "        multinomial = theano_rng.multinomial(pvals=reshaped_events,\n",
    "                                             dtype=p.dtype)\n",
    "\n",
    "        reshaped_multinomial = multinomial.reshape((batch_size, ch, rows,\n",
    "                                                    cols, outcomes))\n",
    "\n",
    "        h_sample = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "        idx = 0\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                h_sample = T.set_subtensor(h_sample[:, :, i:zr:r, j:zc:c],\n",
    "                                           reshaped_multinomial[:, :, :, :,\n",
    "                                           idx])\n",
    "                idx += 1\n",
    "\n",
    "        p_sample = 1 - reshaped_multinomial[:, :, :, :, -1]\n",
    "\n",
    "        return p, h, p_sample, h_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PART 3: Optimizing theano to do it all on the GPU\n",
    "\n",
    "class CRBM:\n",
    "\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.alphabet = _alphabet\n",
    "        self.initializeMotifs()\n",
    "        \n",
    "        # cRBM parameters (2*x to respect both strands of the DNA)\n",
    "        self.bias = theano.shared(value=np.random.rand(2*self.numMotifs), name='bias', borrow=True)\n",
    "        self.c = theano.shared(value=np.array([random.random()]), name='c', borrow=True)\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.rng = np.random.RandomState()\n",
    "        self.theano_rng = RS(seed=1234)\n",
    "        self.theano_rng_slow = RandomStreams(self.rng.randint(2**30))\n",
    "    \n",
    "    \n",
    "    def initializeMotifs (self):\n",
    "        # create random motifs (2*self.numMotifs to respect both strands)\n",
    "        x = np.random.rand(2 * self.numMotifs, 1, 4, self.motifLength).astype(np.float32)\n",
    "        \n",
    "        # create reverse complement\n",
    "        for i in range(self.numMotifs):\n",
    "            x[self.numMotifs+i] = np.flipud(np.fliplr(x[self.numMotifs+i]))\n",
    "            \n",
    "        self.motifs = theano.shared(value=x, name='W', borrow=True)\n",
    "        \n",
    "        \n",
    "    def setCustomKernels (self, customKernels):\n",
    "        if len(customKernels.shape) != 4 or customKernels.shape[1] != 1:\n",
    "            print \"New motifs must be a 4D matrix with dims: (K x 1 x numOfLetters(4) x numOfKMers)\"\n",
    "            return\n",
    "        \n",
    "        self.numMotifs = customKernels.shape[0]\n",
    "        self.motifLength = customKernels.shape[3]\n",
    "        self.bias = theano.shared(value=np.random.rand(self.numMotifs), name='bias', borrow=True)\n",
    "        self.motifs = theano.shared(value=customKernels.astype(np.float32))\n",
    "        print \"New motifs set. # Motifs: \" + str(self.numMotifs) + \" K-mer-Length: \" + str(self.motifLength)\n",
    "\n",
    "        \n",
    "### ------------------------------THE TOUGH STUFF-------------------------------- ###\n",
    "### ----------------------------------------------------------------------------- ###\n",
    "\n",
    "    def forwardBatch (self, data):\n",
    "        out = conv.conv2d(data, self.motifs)[:,:,::-1,::-1] # flip, because conv reverts H\n",
    "        out = out.mean(axis=2, keepdims=True)\n",
    "        #bMod = self.bias.dimshuffle('x', 0, 'x', 'x') # add dims to the bias until it works\n",
    "        pooled = max_pool(out.dimshuffle(0,2,1,3), pool_shape=(2, self.poolingFactor), theano_rng=self.theano_rng)\n",
    "        H = pooled[1]\n",
    "        S = pooled[3]\n",
    "        return [H,S] #only return pooled layer and probs\n",
    "\n",
    "\n",
    "    def backwardBatch (self, H_sample):\n",
    "        K = self.motifs.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "        H_shuffled = H_sample.dimshuffle(0, 2, 1, 3) # interpret the kernels as channels (will be summed automatically)\n",
    "        C = conv.conv2d(H_shuffled, K, border_mode='full')\n",
    "        out = T.sum(C, axis=1, keepdims=True) # sum over all K\n",
    "        #out = out + self.c\n",
    "        \n",
    "        # add fourth dimension (the strands) that was lost during forward pass\n",
    "        res = self.softmax(out)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def gradient (self, hiddenProbs, data):\n",
    "        mean = T.mean(hiddenProbs, axis=0, keepdims=True) # sum over all training data to get avg (but keep dim)\n",
    "        H_reshaped = mean.dimshuffle(2, 0, 1, 3)\n",
    "        out = conv.conv2d(data, H_reshaped)\n",
    "        return T.mean(out, axis=0, keepdims=True).dimshuffle(1, 0, 2, 3) #mean over all training samples\n",
    "\n",
    "    \n",
    "    def performGibbsSampling (self, Sample_H):\n",
    "        #theano.printing.Print('Sample')(Sample_H)\n",
    "        V_model = self.backwardBatch(Sample_H)\n",
    "        S_v = self.sampleVisibleLayer(V_model)\n",
    "        [H_model, S_model] = self.forwardBatch(S_v)\n",
    "        return [H_model, S_model]\n",
    "        \n",
    "        \n",
    "    def getTrainingFunction (self, numOfCDs):\n",
    "        D = T.tensor4('data')\n",
    "        # calculate the data gradient for weights (motifs) and bias\n",
    "        [H_data, S_data] = self.forwardBatch(D)\n",
    "        G_data = self.gradient(H_data, D)\n",
    "        bias_data = T.mean(T.sum(H_data, axis=3), axis=0)\n",
    "        #theano.printing.Print('Shape of sample')(S_data.shape)\n",
    "        \n",
    "        # calculate the model gradient scan does that nnumOfCDs times)\n",
    "        result, updates = theano.scan(fn = self.performGibbsSampling,\n",
    "                                      outputs_info = [None, T.unbroadcast(S_data, 1)],\n",
    "                                      n_steps = numOfCDs)\n",
    "        H_model = result[-1][0] # we only want the last value here. Theano is smart and respects that while optimizing\n",
    "        \n",
    "        # compute the gradients\n",
    "        G_model = self.gradient(H_model, D)\n",
    "        bias_model = T.mean(T.sum(H_model, axis=3), axis=0)\n",
    "        \n",
    "        self.motifs = self.motifs + self.learningRate * (G_data - G_model)\n",
    "        self.bias = self.bias + self.learningRate * (bias_data - bias_model)\n",
    "        \n",
    "        fun = theano.function([D], [self.motifs, self.bias], updates=updates, allow_input_downcast=True)\n",
    "        return fun\n",
    "    \n",
    "    \n",
    "    def trainMinibatch (self, data, epochs, batchSize, numOfCDs):\n",
    "        \n",
    "        # assert that pooling can be done without rest to the division\n",
    "        assert (((data.shape[3] - self.motifLength + 1) % self.poolingFactor) == 0)\n",
    "        \n",
    "        itPerEpoch = data.shape[0] / batchSize\n",
    "        print \"BatchSize: \" + str(batchSize)\n",
    "        print \"Num of iterations per epoch: \" + str(itPerEpoch)\n",
    "        start = time.time()\n",
    "        trainingFun = self.getTrainingFunction(numOfCDs)\n",
    "        print \"Compilation of theano function finished in \" + str(time.time()-start) + \" seconds\"\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(itPerEpoch):\n",
    "                results = trainingFun(data[batch*batchSize:(batch+1)*batchSize])\n",
    "        \n",
    "        return results\n",
    "\n",
    "    \n",
    "    def getDataReconstructionFunction (self):\n",
    "        D = T.tensor4('testData')\n",
    "        [H, S_H] = self.forwardBatch(D)\n",
    "        V = self.sampleVisibleLayer(self.backwardBatch(S_H))\n",
    "        diff = T.abs_(D - V)\n",
    "        scorePerSample = diff.sum(axis=3) # sum over errors for each letter\n",
    "        score = scorePerSample.mean(axis=0) # mean over all training samples\n",
    "        f = theano.function([D], score, allow_input_downcast=True)\n",
    "        return f\n",
    "        \n",
    " \n",
    "    def sampleVisibleLayer (self, V):\n",
    "        S = self.theano_rng_slow.multinomial(n=1,pvals=V.dimshuffle(0,1,3,2)).dimshuffle(0,1,3,2)\n",
    "        S = S.astype('float32')\n",
    "        return S\n",
    "    \n",
    "    def softmax (self, x):\n",
    "        return T.exp(x) / T.exp(x).sum(axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4b: Create the theano functions neccessary for training (compile to C)\n",
    "Theano optimizes the function graph heavily when aiming for maximum performance. Thus, the package spends a long time setting up the whole system and not so much time with the actual training of the data.\n",
    "\n",
    "By generating the function in it's own cell, we can do training seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "MRG_RandomStreams.multinomial only implemented with n == 1 and pvals.ndim = 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-dc6548561a97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCRBM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mreconstructionFun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetDataReconstructionFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-52f0fde4ed88>\u001b[0m in \u001b[0;36mgetDataReconstructionFunction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'testData'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS_H\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampleVisibleLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackwardBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS_H\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mscorePerSample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sum over errors for each letter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-52f0fde4ed88>\u001b[0m in \u001b[0;36msampleVisibleLayer\u001b[1;34m(self, V)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msampleVisibleLayer\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheano_rng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/sandbox/rng_mrg.pyc\u001b[0m in \u001b[0;36mmultinomial\u001b[1;34m(self, size, n, pvals, ndim, dtype, nstreams)\u001b[0m\n\u001b[0;32m   1289\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m             raise NotImplementedError((\"MRG_RandomStreams.multinomial only\"\n\u001b[0m\u001b[0;32m   1292\u001b[0m                 \" implemented with n == 1 and pvals.ndim = 2\"))\n\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: MRG_RandomStreams.multinomial only implemented with n == 1 and pvals.ndim = 2"
     ]
    }
   ],
   "source": [
    "# set some config parameters to make debugging simpler\n",
    "debug = True\n",
    "if debug:\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    theano.config.exception_verbosity='high'\n",
    "    theano.config.optimizer='None'\n",
    "    theano.config.compute_test_value='ignore'\n",
    "    theano.config.profile=True\n",
    "else:\n",
    "    theano.config.exception_verbosity='low'\n",
    "    theano.config.mode='FAST_RUN'\n",
    "    \n",
    "x = CRBM(3, 10, 1.5, 3)\n",
    "reconstructionFun = x.getDataReconstructionFunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel shape: (6, 1, 4, 3)\n",
      "Data shape: (2, 1, 4, 8)\n",
      "BatchSize: 2\n",
      "Num of iterations per epoch: 1\n",
      "Compilation of theano function finished in 0.46496295929 seconds\n",
      "Training (with compilation) performed in: 0.624013185501 seconds.\n",
      "[[[[ 0.25  0.67  0.82 -0.28 -0.06  0.92]\n",
      "   [-0.25 -0.05  0.48  0.12 -0.01  0.14]\n",
      "   [ 0.9   0.24 -0.37  0.39  0.24 -0.16]\n",
      "   [ 0.04  0.18  0.21 -0.07  0.87  0.37]]]\n",
      "\n",
      "\n",
      " [[[ 1.56  0.64  0.91  1.12  0.96  0.51]\n",
      "   [ 1.24  1.09  0.18  0.18  0.57  0.84]\n",
      "   [ 1.1   1.03  0.71  1.17  1.5   0.96]\n",
      "   [ 0.01  0.41  0.7   0.65  0.92  1.08]]]\n",
      "\n",
      "\n",
      " [[[ 0.42  0.47  0.4  -0.19  0.97  0.64]\n",
      "   [-0.44  0.51  0.88  1.17 -0.31  0.76]\n",
      "   [ 1.02 -0.33  0.77  0.21  0.57  0.07]\n",
      "   [ 0.83  1.07 -0.05  0.96  0.61  0.5 ]]]\n",
      "\n",
      "\n",
      " [[[ 1.17  0.42  0.47  0.37  0.2   0.58]\n",
      "   [ 0.68  0.99  0.23  0.51  0.92  1.05]\n",
      "   [-0.13  0.97  0.57  0.12  0.34  1.16]\n",
      "   [ 0.24 -0.04  0.25  0.99  0.16  0.38]]]]\n",
      "[[-0.22  1.33 -0.06  0.63]]\n",
      "(1, 4)\n",
      "[[ 3.  3.  3.  2.]]\n"
     ]
    }
   ],
   "source": [
    "theano.config.profile=True\n",
    "\n",
    "#initialize the learner and set custom kernels\n",
    "x = CRBM(6, 2, 1.5, 3)\n",
    "kernel1 = np.tile(np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]]), [1,1,1])\n",
    "kernel1_ = np.tile(np.flipud(np.fliplr(kernel1[0])),[1,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0],[0,0,0],[1,1,1],[0,0,0]]), [1,1,1])\n",
    "kernel2_ = np.tile(np.flipud(np.fliplr(kernel2[0])), [1,1,1])\n",
    "kernel3 = np.random.rand(1,4,3)\n",
    "kernel3_ = np.tile(np.flipud(np.fliplr(kernel3[0])), [1,1,1])\n",
    "kernel = np.array([kernel1, kernel1_, kernel2, kernel2_, kernel3, kernel3_])\n",
    "#kernel = np.array([kernel3, kernel3_])\n",
    "print \"Kernel shape: \" + str(kernel.shape)\n",
    "\n",
    "# initialize the data\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2], dtype=np.float32)\n",
    "print \"Data shape: \" + str(data.shape)\n",
    "#x.setCustomKernels(kernel)\n",
    "#print x.motifs.shape\n",
    "\n",
    "\n",
    "# perform training on our test data\n",
    "start = time.time()\n",
    "res = x.trainMinibatch(data, 10, 2, 10)\n",
    "print \"Training (with compilation) performed in: \" + str(time.time()-start) + \" seconds.\"\n",
    "#print \"Result from training: \"\n",
    "print res[0]\n",
    "print res[1]\n",
    "\n",
    "\n",
    "\n",
    "# now validate how well we are recognizing the training data\n",
    "result = reconstructionFun(data)\n",
    "print result.shape\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4: Test our GPU solution on the \"real\" training set\n",
    "It's time now to test on some real data to see how good the training is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mat shape: (1000, 1, 4, 150)\n",
      "BatchSize: 1000\n",
      "Num of iterations per epoch: 1\n",
      "Compilation of theano function finished in 0.222053050995 seconds\n",
      "Training of 1000 performed in: 3.66899704933 seconds.\n",
      "(20, 1, 4, 4)\n",
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "theano.config.mode='FAST_RUN'\n",
    "theano.config.profile=True\n",
    "\n",
    "learner = CRBM(4, 10, 0.3, 3)\n",
    "print \"Data mat shape: \" + str(dataMat.shape)\n",
    "#print \"Initial Score: \" + str(reconstructionFun(dataMat))\n",
    "start = time.time()\n",
    "[motifs, bias] = learner.trainMinibatch(dataMat, 1, 1000, 1)\n",
    "#scoreAfterTraining = reconstructionFun(dataMat)\n",
    "print \"Training of \" + str(dataMat.shape[0]) + \" performed in: \" + str(time.time()-start) + \" seconds.\"\n",
    "print motifs.shape\n",
    "print bias.shape\n",
    "#print \"Final Score: \" + str(scoreAfterTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4b: Why is it so slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
