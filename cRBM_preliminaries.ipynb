{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional RBM (cRBM)\n",
    "\n",
    "This notebook takes care of implementing the basic functionality for cRBMs.\n",
    "Or maybe it's just for the preliminaries, that is some simple stuff before it actually comes to the Boltzmann Machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading the data and converting it to various forms of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import numpy as np\n",
    "import Bio.SeqIO as sio\n",
    "import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classes to read biological files (such as FASTA or JASPAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class reads sequences from fasta files.\n",
    "To use it, create an instance of that object and use\n",
    "the function readSequencesFromFile.\n",
    "\"\"\"\n",
    "class FASTAReader:\n",
    "    \n",
    "    def __init__(self, _path):\n",
    "        self.path = _path\n",
    "        \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        dhsSequences = []\n",
    "        for dhs in sio.parse(open(filename), 'fasta', IUPAC.unambiguous_dna):\n",
    "            dhsSequences.append(dhs.seq)\n",
    "        return dhsSequences\n",
    "    \n",
    "    \n",
    "class JASPARReader:\n",
    "    \n",
    "    def __init__ (self):\n",
    "        pass\n",
    "    \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        matrices = []\n",
    "        for mat in motifs.parse(open(filename), 'jaspar'):\n",
    "            matrices.append(mat.pwm)\n",
    "        return matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in JASPAR matrices and FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matReader = JASPARReader()\n",
    "pwms = matReader.readSequencesFromFile('../data/jaspar_matrices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply the two classes to calculate a forward pass in our algorithm\n",
    "seqReader = FASTAReader('.')\n",
    "allSeqs = seqReader.readSequencesFromFile('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert FASTA sequences to matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "test_set = [allSeqs[random.randrange(0,len(allSeqs))] for i in range(100000)]\n",
    "print len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  1.  1.  1.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  1.  1.  1.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]]\n",
      "Time for very small conversion (in ms): 1.84607505798\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "Conversion of test set in (in ms): 29748.7778664\n"
     ]
    }
   ],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    m = len(seq.alphabet.letters)\n",
    "    n = len(seq)\n",
    "    result = np.zeros((2, m, n))\n",
    "    revSeq = seq.reverse_complement()\n",
    "    for i in range(len(seq)):\n",
    "        result[0,getIntToLetter(seq[i]),i] = 1\n",
    "        result[1,getIntToLetter(revSeq[i]),i] = 1\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "#seqMatrices = []\n",
    "#for seq in allSeqs:\n",
    "#    seqMatrices.append(getMatrixFromSeq(seq))\n",
    "#print \"Conversion from DNA to matrix took: \" + str(time.time()-start)\n",
    "print getMatrixFromSeq(randSeq1)\n",
    "print \"Time for very small conversion (in ms): \" + str((time.time()-start)*1000)\n",
    "\n",
    "start = time.time()\n",
    "dataMat = np.array([getMatrixFromSeq(t) for t in test_set])\n",
    "print \"Conversion of test set in (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "\n",
    "\n",
    "def _getLetterToInt (num):\n",
    "    if num == 0:\n",
    "        return 'A'\n",
    "    elif num == 1:\n",
    "        return 'C'\n",
    "    elif num == 2:\n",
    "        return 'G'\n",
    "    elif num == 3:\n",
    "        return 'T'\n",
    "    else:\n",
    "        print 'ERROR: Num ' + str(num) + \" not a valid char in DNA alphabet\"\n",
    "        return -1\n",
    "\n",
    "def _convertPWM2Array (pwm):\n",
    "    result = np.zeros((4, len(pwm['A'])))\n",
    "    for letter in range(len(pwm)):\n",
    "        result[letter] = pwm[_getLetterToInt(letter)]\n",
    "    return result\n",
    "  \n",
    "def makeIt3d (seq, numOfKernels):\n",
    "    x = np.tile(seq, [numOfKernels, 1, 1])\n",
    "    print x.shape\n",
    "    return x\n",
    "\n",
    "# data: 4D matrix with dims = (N_batch x 2 x 4 x N_v)\n",
    "# kernel: 4D matrix with dims = (K x 2 x 4 x number of k-mers)\n",
    "def forwardBatch(_data, kernels):\n",
    "    \n",
    "    # create 4D tensor for theano (BatchSize x K x 2*numOfLetters x lenOfSeqs)\n",
    "    D = T.tensor4('data')\n",
    "    K = T.tensor4('kernels')\n",
    "    out = conv.conv2d(D,K)\n",
    "    f = theano.function([D,K], out, allow_input_downcast=True)\n",
    "    return f(_data, kernels)\n",
    "\n",
    "\n",
    "def backwardBatch(_hidden, kernels):\n",
    "    \n",
    "    # theano convolution call\n",
    "    H = T.tensor4('hidden')\n",
    "    K = T.tensor4('kernels')\n",
    "    K_star = K.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "    C = conv.conv2d(H, K_star, border_mode='full')\n",
    "    out = T.sum(C, axis=1) # sum over all K\n",
    "    f = theano.function([H,K], out, allow_input_downcast=True)\n",
    "    \n",
    "    return f(_hidden, kernels)\n",
    "    \n",
    "def sigmoid(_H):\n",
    "    return (1.0 / (1.0 + np.exp(-_H)))\n",
    "\n",
    "def gradient (H, V_0):\n",
    "    H_sum = np.sum(H, axis=0)\n",
    "    print H_sum.shape\n",
    "    \n",
    "    H = T.tensor4('hidden')\n",
    "    S = T.tensor3('sample')\n",
    "    H_sum = T.sum(H, axis=0)\n",
    "    S_star = T.tile(S, [1,1,1,1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing incredibly fast theano convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "kernel1 = np.tile(np.eye(4), [2,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1])\n",
    "kernel = np.array([kernel1, kernel2])\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2])\n",
    "data.shape\n",
    "res = sigmoid(forwardBatch(data, kernel))\n",
    "vis = (backwardBatch(res, kernel))\n",
    "print vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2, 4, 4)\n",
      "(100000, 2, 4, 150)\n",
      "(100000, 10, 1, 147)\n",
      "Time for convolution of 100000 sequences (in ms): 3603.99007797\n"
     ]
    }
   ],
   "source": [
    "kernel = np.tile(np.eye(4), [10, 2,1,1])\n",
    "print kernel.shape\n",
    "print dataMat.shape\n",
    "start = time.time()\n",
    "res = forwardBatch(dataMat, kernel)\n",
    "#vis = backwardBatch(res, kernel)\n",
    "print res.shape\n",
    "print \"Time for convolution of \" + str(dataMat.shape[0]) + \" sequences (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation of our convRBM so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class implements a cRBM for sequence analysis.\n",
    "It does perform efficient forward and backward pass of any given DNA sequence.\n",
    "It implements softmax and sigmoid functions as activation and performs probabilistic max pooling\n",
    "after the convolution step.\n",
    "So this class is basically a two layer network with a convolution layer and a pooling layer on top\n",
    "of that.\n",
    "The learning procedure uses contrastive divergence (CD) with a variable amount of steps.\n",
    "\"\"\"\n",
    "\n",
    "class ConvRBM:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the cRBM. The parameters here are global params that should not change\n",
    "    during the execution of training or testing and characterize the network.\n",
    "    \n",
    "    Parameters:\n",
    "    _motifLength:    How long are the motifs (position weight matrices PWM). This\n",
    "                     This is equivalent to ask what the number of k-mers is.\n",
    "                     The current approach only deals with one fixed motif length.\n",
    "                     \n",
    "    _numMotifs:      How many motifs are applied to the sequence, that is how many\n",
    "                     hidden units does the network have. Each hidden unit consists\n",
    "                     of a vector of size (sequenceLength-motifLength+1)\n",
    "                     \n",
    "    _poolingFactor:  How many units from the hidden layer are pooled together.\n",
    "                     Note that the number has to divide evenly to the length of\n",
    "                     the hidden units, that is:\n",
    "                     mod(sequenceLength-motifLength+1, poolingFactor) == 0\n",
    "                     (1 = equivalent to sigmoid activation)\n",
    "                     \n",
    "    _alphabet:       Biopython uses alphabets for sequences to do sanity checks.\n",
    "                     However, all of the code is written for DNA sequences and even\n",
    "                     though in theory there should be no difference between that\n",
    "                     and other alphabets, Biopython may have trouble with the convolution.\n",
    "    \"\"\"\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.motifs = []\n",
    "        self.alphabet = _alphabet\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        \n",
    "        # cRBM parameters\n",
    "        self.bias = np.random.rand(self.numMotifs)\n",
    "        self.c = random.random()\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "    \"\"\"\n",
    "    This function initializes the motifs (or PWMs) of the cRBM. Maybe this function will\n",
    "    be removed in future versions and initialization will be performed by the\n",
    "    c'tor.\n",
    "    \"\"\"\n",
    "    def initializePWMs (self):\n",
    "        # set up PWMs\n",
    "        for m in range(self.numMotifs):\n",
    "            self.motifs.append(self._createRandomMotif(self.motifLength, self.alphabet))\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Calculate the forward pass for any given sequence, that is P(H | V).\n",
    "    This method applies convolution of all filters to the sequence, using the Biopython package.\n",
    "    It also looks on both strands for matches and returns the hidden activation layer.\n",
    "    It performs also the probabilistic max pooling on the convoluted data.\n",
    "    \n",
    "    Parameters:\n",
    "    seq:             The DNA sequence to calculate the forward pass on. The sequence is of\n",
    "                     type seq.\n",
    "                     \n",
    "    Return:\n",
    "    The function returns the hidden activation layer as numpy matrix.\n",
    "    That matrix has dimensionality 2 * (len(seq) - motifLength + 1) x K where K is the number\n",
    "    of kernels (motifs/PWMs) that are applied.\n",
    "    Since the algorithm looks on both strands, the factor 2 is present.\n",
    "    \"\"\"\n",
    "    def forwardPass (self, seq):\n",
    "        \n",
    "        # check that we actually have some motifs to do convolution on\n",
    "        if self.motifs == []:\n",
    "            print 'Error: No motifs created so far. Try executing initializePWMs before!'\n",
    "            return\n",
    "        if (len(seq)-self.motifLength+1) % self.poolingFactor != 0:\n",
    "            print 'Dimension mismatch: cannot create pooling layer because it would not fit!'\n",
    "\n",
    "        # perform convolution of motif and sequence (that is, apply the motif to the sequence)\n",
    "        hiddenActivation = np.zeros((2*self.numMotifs, len(seq)-self.motifLength+1))\n",
    "        motifCount = 0\n",
    "        for motif in self.motifs:\n",
    "            pssm = motif.log_odds()\n",
    "            \n",
    "            # apply convolution on both strands\n",
    "            hiddenActivation[motifCount*2,:] = pssm.calculate(seq) + self.bias[motifCount]\n",
    "            hiddenActivation[motifCount*2+1,:] = pssm.calculate(seq.reverse_complement()) + self.bias[motifCount]\n",
    "            hiddenActivation[motifCount*2:motifCount*2+2,:] = self._probMaxPooling(hiddenActivation[motifCount*2:motifCount*2+2,:])\n",
    "            motifCount += 1\n",
    "\n",
    "        return hiddenActivation\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the backward pass on the data, that is P(V | H).\n",
    "    It does so by performing convolution of the hidden layer and the data for each letter\n",
    "    in the alphabet, respecting both strands.\n",
    "    The de-convolutions for all kernels are added and the final sequence is obtained\n",
    "    using the softmax over all four letters.\n",
    "    \n",
    "    Parameters:\n",
    "    hidden layer:   The layer that was previously computed by the forward pass.\n",
    "    \n",
    "    Returns:        The reconstructed sequence. \n",
    "    \"\"\"\n",
    "    def backwardPass (self, hiddenActivation):\n",
    "        \n",
    "        # apply convolution on all of the filters\n",
    "        restoredLength = hiddenActivation.shape[1] + self.motifLength - 1\n",
    "        numOfLetters = len(self.alphabet.letters)\n",
    "        reConv = np.zeros((numOfLetters, restoredLength))\n",
    "\n",
    "        #start = time.time()\n",
    "        reConv = np.zeros((numOfLetters, restoredLength))\n",
    "        for k in range(len(self.motifs)):\n",
    "            # apply convolution on each of the channels (A, C, G, T) seperately\n",
    "            matrix = self._convertPWM2Array(self.motifs[k])\n",
    "            for i in range(numOfLetters):\n",
    "                conv1 = np.convolve(hiddenActivation[k], matrix[i])\n",
    "                conv2 = np.convolve(hiddenActivation[k+1], matrix[i])\n",
    "                reConv[i,:] = reConv[i,:] + conv1 + conv2 + self.c\n",
    "\n",
    "        #convTime = time.time()\n",
    "        # perform softmax and select index of the most promising one (results in visibleActivation)\n",
    "        visibleActivation = np.zeros((1, restoredLength))\n",
    "        \n",
    "        # calculate exp for whole matrix\n",
    "        reConv = np.exp(reConv)\n",
    "        \n",
    "        # calculate the sum (over all four letters for whole sequence)\n",
    "        sums = np.sum(reConv, 0)\n",
    "\n",
    "        # divide by the sum\n",
    "        for i in range(numOfLetters):\n",
    "            reConv[i,:] = reConv[i,:] / sums[i]\n",
    "            \n",
    "        # and the maximum is our letter...\n",
    "        visibleActivation = np.argmax(reConv, 0)\n",
    "        \n",
    "        #print \"Done with Softmax in: \" + str((time.time()-convTime)*1000)\n",
    "        # convert the resulting sequence to actual letters (A, C, G, T instead of 0, 1, 2, 3)\n",
    "        return self._getDNASeqFromNumericals(visibleActivation)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    The training algorithm for cRBMs. This uses the forward and backward pass\n",
    "    to collect the statistics for them.\n",
    "    \"\"\"\n",
    "    def miniBatchTraining (self, sequences, batchSize, cd_value):\n",
    "        \n",
    "        # first, some vars that we need all the time\n",
    "        sequenceLength = len(sequences[0])\n",
    "        hiddenUnitLength = sequenceLength - self.motifLength + 1\n",
    "        \n",
    "        # first, compute expected value of the data\n",
    "        # that means computing forward pass for all datapoints and computing the gradient\n",
    "        hiddenActivation = np.zeros((2*self.numMotifs, hiddenUnitLength))\n",
    "        derivatives = np.zeros((self.motifLength, self.numMotifs))\n",
    "        for seq in sequences:\n",
    "            hiddenActivation = self.forwardPass(seq)\n",
    "            \n",
    "            # calculate the gradients of the data\n",
    "            # In order to do that, we have to convolve the DNA and hidden layer\n",
    "            # This can be done by expressing DNA as four different convolutions (for each letter)\n",
    "            # We would need to represent the DNA as matrix of 4 x lengthOfDHS\n",
    "            # Example:\n",
    "            # -------\n",
    "            # ACGTGGGG\n",
    "            # 10000000\n",
    "            # 01000000\n",
    "            # 00101111\n",
    "            # 00010000\n",
    "            # Then, we can apply 4 convolutions to the sequence\n",
    "            for k in range(self.numMotifs):\n",
    "                derivatives[k,:] += np.convolve( hiddenActivation[k,:])\n",
    "            \n",
    "        print derivatives\n",
    "        # Now, sample from this distribution and collect the model's statistics\n",
    "        # Do that by sampling from the data statistics, recompute the model's statistics\n",
    "        # and so on until reaching the stationary distribution (in case of CD, do it once)\n",
    "        hidden_model = hiddenActivation\n",
    "        for it in range(cd_value):\n",
    "            sample = self._sampleFromHidden(hidden_model)\n",
    "            visible = self.backwardPass(sample)\n",
    "            hidden_model = self.forwardPass(visible)\n",
    "\n",
    "        print hidden_model.shape\n",
    "        \n",
    "        return self.learningRate * (hiddenActivation - hidden_model)\n",
    "            \n",
    "    \n",
    "    def _probMaxPooling (self, h_k):\n",
    "\n",
    "        # first of all some easy definitions\n",
    "        l = h_k.shape[1]\n",
    "        numOfGroups = l/self.poolingFactor\n",
    "        P = np.zeros((2, l))\n",
    "\n",
    "        # exponent of everything\n",
    "        ex = np.exp(h_k)\n",
    "        \n",
    "        # reshape s.t. each group forms one row\n",
    "        newDim = (numOfGroups, -1)\n",
    "        reordered = np.append(ex[0].reshape(newDim), ex[1].reshape(newDim), 1)\n",
    "        #print \"Shape of reordered: \" + str(reordered.shape)\n",
    "        # calculate denominators (sum of all rows)\n",
    "        denoms = np.sum(reordered, 1) + 1 # denoms for all groups (add 1 to have log. unit)\n",
    "        res = np.argmax(reordered.T / denoms, 0)\n",
    "\n",
    "        # calculate the actual values of the pooling layer P\n",
    "        for group in range(numOfGroups):\n",
    "            if reordered[group,res[group]] > 1: # check if really any element from P should be active\n",
    "                # we don't care about strands so just set res = res/2 for the index\n",
    "                idx = group * self.poolingFactor + int(res[group]/2)\n",
    "                P[res[group] % 2, idx] = reordered[group,res[group]] / denoms[group]\n",
    "        return P\n",
    "        \n",
    "    def _createRandomMotif (self, motifLength, alphabet):\n",
    "        counts = {}\n",
    "        for letter in alphabet.letters:\n",
    "            counts[letter] = [random.randint(0,100) for x in xrange(motifLength)]\n",
    "        return mat.PositionWeightMatrix(alphabet, counts)\n",
    "        \n",
    "    def _sampleFromHidden (self, hiddenActivation):\n",
    "        boolean_mat = hiddenActivation > self.rng.random_sample(hiddenActivation.shape)\n",
    "        return boolean_mat.astype(int)\n",
    "        \n",
    "        \n",
    "    def _sigmoid (self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _softmaxActivation (self, col, idx):\n",
    "        p_all = np.sum(np.exp(col))\n",
    "        p_x = np.exp(col[idx])\n",
    "        return p_x / p_all\n",
    "\n",
    "    def _getLetterToInt (self, num):\n",
    "        if num == 0:\n",
    "            return 'A'\n",
    "        elif num == 1:\n",
    "            return 'C'\n",
    "        elif num == 2:\n",
    "            return 'G'\n",
    "        elif num == 3:\n",
    "            return 'T'\n",
    "        else:\n",
    "            print 'ERROR: Num ' + str(num) + \" not a valid char in DNA alphabet\"\n",
    "            return -1\n",
    "\n",
    "    def _getDNASeqFromNumericals (self, seq):\n",
    "        dna_seq = []\n",
    "        for num in range(seq.shape[0]):\n",
    "            dna_seq.append(self._getLetterToInt(seq[num]))\n",
    "        return Seq(\"\".join(dna_seq), alphabet=self.alphabet)\n",
    "\n",
    "    def _convertPWM2Array (self, pwm):\n",
    "        result = np.zeros((len(self.alphabet.letters), self.motifLength))\n",
    "        for letter in range(len(pwm)):\n",
    "            result[letter] = pwm[self._getLetterToInt(letter)]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "convolve() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-670528c95fe1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# take some sequences and learn the 2 motifs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdata_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminiBatchTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Time for minibatch training (in sec): \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-122-36b810ee581e>\u001b[0m in \u001b[0;36mminiBatchTraining\u001b[1;34m(self, sequences, batchSize, cd_value)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;31m# Then, we can apply 4 convolutions to the sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumMotifs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0mderivatives\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mhiddenActivation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mderivatives\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: convolve() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "# initialize cRBM\n",
    "L = ConvRBM(4,2)\n",
    "L.initializePWMs()\n",
    "\n",
    "# create test set of sequences\n",
    "testSet = [allSeqs[random.randrange(0, len(allSeqs))] for x in range(100)]\n",
    "# take some sequences and learn the 2 motifs\n",
    "start = time.time()\n",
    "data_stats = L.miniBatchTraining(testSet, 1, 1)\n",
    "print \"Time for minibatch training (in sec): \" + str(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code from someone else that did it with theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T_activation_v_OLD(self, h):\n",
    "    # vbias is per visible dimension\n",
    "    # The first two dimensions of the weights tensor have to be swapped here,\n",
    "    # because they represent the number of input and output feature maps.\n",
    "    W_shuffled = self.W.dimshuffle(1, 0, 2, 3)\n",
    "    filter_shape = [self.filter_shape[k] for k in [1, 0, 2, 3]]\n",
    "    return conv.conv2d(h, W_shuffled, border_mode='full', image_shape=self.hiddens_shape, filter_shape=filter_shape) + self.vbias.dimshuffle('x', 'x', 0, 'x')\n",
    "\n",
    "\n",
    "def T_activation_v(self, h):\n",
    "    # this version of T_activation_v swaps 'dim' and 'hmaps'dimensions,\n",
    "    # because this seems to be a LOT faster.\n",
    "\n",
    "    # It seems to be a little less precise than the original, butotherwise\n",
    "    # the output is identical.\n",
    "\n",
    "    W_shuffled = self.W.dimshuffle(2, 1, 0, 3)\n",
    "\n",
    "    # The hmaps dimension needs to be flipped, because it waspreviously not convolved over,\n",
    "    # and now it is. Note that a valid convolution with filtersize =inputsize is NOT\n",
    "    # equivalent with a product - this is only the case if you FLIPthe filter.\n",
    "    # Note that the dim dimension need not be flipped, because this isconvolved over\n",
    "    # when computing T_activation_h (so it is already inherentlyflipped).\n",
    "    W_shuffled = W_shuffled[:,:,::-1,:]\n",
    "\n",
    "    h_shuffled = h.dimshuffle(0, 2, 1, 3)\n",
    "\n",
    "    # this is a full convolution in the time direction and a valid onein the other,\n",
    "    # hence the need to pad the input manually.\n",
    "    zero_padding = T.zeros_like(h_shuffled)[:,:,:,0:self.filter_width-1]\n",
    "    h_padded = T.concatenate([zero_padding, h_shuffled, zero_padding],axis=3)\n",
    "\n",
    "    filter_shape = [self.filter_shape[k] for k in [2, 1, 0, 3]]\n",
    "    image_shape = [self.hiddens_shape[k] for k in [0, 2, 1, 3]]\n",
    "    image_shape[3] += 2*(self.filter_width-1)\n",
    "    tmp = conv.conv2d(h_padded, W_shuffled, border_mode='valid',image_shape=image_shape, filter_shape=filter_shape)\n",
    "    return tmp.dimshuffle(0, 2, 1, 3) + self.vbias.dimshuffle('x','x', 0, 'x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test how to improve representation of DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small test matrix converstion took (in ms): 0.379085540771\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "Conversion from DNA to matrix took: 23.2509000301\n",
      "[[ 0.  1.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  0.  1.  1.  1.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.\n",
      "   0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  1.\n",
      "   1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "   0.  1.  0.  1.  1.  0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.  1.\n",
      "   1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.\n",
      "   0.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "   1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.\n",
      "   1.  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.\n",
      "   0.  0.  1.  0.  0.  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.\n",
      "   0.  0.  0.  1.  1.  0.  1.  0.  0.  1.  1.  1.  0.  1.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.\n",
      "   0.  0.  1.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    numOfLetters = len(seq.alphabet.letters)\n",
    "    result = np.zeros((numOfLetters, len(seq)))\n",
    "    for i in range(len(seq)):\n",
    "        result[getIntToLetter(seq[i]),i] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "start1 = time.time()\n",
    "randSeq = Seq(\"ACGTGGGG\", L.alphabet)\n",
    "randSeq = allSeqs[random.randrange(0, len(allSeqs))]\n",
    "randSeq = randSeq.upper()\n",
    "m = getMatrixFromSeq(randSeq)\n",
    "print \"Small test matrix converstion took (in ms): \" + str((time.time()-start1)*1000)\n",
    "start = time.time()\n",
    "for seq in allSeqs:\n",
    "    getMatrixFromSeq(seq)\n",
    "print \"Conversion from DNA to matrix took: \" + str(time.time()-start)\n",
    "print m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a simple test sequence to verify that forward and backward pass work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer:\n",
      "[[ 0.          0.          0.          0.          0.99966465]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.99966465  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.99966465]]\n",
      "Reconstruction:\n",
      "ACGTGGGG\n"
     ]
    }
   ],
   "source": [
    "learner = ConvRBM(4, 2)\n",
    "learner.bias = np.zeros(2)\n",
    "learner.c = 0\n",
    "# create PWM (that will look for a sequence of 4 Gs)\n",
    "counts = {}\n",
    "for letter in learner.alphabet.letters:\n",
    "    if letter != 'G':\n",
    "        counts[letter] = [0 for x in xrange(learner.motifLength)]\n",
    "    else:\n",
    "        counts[letter] = [1 for x in xrange(learner.motifLength)]\n",
    "kernel = mat.PositionWeightMatrix(learner.alphabet, counts)\n",
    "learner.motifs.append(kernel)\n",
    "\n",
    "# create second kernel that will look for ACGT\n",
    "counts = {}\n",
    "for letter in range(len(learner.alphabet.letters)):\n",
    "    counts[learner._getLetterToInt(letter)] = [int(x == letter) for x in xrange(learner.motifLength)]\n",
    "\n",
    "kernel = mat.PositionWeightMatrix(learner.alphabet, counts)\n",
    "learner.motifs.append(kernel)\n",
    "\n",
    "# set the cRBMs other params to 0\n",
    "learner.bias = np.zeros(learner.numMotifs)\n",
    "learner.c = 0\n",
    "\n",
    "# now test forward pass on a simple sequence\n",
    "testSeq = Seq(\"ACGTGGGG\", learner.alphabet)\n",
    "h = learner.forwardPass(testSeq)\n",
    "print \"Hidden Layer:\"\n",
    "print h\n",
    "\n",
    "maxPooled = learner._probMaxPooling(h[:2])\n",
    "reconstructed = learner.backwardPass(h)\n",
    "print \"Reconstruction:\"\n",
    "print reconstructed\n",
    "\n",
    "# Should be able to completely reconstruct the sequence because we gave two kernels\n",
    "# which both can be hold responsible for a portion of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for forward: 4.03904914856\n",
      "Time for backward: 1.16205215454\n"
     ]
    }
   ],
   "source": [
    "x = ConvRBM(4, 10, 1)\n",
    "x.initializePWMs()\n",
    "\n",
    "testSeq = allSeqs[7190]\n",
    "startForward = time.time()\n",
    "h = x.forwardPass(testSeq)\n",
    "print \"Time for forward: \" + str((time.time()-startForward)*1000)\n",
    "startBackward = time.time()\n",
    "x.backwardPass(h)\n",
    "print \"Time for backward: \" + str((time.time()-startBackward)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the JASPAR data and see whether the sequences can be reproduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of JASPAR matrices: 593\n",
      "Average motif length (k-mer length): 10.7993254637\n",
      "\n",
      "Number of motifs with length 6 : 37\n",
      "Verfication: 6.0\n",
      "average correct: 28.935\n",
      "average error: 121.065\n",
      "var of error: 204.920775\n",
      "average time for forward and backward pass (in ms): 0.828126907349\n"
     ]
    }
   ],
   "source": [
    "l = 6\n",
    "numMats = len(pwms)\n",
    "avgLength = np.mean([len(x[0]) for x in pwms])\n",
    "matsOfSpecificLength = [x for x in pwms if len(x[0]) == l]\n",
    "avgRedLength = np.mean([len(x[0]) for x in matsOfSpecificLength])\n",
    "print \"Total number of JASPAR matrices: \" + str(numMats)\n",
    "print \"Average motif length (k-mer length): \" + str(avgLength)\n",
    "print\n",
    "print \"Number of motifs with length \" + str(l) + \" : \" + str(len(matsOfSpecificLength))\n",
    "print \"Verfication: \" + str(avgRedLength)\n",
    "\n",
    "cRBM = ConvRBM(11, len(matsOfSpecificLength))\n",
    "cRBM.bias = np.zeros(len(matsOfSpecificLength))\n",
    "cRBM.c = 0\n",
    "# insert our pwms\n",
    "cRBM.motifs = matsOfSpecificLength\n",
    "\n",
    "# perform forward and backward pass\n",
    "correct = []\n",
    "errors = []\n",
    "times = []\n",
    "for i in range(1000):\n",
    "    testSeq = allSeqs[random.randrange(0, len(allSeqs))]\n",
    "    start = time.time()\n",
    "    hiddenActivation = learner.forwardPass(testSeq)\n",
    "    restored = learner.backwardPass(hiddenActivation)\n",
    "    times.append(time.time()-start)\n",
    "    \n",
    "    # count the differences between the two sequences\n",
    "    differences = 0\n",
    "    for elem in range(len(string)):\n",
    "        if string[elem] != testSeq[elem]:\n",
    "            differences += 1\n",
    "\n",
    "    #print \"Correct: \" + str(len(string)-differences)\n",
    "    #print \"Errors: \" + str(differences)\n",
    "    correct.append(len(string)-differences)\n",
    "    errors.append(differences)\n",
    "    \n",
    "print \"average correct: \" + str(np.mean(correct))\n",
    "print \"average error: \" + str(np.mean(errors))\n",
    "print \"var of error: \" + str(np.var(errors))\n",
    "print \"average time for forward and backward pass (in ms): \" + str(np.mean(times)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the forward and backward pass do anything meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05172414  0.02727273  0.53293413  0.27492447  0.02362205  0.28571429]\n",
      " [ 0.70689655  0.39090909  0.25149701  0.17522659  0.33858268  0.00840336]\n",
      " [ 0.20689655  0.25454545  0.20359281  0.28700906  0.07874016  0.05882353]\n",
      " [ 0.03448276  0.32727273  0.01197605  0.26283988  0.55905512  0.64705882]]\n",
      "Restored:\n",
      "['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'C', 'C', 'A', 'G', 'T', 'T', 'C', 'C', 'A', 'C', 'T', 'A']\n",
      "\n",
      "Original:\n",
      "TTTATCCTGCAGCTCGCCTG\n",
      "Now the real implementation:\n",
      "Restored:\n",
      "['A', 'A', 'A', 'A', 'A', 'C', 'C', 'A', 'C', 'T', 'A', 'G', 'T', 'T', 'C', 'C', 'A', 'C', 'T', 'A']\n"
     ]
    }
   ],
   "source": [
    "learner = ConvRBM(6, 1)\n",
    "learner.initializePWMs()\n",
    "\n",
    "hiddenActivation = learner.forwardPass(allSeqs[246])\n",
    "restoredLength = hiddenActivation.shape[1] + learner.motifLength - 1\n",
    "reConv = np.zeros((len(learner.alphabet.letters), restoredLength))\n",
    "matrix = learner._convertPWM2Array(learner.motifs[0])\n",
    "print matrix\n",
    "for i in range(len(learner.alphabet.letters)):\n",
    "    reConv[i,:] = np.convolve(hiddenActivation[0], matrix[i])\n",
    "    \n",
    "#print reConv\n",
    "\n",
    "def softmaxActivation(col, idx):\n",
    "    p_all = np.sum(np.exp(col))\n",
    "    p_x = np.exp(col[idx])\n",
    "    return p_x / p_all\n",
    "\n",
    "visibleActivation = np.zeros((1, 150))\n",
    "for i in range(150):\n",
    "    visibleActivation[0,i] = np.argmax([softmaxActivation(reConv[:,i], x) for x in range(4)])\n",
    "    \n",
    "def convertNumericalToLetter(seq):\n",
    "    dna_seq = []\n",
    "    for num in range(seq.shape[1]):\n",
    "        dna_seq.append(learner._getLetterToInt(seq[0,num]))\n",
    "    return dna_seq\n",
    "\n",
    "print \"Restored:\"\n",
    "print convertNumericalToLetter(visibleActivation)[:20]\n",
    "print\n",
    "print \"Original:\"\n",
    "print allSeqs[246][:20]\n",
    "\n",
    "print \"Now the real implementation:\"\n",
    "print \"Restored:\"\n",
    "print learner.backwardPass(hiddenActivation)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the forward pass on the whole set of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0.0\n",
      "5000 -> 0.0\n",
      "10000 -> 0.0\n",
      "15000 -> 0.0\n",
      "20000 -> 0.0\n",
      "25000 -> 0.842827207671\n",
      "30000 -> 0.0\n",
      "35000 -> 0.0\n",
      "40000 -> 0.0\n",
      "45000 -> 0.0\n",
      "50000 -> 0.561771898779\n",
      "55000 -> 0.0\n",
      "60000 -> 0.0\n",
      "65000 -> 0.0\n",
      "70000 -> 0.0\n",
      "75000 -> 0.0\n",
      "80000 -> 0.58698077005\n",
      "85000 -> 0.0\n",
      "90000 -> 0.0\n",
      "95000 -> 0.0\n",
      "100000 -> 0.0\n",
      "105000 -> 0.0\n",
      "110000 -> 0.692016137551\n",
      "115000 -> 0.0\n",
      "120000 -> 0.0\n",
      "125000 -> 0.0\n",
      "130000 -> 0.0\n",
      "135000 -> 0.0\n",
      "140000 -> 0.0\n",
      "145000 -> 0.0\n",
      "150000 -> 0.0\n",
      "155000 -> 0.465888195173\n",
      "160000 -> 0.0\n",
      "165000 -> 0.0\n",
      "170000 -> 0.0\n",
      "\n",
      "\n",
      "Number of filters: 1\n",
      "Number of DHSs: 171275\n",
      "Average Length of Sequences: 150.0\n",
      "Execution Time: 62.1381750107\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 0\n",
    "lengthes = []\n",
    "someScores = []\n",
    "for seq in allSeqs:\n",
    "    convoluted = learner.forwardPass(seq)\n",
    "    lengthes.append(len(seq))\n",
    "    if i % 5000 == 0:\n",
    "        someScores.append(convoluted[0][random.randint(0, len(convoluted))])\n",
    "        print str(i) + \" -> \" + str(someScores[-1])\n",
    "    i += 1\n",
    "    \n",
    "print\n",
    "print\n",
    "print \"Number of filters: \" + str(learner.numMotifs)\n",
    "print \"Number of DHSs: \" + str(i)\n",
    "print \"Average Length of Sequences: \" + str(np.array(lengthes).mean())\n",
    "print \"Execution Time: \" + str(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test both passes on all sequences using parallelization (just CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE CPUs: 4\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5c582c38f323>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"AVAILABLE CPUs: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0msizePerCPU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallSeqs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0msublists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_repopulate_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         self._worker_handler = threading.Thread(\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36m_repopulate_pool\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Process'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PoolWorker'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'added worker'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0m_current_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'random'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "from multiprocessing.pool import Pool\n",
    "\n",
    "def calculatePassesForSeqs(seqs):\n",
    "    print \"Started thread with \" + str(len(seqs)) + \" Sequences!\"\n",
    "    lengthes = []\n",
    "    for seq in seqs:\n",
    "        hiddenActivation = learner.forwardPass(seq)\n",
    "        reconstruction = learner.backwardPass(hiddenActivation)\n",
    "        lengthes.append(len(seq))\n",
    "    return np.mean(lengthes)\n",
    "\n",
    "\n",
    "cpu_count = 4\n",
    "print \"AVAILABLE CPUs: \" + str(cpu_count)\n",
    "sizePerCPU = len(allSeqs) / cpu_count\n",
    "p = Pool(processes = cpu_count)\n",
    "sublists = []\n",
    "for i in range(cpu_count):\n",
    "    if not i == cpu_count-1:\n",
    "        sublists.append(allSeqs[i*sizePerCPU:(i+1)*sizePerCPU])\n",
    "    else:\n",
    "        sublists.append(allSeqs[i*sizePerCPU:])\n",
    "start = time.time()\n",
    "result = p.map(calculatePassesForSeqs, sublists)\n",
    "print result\n",
    "print\n",
    "print\n",
    "print \"Number of filters: \" + str(learner.numMotifs)\n",
    "print \"Number of DHSs: \" + str(len(allSeqs))\n",
    "print \"Execution Time: \" + str(time.time()-start)\n",
    "\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-482-3fd54ce79a77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests to learn how to do things with Biopython and Theano\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all DHS sequences have the same length by default?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 171275\n",
      "Number of seqs with length != 150: 0\n"
     ]
    }
   ],
   "source": [
    "fasta_seqs = sio.parse(open('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa'), 'fasta')\n",
    "count = 0\n",
    "countNotSameLength = 0\n",
    "for seq in fasta_seqs:\n",
    "    if len(seq) != 150:\n",
    "        print 'not length 150'\n",
    "        countNotSameLength += 1\n",
    "    count += 1\n",
    "\n",
    "print 'Number of sequences: ' + str(count)\n",
    "print 'Number of seqs with length != 150: ' + str(countNotSameLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we generate random motif matrices (PWMs or PSSMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Bio.NeuralNetwork.Gene.Schema as schema\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import IUPAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTGCGAT\n",
      "CGTGACCA\n",
      "GTACACCGA\n"
     ]
    }
   ],
   "source": [
    "alphabet = IUPAC.unambiguous_dna\n",
    "generator = schema.RandomMotifGenerator(alphabet, 6, 10)\n",
    "for i in range(3):\n",
    "    x = generator.random_motif()\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet.letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Bio.motifs.matrix as mat\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createRandomMotif (motifLength, alphabet):\n",
    "    counts = {}\n",
    "    for letter in alphabet.letters:\n",
    "        counts[letter] = [random.randint(0,100) for x in xrange(motifLength)]\n",
    "    return mat.PositionWeightMatrix(alphabet, counts)\n",
    "#x = mat.PositionWeightMatrix(alphabet, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0      1      2      3      4      5      6      7      8      9\n",
      "A:   0.24   0.18   0.19   0.25   0.41   0.47   0.09   0.07   0.21   0.29\n",
      "C:   0.28   0.26   0.16   0.42   0.13   0.24   0.39   0.49   0.16   0.38\n",
      "G:   0.20   0.24   0.22   0.00   0.14   0.15   0.46   0.43   0.63   0.18\n",
      "T:   0.29   0.32   0.43   0.32   0.32   0.14   0.06   0.02   0.00   0.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = createRandomMotif(10, alphabet)\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.24,  0.18,  0.19,  0.25,  0.41,  0.47,  0.09,  0.07,  0.21,\n",
       "         0.29],\n",
       "       [ 0.28,  0.26,  0.16,  0.42,  0.13,  0.24,  0.39,  0.49,  0.16,\n",
       "         0.38],\n",
       "       [ 0.2 ,  0.24,  0.22,  0.  ,  0.14,  0.15,  0.46,  0.43,  0.63,\n",
       "         0.18],\n",
       "       [ 0.29,  0.32,  0.43,  0.32,  0.32,  0.14,  0.06,  0.02,  0.  ,\n",
       "         0.14]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((len(alphabet.letters), 10))\n",
    "\n",
    "def getLetterToInt (num):\n",
    "    if num == 0:\n",
    "        return 'A'\n",
    "    elif num == 1:\n",
    "        return 'C'\n",
    "    elif num == 2:\n",
    "        return 'G'\n",
    "    elif num == 3:\n",
    "        return 'T'\n",
    "    else:\n",
    "        print 'ERROR: Num ' + str(num) + \" not a valid char in DNA alphabet\"\n",
    "        return -1\n",
    "\n",
    "\n",
    "for letter in range(len(x)):\n",
    "    print letter\n",
    "    y[letter] = x[getLetterToInt(letter)]\n",
    "    letterCount += 1\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the elements of a PWM interpretable as probabilites?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 1.0\n",
      "1 -> 1.0\n",
      "2 -> 1.0\n",
      "3 -> 1.0\n",
      "4 -> 1.0\n",
      "5 -> 1.0\n",
      "6 -> 1.0\n",
      "7 -> 1.0\n",
      "8 -> 1.0\n",
      "9 -> 1.0\n"
     ]
    }
   ],
   "source": [
    "# verify that we're dealing with probabilities by summing up over all letters for each position\n",
    "for pos in range(x.length):\n",
    "    c = 0\n",
    "    for letter in alphabet.letters:\n",
    "        c += x[letter][pos]\n",
    "    print str(pos) + \" -> \" + str(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
