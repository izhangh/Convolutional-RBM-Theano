{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Implementation vs GPU code\n",
    "This is for debugging purposes only, to see if the GPU implementation is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '26860' (I am process '28057')\n",
      "WARNING:theano.gof.compilelock:Overriding existing lock by dead process '26860' (I am process '28057')\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "# the underlying convRBM implementation\n",
    "sys.path.append(os.path.abspath('../code'))\n",
    "from convRBM import CRBM\n",
    "import getData as dataRead\n",
    "\n",
    "# biopython stuff\n",
    "#import Bio.SeqIO as sio\n",
    "#import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "#from Bio import motifs\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the naive cRBM\n",
    "All formulas are the same as in the writing section of our git repo.\n",
    "We implement all the basic functions for upward pass, downward pass, prob max pooling, derivative calculation, gibbs sampling and a training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveCRBM:\n",
    "\n",
    "    def __init__ (self, motifLength=1, numMotifs=1, learningRate=0.1, poolingFactor=1):\n",
    "        self.numberOfKernels = numMotifs\n",
    "        self.kernelLength = motifLength\n",
    "        self.poolingFactor = poolingFactor\n",
    "        self.learningRate = learningRate\n",
    "        self.setParamsToZero = True\n",
    "        self.debug = True\n",
    "        self.updateWeights = True\n",
    "        if self.setParamsToZero:\n",
    "            self.kernels = np.zeros((self.numberOfKernels, 1, 4, self.kernelLength))\n",
    "            self.bias = np.zeros(self.numberOfKernels)\n",
    "            self.c = np.zeros(4)\n",
    "        else:\n",
    "            self.kernels = np.random.rand(self.numberOfKernels, 1, 4, self.kernelLength)\n",
    "            self.bias = np.random.rand(self.numberOfKernels)\n",
    "            self.c = np.random.rand(4)\n",
    "    \n",
    "    def setCustomKernels (self, kernels):\n",
    "        self.numberOfKernels = kernels.shape[0]\n",
    "        self.kernelLength = kernels.shape[3]\n",
    "        self.kernels = kernels.astype(float)\n",
    "        if self.setParamsToZero:\n",
    "            self.bias = np.zeros(self.numberOfKernels)\n",
    "        else:\n",
    "            self.bias = np.random.rand(self.numberOfKernels)\n",
    "\n",
    "    def initializeMotifs (self):\n",
    "        pass\n",
    "        \n",
    "    def complement (self, kernelSlice):\n",
    "        return kernelSlice[::-1]\n",
    "\n",
    "    def computeHgivenV (self, data):\n",
    "        N_h = data.shape[3]-self.kernelLength+1\n",
    "        H = np.zeros((data.shape[0], self.numberOfKernels, 1, N_h))\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n in range(N_h):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        # calculate the x_i, that is the cross-correlation\n",
    "                        x = data[sample,0,:,n+m].T.dot(self.kernels[k,0,:,m]) + self.bias[k]\n",
    "                        #cKernel = self.complement(self.kernels[k,0,:,self.kernelLength-m-1])\n",
    "                        #x_prime = data[sample,0,:,n+m].T.dot(cKernel) + self.bias[k]\n",
    "                        H[sample, k, 0, n] += x # + x_prime\n",
    "        \n",
    "        if self.debug:\n",
    "            print \"Pre Sigmoid Hidden Layer:\"\n",
    "            print H\n",
    "        # perform prob max pooling\n",
    "        P = np.zeros(H.shape)\n",
    "        S = np.zeros(H.shape)\n",
    "        H_exp = np.exp(H)\n",
    "        numBins = N_h / self.poolingFactor\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k_pos in range(0, self.numberOfKernels, 1):\n",
    "                for unit in range(numBins):\n",
    "                    #print \"Doing unit: \" + str(unit)\n",
    "                    # calculate sum within unit\n",
    "                    sumInUnit = 0\n",
    "                    for cell in range(self.poolingFactor):\n",
    "                        curPos = unit*self.poolingFactor+cell\n",
    "                        sumInUnit += H_exp[sample,k_pos,0,curPos]# + H_exp[sample,k_pos+1,0,curPos]\n",
    "                        \n",
    "                    # now, calculate the single positions in P\n",
    "                    arr = []\n",
    "                    for cell in range(self.poolingFactor):\n",
    "                        curPos = unit*self.poolingFactor+cell\n",
    "                        P[sample,k_pos,0,curPos] = H_exp[sample,k_pos,0,curPos] / (sumInUnit + 1)\n",
    "                        #P[sample,k_pos+1,0,curPos] = H_exp[sample,k_pos+1,0,curPos] / (sumInUnit + 1)\n",
    "                        arr.append(P[sample,k_pos,0,curPos])\n",
    "                        #arr.append(P[sample,k_pos+1,0,curPos])\n",
    "                    \n",
    "                    # finally, do the sampling step\n",
    "                    arr.append(1 / (sumInUnit+1))\n",
    "                    s = np.random.multinomial(n=1, pvals=np.array(arr),size=1)\n",
    "                    am = np.argmax(s)\n",
    "                    if am < self.poolingFactor:#*2:\n",
    "                        strand = am % 2\n",
    "                        pos = unit * self.poolingFactor + am #(am // 2)\n",
    "                        #print \"Strand: \" + str(strand) + \" Pos: \" + str(pos)\n",
    "                        S[sample,k_pos,0,pos] = 1\n",
    "        return [P,S]\n",
    "\n",
    "\n",
    "    def computeVgivenH (self, H):\n",
    "        \n",
    "        # calculate full convolution (not valid, therefore padding is applied with zeros)\n",
    "        N_v = H.shape[3] + self.kernelLength - 1\n",
    "        pad = self.kernelLength-1\n",
    "        V = np.zeros((H.shape[0],1,4,N_v))\n",
    "        Y = np.zeros(V.shape)\n",
    "        H_pad = np.pad(H,[(0,0),(0,0),(0,0),(pad, pad)], 'constant',constant_values=(0,0))\n",
    "        for sample in range(H.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n in range(N_v):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        Y[sample,0,:,n] += self.kernels[k,0,:,m] * H_pad[sample,k,0,pad+n-m]\n",
    "                        \n",
    "        # calculate softmax on convolved data\n",
    "        P_V = self.softmax(Y)\n",
    "        \n",
    "        # sample the visible layer from probabilities\n",
    "        V = np.zeros(P_V.shape)\n",
    "        for sample in range(P_V.shape[0]):\n",
    "            for col in range(P_V.shape[3]):\n",
    "                V[sample,0,:,col] = np.random.multinomial(n=1,pvals=P_V[sample,0,:,col],size=1)\n",
    "        \n",
    "        return [P_V, V]\n",
    "        \n",
    "\n",
    "    def collectUpdateStatistics (self, H, data):\n",
    "        G = np.zeros(self.kernels.shape)\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n_h in range(H.shape[3]):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        G[k,0,:,m] += data[sample,0,:,n_h+m] * H[sample,k,0,n_h]\n",
    "\n",
    "        der_bias = np.mean(np.mean(H, axis=3), axis=0).reshape(-1)\n",
    "        der_c = np.mean(np.mean(data, axis=3), axis=0).reshape(-1)\n",
    "        return [G, der_bias, der_c]\n",
    "    \n",
    "    def updateWeightsOnMinibatch (self, D, numOfCDs):\n",
    "        # calculate the data gradient for weights (motifs) and bias\n",
    "        [P_H_data, H_data] = self.computeHgivenV(D)\n",
    "        if self.debug:\n",
    "            print \"Hidden Layer Probabilities:\"\n",
    "            print P_H_data\n",
    "            print \"Hidden Layer Sample:\"\n",
    "            print H_data\n",
    "\n",
    "        # calculate data gradients\n",
    "        [G_motif_data, G_bias_data, G_c_data] = self.collectUpdateStatistics(P_H_data, D)\n",
    "\n",
    "        if self.debug:\n",
    "            print \"Data gradient for motifs\"\n",
    "            print G_motif_data\n",
    "\n",
    "        # calculate model probs\n",
    "        H = H_data\n",
    "        for i in range(numOfCDs):\n",
    "            [P_V, V] = self.computeVgivenH(H)\n",
    "            if self.debug:\n",
    "                print \"Visible Sample for CD \" + str(i)\n",
    "                print V\n",
    "            [P_H_model, H] = self.computeHgivenV(V)\n",
    "        \n",
    "        # compute the model gradients\n",
    "        [G_motif_model, G_bias_model, G_c_model] = self.collectUpdateStatistics(P_H_model, V)\n",
    "        \n",
    "        if self.debug:\n",
    "            print \"Model gradient for motifs:\"\n",
    "            print G_motif_model\n",
    "        \n",
    "        # update the parameters\n",
    "        new_kernels = self.learningRate * (G_motif_data - G_motif_model)\n",
    "        new_bias = self.learningRate * (G_bias_data - G_bias_model)\n",
    "        new_c = self.learningRate * (G_c_data - G_c_model)\n",
    "\n",
    "        if self.updateWeights:\n",
    "            self.kernels += new_kernels\n",
    "            self.bias += new_bias\n",
    "            self.c += new_c\n",
    "\n",
    "        return (new_kernels, new_bias, new_c)\n",
    "\n",
    "        \n",
    "    def trainModel (self, trainData, epochs, batchSize, numOfCDs):\n",
    "        iterations = trainData.shape[0] / batchSize\n",
    "        for epoch in range(epochs):\n",
    "            for batchIdx in range(iterations):\n",
    "                self.updateWeightsOnMinibatch(trainData[batchIdx*batchSize:(batchIdx+1)*batchSize], numOfCDs)\n",
    "        \n",
    "    def softmax (self, x):\n",
    "        return np.exp(x) / np.exp(x).sum(axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct toy data to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: [[[[1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]]]]\n",
      "Data shape: (1, 1, 4, 8)\n",
      "[[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  1.  1.  1.  1.]\n",
      "   [ 0.  0.  0.  1.  0.  0.  0.  0.]]]]\n",
      "(2, 1, 4, 3)\n",
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'batch_size': 1,\n",
      " 'cd_k': 1,\n",
      " 'epochs': 100,\n",
      " 'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 1}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "kernel1 = np.tile(np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]]), [1,1,1])\n",
    "kernel1_ = np.tile(np.flipud(np.fliplr(kernel1[0])),[1,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0],[0,0,0],[1,1,1],[0,0,0]]), [1,1,1])\n",
    "kernel2_ = np.tile(np.flipud(np.fliplr(kernel2[0])), [1,1,1])\n",
    "kernel3 = np.random.rand(1,4,3)\n",
    "kernel3_ = np.tile(np.flipud(np.fliplr(kernel3[0])), [1,1,1])\n",
    "kernel = np.array([kernel1, kernel1_])#, kernel2, kernel2_])#, kernel3, kernel3_])\n",
    "#kernel = np.array([kernel3, kernel3_])\n",
    "print \"Kernel: \" + str(kernel)\n",
    "\n",
    "# initialize the data\n",
    "randSeq1 = dataRead.getOneHotSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = dataRead.getOneHotSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1], dtype=np.float32)\n",
    "print \"Data shape: \" + str(data.shape)\n",
    "print data\n",
    "print kernel.shape\n",
    "\n",
    "#initialize the learner and set custom kernels\n",
    "hyper_params = {'number_of_motifs':1,\n",
    "                'motif_length':3,\n",
    "                'learning_rate':0.1,\n",
    "                'pooling_factor':1,\n",
    "                'epochs':100,\n",
    "                'cd_k':1,\n",
    "                'batch_size':1\n",
    "}\n",
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "print gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip sequence containing N\n"
     ]
    }
   ],
   "source": [
    "seqReader = dataRead.SeqReader()\n",
    "allSeqs = seqReader.readSequencesFromFile('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 4, 150)\n"
     ]
    }
   ],
   "source": [
    "realData = np.array([allSeqs[random.randrange(0, len(allSeqs))] for i in range(10)])\n",
    "print realData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform test of both, the GPU and Naive variant of the code\n",
    "Test scenarios are the following:\n",
    "* **Upward pass**\n",
    "* **Downward pass**\n",
    "* **Calculation of Derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiliing theano functions...\n",
      "Starting forward pass test:\n",
      "----------------------------\n",
      "ERROR MADE: 1.22203293133e-08\n",
      "\n",
      "Starting backward pass test:\n",
      "----------------------------\n",
      "ERROR MADE: 2.82151124555e-07\n",
      "\n",
      "Starting Gradient pass test:\n",
      "----------------------------\n",
      "ERROR MADE (Motifs): 2.38418579102e-07\n",
      "ERROR MADE (Bias): 4.43088736812e-08\n",
      "ERROR MADE (c): 0.0\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "\n",
    "# create theano functions\n",
    "# forward\n",
    "print \"Compiliing theano functions...\"\n",
    "D = T.tensor4('data')\n",
    "[P_H, H] = gpuModel.computeHgivenV(D)\n",
    "forward = theano.function([D], [P_H,H], allow_input_downcast=True)\n",
    "\n",
    "# backward\n",
    "H = T.tensor4('Hidden')\n",
    "[P_V, V] = gpuModel.computeVgivenH(H)\n",
    "backward = theano.function([H], [P_V,V], allow_input_downcast=True)\n",
    "\n",
    "# gradient\n",
    "H = T.tensor4('Hidden Probabilities')\n",
    "D = T.tensor4('Data')\n",
    "G_m,G_b,G_c = gpuModel.collectUpdateStatistics(H,D)\n",
    "gradient = theano.function([H,D], [G_m,G_b,G_c], allow_input_downcast=True)\n",
    "\n",
    "# gibbs sampler (up, down, sample)\n",
    "D = T.tensor4('data')\n",
    "[P_H,H] = gpuModel.computeHgivenV(D)\n",
    "[P_V,V] = gpuModel.computeVgivenH(H)\n",
    "gibbs = theano.function([D], V, allow_input_downcast=True)\n",
    "\n",
    "print \"Starting forward pass test:\"\n",
    "print \"----------------------------\"\n",
    "[P_naive,S_n] = naiveModel.computeHgivenV(data)\n",
    "[P_GPU,S_g] = forward(data)\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(P_naive-P_GPU)))\n",
    "print\n",
    "print \"Starting backward pass test:\"\n",
    "print \"----------------------------\"\n",
    "[P_V_naive, V_naive] = naiveModel.computeVgivenH(S_n)\n",
    "[P_V_gpu,V_gpu] = backward(S_n)\n",
    "print \"ERROR MADE: \" + str(np.sum(np.abs(P_V_naive-P_V_gpu)))\n",
    "print\n",
    "print \"Starting Gradient pass test:\"\n",
    "print \"----------------------------\"\n",
    "G_M_naive, G_b_naive, G_c_naive = naiveModel.collectUpdateStatistics(P_naive, data)\n",
    "G_M_gpu,G_b_gpu,G_c_gpu = gradient(P_naive, data)\n",
    "print \"ERROR MADE (Motifs): \" + str(np.sum(np.abs(G_M_naive-G_M_gpu)))\n",
    "print \"ERROR MADE (Bias): \" + str(np.sum(np.abs(G_b_naive-G_b_gpu)))\n",
    "print \"ERROR MADE (c): \" + str(np.sum(np.abs(G_c_naive-G_c_gpu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test the gibbs sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gibbs Sampling test:\n",
      "----------------------------\n",
      "ERROR MADE: 0.04696\n"
     ]
    }
   ],
   "source": [
    "print \"Starting Gibbs Sampling test:\"\n",
    "print \"----------------------------\"\n",
    "#data = np.array([allSeqs[random.randrange(0,len(allSeqs))] for i in range(1)])\n",
    "precision = 100\n",
    "V_naive_acc = np.zeros(realData.shape)\n",
    "V_gpu_acc = np.zeros(realData.shape)\n",
    "for i in range(precision):\n",
    "    V_naive = naiveModel.computeVgivenH(naiveModel.computeHgivenV(realData)[1])[1]\n",
    "    V_gpu = gibbs(realData)\n",
    "    V_naive_acc += V_naive\n",
    "    V_gpu_acc += V_gpu\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        print \"100 iterations done\"\n",
    "\n",
    "V_naive_acc /= precision\n",
    "V_gpu_acc /= precision\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(V_naive_acc-V_gpu_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare training procedures of both methods\n",
    "First, compare the training of the data with multiple epochs. That should converge to the same result after enough epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'batch_size': 1,\n",
      " 'cd_k': 1,\n",
      " 'epochs': 100,\n",
      " 'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 1}\n",
      "DONE WITH NAIVE---------\n",
      "BatchSize: 1\n",
      "Num of iterations per epoch: 1\n",
      "Start compiling Theano training function...\n",
      "Compilation of Theano training function finished in 4.93964099884 seconds\n",
      "Start training the model...\n",
      "[Epoch 0] done!\n",
      "[Epoch 1] done!\n",
      "[Epoch 2] done!\n",
      "[Epoch 3] done!\n",
      "[Epoch 4] done!\n",
      "[Epoch 5] done!\n",
      "[Epoch 6] done!\n",
      "[Epoch 7] done!\n",
      "[Epoch 8] done!\n",
      "[Epoch 9] done!\n",
      "[Epoch 10] done!\n",
      "[Epoch 11] done!\n",
      "[Epoch 12] done!\n",
      "[Epoch 13] done!\n",
      "[Epoch 14] done!\n",
      "[Epoch 15] done!\n",
      "[Epoch 16] done!\n",
      "[Epoch 17] done!\n",
      "[Epoch 18] done!\n",
      "[Epoch 19] done!\n",
      "[Epoch 20] done!\n",
      "[Epoch 21] done!\n",
      "[Epoch 22] done!\n",
      "[Epoch 23] done!\n",
      "[Epoch 24] done!\n",
      "[Epoch 25] done!\n",
      "[Epoch 26] done!\n",
      "[Epoch 27] done!\n",
      "[Epoch 28] done!\n",
      "[Epoch 29] done!\n",
      "[Epoch 30] done!\n",
      "[Epoch 31] done!\n",
      "[Epoch 32] done!\n",
      "[Epoch 33] done!\n",
      "[Epoch 34] done!\n",
      "[Epoch 35] done!\n",
      "[Epoch 36] done!\n",
      "[Epoch 37] done!\n",
      "[Epoch 38] done!\n",
      "[Epoch 39] done!\n",
      "[Epoch 40] done!\n",
      "[Epoch 41] done!\n",
      "[Epoch 42] done!\n",
      "[Epoch 43] done!\n",
      "[Epoch 44] done!\n",
      "[Epoch 45] done!\n",
      "[Epoch 46] done!\n",
      "[Epoch 47] done!\n",
      "[Epoch 48] done!\n",
      "[Epoch 49] done!\n",
      "[Epoch 50] done!\n",
      "[Epoch 51] done!\n",
      "[Epoch 52] done!\n",
      "[Epoch 53] done!\n",
      "[Epoch 54] done!\n",
      "[Epoch 55] done!\n",
      "[Epoch 56] done!\n",
      "[Epoch 57] done!\n",
      "[Epoch 58] done!\n",
      "[Epoch 59] done!\n",
      "[Epoch 60] done!\n",
      "[Epoch 61] done!\n",
      "[Epoch 62] done!\n",
      "[Epoch 63] done!\n",
      "[Epoch 64] done!\n",
      "[Epoch 65] done!\n",
      "[Epoch 66] done!\n",
      "[Epoch 67] done!\n",
      "[Epoch 68] done!\n",
      "[Epoch 69] done!\n",
      "[Epoch 70] done!\n",
      "[Epoch 71] done!\n",
      "[Epoch 72] done!\n",
      "[Epoch 73] done!\n",
      "[Epoch 74] done!\n",
      "[Epoch 75] done!\n",
      "[Epoch 76] done!\n",
      "[Epoch 77] done!\n",
      "[Epoch 78] done!\n",
      "[Epoch 79] done!\n",
      "[Epoch 80] done!\n",
      "[Epoch 81] done!\n",
      "[Epoch 82] done!\n",
      "[Epoch 83] done!\n",
      "[Epoch 84] done!\n",
      "[Epoch 85] done!\n",
      "[Epoch 86] done!\n",
      "[Epoch 87] done!\n",
      "[Epoch 88] done!\n",
      "[Epoch 89] done!\n",
      "[Epoch 90] done!\n",
      "[Epoch 91] done!\n",
      "[Epoch 92] done!\n",
      "[Epoch 93] done!\n",
      "[Epoch 94] done!\n",
      "[Epoch 95] done!\n",
      "[Epoch 96] done!\n",
      "[Epoch 97] done!\n",
      "[Epoch 98] done!\n",
      "[Epoch 99] done!\n",
      "Training finished after: 0.0127279758453 seconds!\n",
      "ERROR MADE (motifs): 0.161822831713\n"
     ]
    }
   ],
   "source": [
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False\n",
    "\n",
    "naiveModel.trainModel(data, hyper_params['epochs'], hyper_params['batch_size'], hyper_params['cd_k'])\n",
    "print \"DONE WITH NAIVE---------\"\n",
    "gpuModel.trainModel(data)\n",
    "\n",
    "new_motifs_gpu = gpuModel.motifs.get_value()\n",
    "new_motifs_naive = naiveModel.kernels\n",
    "\n",
    "print \"ERROR MADE (motifs): \" + str(np.mean(np.abs(new_motifs_gpu-new_motifs_naive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, test both training procedures in another way.\n",
    "We don't apply the gradients to the model anymore and compare only the calculated gradients for the data.\n",
    "These gradients should be more or less the same after a couple of iterations.\n",
    "Due to the sampling, some errors can still exist of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'batch_size': 1,\n",
      " 'cd_k': 1,\n",
      " 'epochs': 100,\n",
      " 'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 1}\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "ERROR MADE (motifs): 0.000505311065425\n",
      "ERROR MADE (bias): 1.96993795696e-05\n",
      "ERROR MADE (c): 0.000112499927874\n"
     ]
    }
   ],
   "source": [
    "#data = np.array([allSeqs[random.randrange(0,len(allSeqs))] for i in range(1)])\n",
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False\n",
    "naiveModel.updateWeights = False\n",
    "\n",
    "# compile theano function\n",
    "D = T.tensor4('data')\n",
    "updates = gpuModel.updateWeightsOnMinibatch(D, 1)\n",
    "der_m = updates[0][1]-updates[0][0]\n",
    "der_bias = updates[1][1]-updates[1][0]\n",
    "der_c = updates[2][1]-updates[2][0]\n",
    "train = theano.function([D], [der_m, der_bias, der_c], allow_input_downcast=True)\n",
    "\n",
    "precision = 10000\n",
    "der_m_naive = np.zeros(kernel.shape)\n",
    "der_m_gpu = np.zeros(kernel.shape)\n",
    "der_bias_naive = np.zeros(naiveModel.bias.shape)\n",
    "der_bias_gpu = np.zeros(gpuModel.bias.get_value().shape)\n",
    "der_c_naive = np.zeros(naiveModel.c.shape)\n",
    "der_c_gpu = np.zeros(gpuModel.c.get_value().shape)\n",
    "\n",
    "for i in range(precision):\n",
    "    # naive\n",
    "    [der_m_naive_l, der_bias_naive_l, der_c_naive_l] = naiveModel.updateWeightsOnMinibatch(data, 1)\n",
    "    der_m_naive += der_m_naive_l\n",
    "    der_bias_naive += der_bias_naive_l\n",
    "    der_c_naive += der_c_naive_l\n",
    "    # gpu\n",
    "    [der_m_gpu_l, der_bias_gpu_l, der_c_gpu_l] = train(data)\n",
    "    der_m_gpu += der_m_gpu_l\n",
    "    der_bias_gpu += der_bias_gpu_l\n",
    "    der_c_gpu += der_c_gpu_l\n",
    "    \n",
    "    if i % 100 == 0 and i > 0:\n",
    "        print \"100 iterations done\"\n",
    "\n",
    "der_m_naive /= precision\n",
    "der_bias_naive /= precision\n",
    "der_c_naive /= precision\n",
    "\n",
    "der_m_gpu /= precision\n",
    "der_bias_gpu /= precision\n",
    "der_c_gpu /= precision\n",
    "\n",
    "print \"ERROR MADE (motifs): \" + str(np.mean(np.abs(der_m_naive - der_m_gpu)))\n",
    "print \"ERROR MADE (bias): \" + str(np.mean(np.abs(der_bias_naive - der_bias_gpu)))\n",
    "print \"ERROR MADE (c): \" + str(np.mean(np.abs(der_c_naive - der_c_gpu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test the sampling methods of both implementations\n",
    "In order to do that, sampling from both implementations has to be done multiple times to ensure the correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test of sampling for prob max pooling\n",
      "----------------------------\n",
      "ERROR MADE: 0.006525\n"
     ]
    }
   ],
   "source": [
    "precision = 10000\n",
    "naiveModel.debug = False\n",
    "\n",
    "print \"Start test of sampling for prob max pooling\"\n",
    "print \"----------------------------\"\n",
    "[P_naive,S_n] = naiveModel.computeHgivenV(data)\n",
    "\n",
    "S_naive = np.zeros((data.shape[0], naiveModel.numberOfKernels, 1, data.shape[3]-naiveModel.kernelLength+1))\n",
    "S_GPU = np.zeros(P_naive.shape)\n",
    "for i in range(precision):\n",
    "    [P_n,S_n] = naiveModel.computeHgivenV(data)\n",
    "    [P_g,S_g] = forward(data)\n",
    "    S_naive += S_n\n",
    "    S_GPU += S_g\n",
    "S_naive /= precision\n",
    "S_GPU /= precision\n",
    "\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(S_naive-S_GPU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array(allSeqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171274, 1, 4, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19  0.24  0.31  0.26]\n"
     ]
    }
   ],
   "source": [
    "print np.mean(np.mean(np.sum(x[:2], axis=1), axis=2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17  0.21  0.21  0.22  0.23  0.19  0.21  0.25  0.21  0.23  0.16  0.21\n",
      "   0.24  0.17  0.16  0.22  0.19  0.22  0.27  0.18  0.22  0.21  0.19  0.16\n",
      "   0.13  0.16  0.26  0.18  0.16  0.2   0.22  0.21  0.17  0.19  0.21  0.22\n",
      "   0.22  0.19  0.16  0.2   0.17  0.21  0.24  0.19  0.19  0.18  0.2   0.29\n",
      "   0.18  0.18  0.25  0.2   0.17  0.15  0.19  0.16  0.23  0.18  0.23  0.14\n",
      "   0.21  0.15  0.19  0.22  0.23  0.2   0.22  0.16  0.23  0.23  0.14  0.23\n",
      "   0.21  0.22  0.19  0.17  0.16  0.17  0.21  0.18  0.13  0.19  0.21  0.18\n",
      "   0.16  0.18  0.21  0.19  0.17  0.13  0.21  0.19  0.19  0.22  0.15  0.25\n",
      "   0.15  0.21  0.17  0.26  0.21  0.19  0.2   0.21  0.21  0.31  0.17  0.2\n",
      "   0.17  0.19  0.21  0.2   0.28  0.2   0.14  0.18  0.2   0.22  0.21  0.26\n",
      "   0.22  0.22  0.19  0.18  0.17  0.19  0.22  0.23  0.13  0.22  0.19  0.25\n",
      "   0.23  0.16  0.25  0.17  0.16  0.12  0.22  0.2   0.17  0.16  0.15  0.16\n",
      "   0.24  0.12  0.22  0.26  0.27  0.2 ]]\n"
     ]
    }
   ],
   "source": [
    "xa = x[:100,:,0,:]\n",
    "print np.mean(xa, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
