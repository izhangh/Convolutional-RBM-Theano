{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Implementation vs GPU code\n",
    "This is for debugging purposes only, to see if the GPU implementation is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '20264' (I am process '24558')\n",
      "WARNING:theano.gof.compilelock:Overriding existing lock by dead process '20264' (I am process '24558')\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "# the underlying convRBM implementation\n",
    "sys.path.append(os.path.abspath('../code'))\n",
    "from convRBM import CRBM\n",
    "import getData as dataRead\n",
    "\n",
    "# biopython stuff\n",
    "#import Bio.SeqIO as sio\n",
    "#import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "#from Bio import motifs\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the naive cRBM\n",
    "All formulas are the same as in the writing section of our git repo.\n",
    "We implement all the basic functions for upward pass, downward pass, prob max pooling, derivative calculation, gibbs sampling and a training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveCRBM:\n",
    "\n",
    "    def __init__ (self, motifLength=1, numMotifs=1, learningRate=0.1, poolingFactor=1):\n",
    "        self.numberOfKernels = numMotifs\n",
    "        self.kernelLength = motifLength\n",
    "        self.poolingFactor = poolingFactor\n",
    "        self.learningRate = learningRate\n",
    "        self.setParamsToZero = True\n",
    "        self.debug = True\n",
    "        if self.setParamsToZero:\n",
    "            self.kernels = np.zeros((self.numberOfKernels, 1, 4, self.kernelLength))\n",
    "            self.bias = np.zeros(self.numberOfKernels)\n",
    "            self.c = np.zeros(4)\n",
    "        else:\n",
    "            self.kernels = np.random.rand(self.numberOfKernels, 1, 4, self.kernelLength)\n",
    "            self.bias = np.random.rand(self.numberOfKernels)\n",
    "            self.c = np.random.rand(4)\n",
    "    \n",
    "    def setCustomKernels (self, kernels):\n",
    "        self.numberOfKernels = kernels.shape[0]\n",
    "        self.kernelLength = kernels.shape[3]\n",
    "        self.kernels = kernels.astype(float)\n",
    "        if self.setParamsToZero:\n",
    "            self.bias = np.zeros(self.numberOfKernels)\n",
    "        else:\n",
    "            self.bias = np.random.rand(self.numberOfKernels)\n",
    "\n",
    "    def initializeMotifs (self):\n",
    "        pass\n",
    "        \n",
    "    def complement (self, kernelSlice):\n",
    "        return kernelSlice[::-1]\n",
    "\n",
    "    def forwardBatch (self, data):\n",
    "        N_h = data.shape[3]-self.kernelLength+1\n",
    "        H = np.zeros((data.shape[0], self.numberOfKernels, 1, N_h))\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n in range(N_h):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        # calculate the x_i, that is the cross-correlation\n",
    "                        x = data[sample,0,:,n+m].T.dot(self.kernels[k,0,:,m]) + self.bias[k]\n",
    "                        #cKernel = self.complement(self.kernels[k,0,:,self.kernelLength-m-1])\n",
    "                        #x_prime = data[sample,0,:,n+m].T.dot(cKernel) + self.bias[k]\n",
    "                        H[sample, k, 0, n] += x # + x_prime\n",
    "        \n",
    "        if self.debug:\n",
    "            print \"Pre Sigmoid Hidden Layer:\"\n",
    "            print H\n",
    "        # perform prob max pooling\n",
    "        P = np.zeros(H.shape)\n",
    "        S = np.zeros(H.shape)\n",
    "        H_exp = np.exp(H)\n",
    "        numBins = N_h / self.poolingFactor\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k_pos in range(0, self.numberOfKernels, 2):\n",
    "                for unit in range(numBins):\n",
    "                    #print \"Doing unit: \" + str(unit)\n",
    "                    # calculate sum within unit\n",
    "                    sumInUnit = 0\n",
    "                    for cell in range(self.poolingFactor):\n",
    "                        curPos = unit*self.poolingFactor+cell\n",
    "                        sumInUnit += H_exp[sample,k_pos,0,curPos] + H_exp[sample,k_pos+1,0,curPos]\n",
    "                        \n",
    "                    # now, calculate the single positions in P\n",
    "                    arr = []\n",
    "                    for cell in range(self.poolingFactor):\n",
    "                        curPos = unit*self.poolingFactor+cell\n",
    "                        P[sample,k_pos,0,curPos] = H_exp[sample,k_pos,0,curPos] / (sumInUnit + 1)\n",
    "                        P[sample,k_pos+1,0,curPos] = H_exp[sample,k_pos+1,0,curPos] / (sumInUnit + 1)\n",
    "                        arr.append(P[sample,k_pos,0,curPos])\n",
    "                        arr.append(P[sample,k_pos+1,0,curPos])\n",
    "                    \n",
    "                    # finally, do the sampling step\n",
    "                    arr.append(1 / (sumInUnit+1))\n",
    "                    s = np.random.multinomial(n=1, pvals=np.array(arr),size=1)\n",
    "                    am = np.argmax(s)\n",
    "                    if am < self.poolingFactor*2:\n",
    "                        strand = am % 2\n",
    "                        pos = unit * self.poolingFactor + (am // 2)\n",
    "                        #print \"Strand: \" + str(strand) + \" Pos: \" + str(pos)\n",
    "                        S[sample,k_pos+strand,0,pos] = 1\n",
    "        return [P,S]\n",
    "\n",
    "\n",
    "    def backwardBatch (self, H):\n",
    "        \n",
    "        # calculate full convolution (not valid, therefore padding is applied with zeros)\n",
    "        N_v = H.shape[3] + self.kernelLength - 1\n",
    "        pad = self.kernelLength-1\n",
    "        V = np.zeros((H.shape[0],1,4,N_v))\n",
    "        Y = np.zeros(V.shape)\n",
    "        H_pad = np.pad(H,[(0,0),(0,0),(0,0),(pad, pad)], 'constant',constant_values=(0,0))\n",
    "        for sample in range(H.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n in range(N_v):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        Y[sample,0,:,n] += self.kernels[k,0,:,m] * H_pad[sample,k,0,pad+n-m]\n",
    "                        \n",
    "        # calculate softmax on convolved data\n",
    "        P_V = self.softmax(Y)\n",
    "        return P_V\n",
    "        \n",
    "    def sampleVisibleLayer(self, P_V):\n",
    "        # sample the visible layer from probabilities\n",
    "        V = np.zeros(P_V.shape)\n",
    "        for sample in range(P_V.shape[0]):\n",
    "            for col in range(P_V.shape[3]):\n",
    "                V[sample,0,:,col] = np.random.multinomial(n=1,pvals=P_V[sample,0,:,col],size=1)\n",
    "        return V\n",
    "\n",
    "    def expectedDerivative (self, H, data):\n",
    "        G = np.zeros(self.kernels.shape)\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n_h in range(H.shape[3]):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        G[k,0,:,m] += data[sample,0,:,n_h+m] * H[sample,k,0,n_h]\n",
    "\n",
    "        der_bias = np.mean(np.sum(H, axis=3), axis=0).reshape(-1)\n",
    "        der_c = np.mean(np.sum(data, axis=3), axis=0).reshape(-1)\n",
    "        return [G, der_bias, der_c]\n",
    "    \n",
    "    def train_model (self, D, numOfCDs):\n",
    "        # calculate the data gradient for weights (motifs) and bias\n",
    "        [H_data, S_data] = self.forwardBatch(D)\n",
    "        if self.debug:\n",
    "            print \"Hidden Layer Probabilities:\"\n",
    "            print H_data\n",
    "            print \"Hidden Layer Sample:\"\n",
    "            print S_data\n",
    "\n",
    "        # calculate data gradients\n",
    "        [G_motif_data, G_bias_data, G_c_data] = self.expectedDerivative(H_data, D)\n",
    "\n",
    "        if self.debug:\n",
    "            print \"Data gradient for motifs\"\n",
    "            print G_motif_data\n",
    "\n",
    "        # calculate model probs\n",
    "        S_H = S_data\n",
    "        for i in range(numOfCDs):\n",
    "            V_model = self.backwardBatch(S_H)\n",
    "            S_V = self.sampleVisibleLayer(V_model)\n",
    "            if self.debug:\n",
    "                print \"Visible Sample for CD \" + str(i)\n",
    "                print S_V\n",
    "            [H_model, S_H] = self.forwardBatch(S_V)\n",
    "        \n",
    "        # compute the model gradients\n",
    "        [G_motif_model, G_bias_model, G_c_model] = self.expectedDerivative(H_model, S_V)\n",
    "        \n",
    "        if self.debug:\n",
    "            print \"Model gradient for motifs:\"\n",
    "            print G_motif_model\n",
    "        \n",
    "        # update the parameters\n",
    "        self.kernels += self.learningRate * (G_motif_data - G_motif_model)\n",
    "        self.bias += self.learningRate * (G_bias_data - G_bias_model)\n",
    "        self.c += self.learningRate * (G_c_data - G_c_model)\n",
    "\n",
    "        \n",
    "    def trainMinibatch (self, trainData, epochs, batchSize, numOfCDs):\n",
    "        iterations = trainData.shape[0] / batchSize\n",
    "        for epoch in range(epochs):\n",
    "            for batchIdx in range(iterations):\n",
    "                self.train_model(trainData[batchIdx*batchSize:(batchIdx+1)*batchSize], numOfCDs)\n",
    "        \n",
    "    def softmax (self, x):\n",
    "        return np.exp(x) / np.exp(x).sum(axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct toy data to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: [[[[1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]]]]\n",
      "Data shape: (1, 1, 4, 8)\n",
      "[[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  1.  1.  1.  1.]\n",
      "   [ 0.  0.  0.  1.  0.  0.  0.  0.]]]]\n",
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 2}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "kernel1 = np.tile(np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]]), [1,1,1])\n",
    "kernel1_ = np.tile(np.flipud(np.fliplr(kernel1[0])),[1,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0],[0,0,0],[1,1,1],[0,0,0]]), [1,1,1])\n",
    "kernel2_ = np.tile(np.flipud(np.fliplr(kernel2[0])), [1,1,1])\n",
    "kernel3 = np.random.rand(1,4,3)\n",
    "kernel3_ = np.tile(np.flipud(np.fliplr(kernel3[0])), [1,1,1])\n",
    "kernel = np.array([kernel1, kernel1_])#, kernel2, kernel2_])#, kernel3, kernel3_])\n",
    "#kernel = np.array([kernel3, kernel3_])\n",
    "print \"Kernel: \" + str(kernel)\n",
    "\n",
    "# initialize the data\n",
    "randSeq1 = dataRead.getOneHotSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = dataRead.getOneHotSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1], dtype=np.float32)\n",
    "print \"Data shape: \" + str(data.shape)\n",
    "print data\n",
    "\n",
    "#initialize the learner and set custom kernels\n",
    "hyper_params = {'number_of_motifs':1,\n",
    "                'motif_length':3,\n",
    "                'learning_rate':0.1,\n",
    "                'pooling_factor':2\n",
    "}\n",
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "print gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip sequence containing N\n",
      "(1, 1, 4, 150)\n"
     ]
    }
   ],
   "source": [
    "seqReader = dataRead.SeqReader()\n",
    "allSeqs = seqReader.readSequencesFromFile('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa')\n",
    "realData = np.array([allSeqs[random.randrange(0, len(allSeqs))]])\n",
    "print realData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform test of both, the GPU and Naive variant of the code\n",
    "Test scenarios are the following:\n",
    "* **Upward pass**\n",
    "* **Downward pass**\n",
    "* **Calculation of Derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiliing theano functions...\n",
      "Starting forward pass test:\n",
      "----------------------------\n",
      "Done with naive Model\n",
      "ERROR MADE: 3.64856668142e-08\n",
      "\n",
      "Starting backward pass test:\n",
      "----------------------------\n",
      "Hidden Sample: \n",
      "ERROR MADE: 6.18965526611e-08\n",
      "\n",
      "Starting Gradient pass test:\n",
      "----------------------------\n",
      "ERROR MADE (Motifs): 9.685754776e-08\n",
      "ERROR MADE (Bias): 1.01162649813e-07\n",
      "ERROR MADE (c): 0.0\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "\n",
    "# create theano functions\n",
    "# forward\n",
    "print \"Compiliing theano functions...\"\n",
    "D = T.tensor4('data')\n",
    "[P,S] = gpuModel.forwardBatch(D)\n",
    "forward = theano.function([D], [P,S], allow_input_downcast=True)\n",
    "\n",
    "# backward\n",
    "H = T.tensor4('Hidden')\n",
    "V = gpuModel.backwardBatch(H)\n",
    "backward = theano.function([H], V, allow_input_downcast=True)\n",
    "\n",
    "# gradient\n",
    "H = T.tensor4('Hidden Probabilities')\n",
    "D = T.tensor4('Data')\n",
    "G_m,G_b,G_c = gpuModel.expectedDerivative(H,D)\n",
    "gradient = theano.function([H,D], [G_m,G_b,G_c], allow_input_downcast=True)\n",
    "\n",
    "# gibbs sampler (up, down, sample)\n",
    "D = T.tensor4('data')\n",
    "[P,S] = gpuModel.forwardBatch(D)\n",
    "V_P = gpuModel.backwardBatch(S)\n",
    "V = gpuModel.sampleVisibleLayer(V_P)\n",
    "gibbs = theano.function([D], V, allow_input_downcast=True)\n",
    "\n",
    "print \"Starting forward pass test:\"\n",
    "print \"----------------------------\"\n",
    "[P_naive,S_n] = naiveModel.forwardBatch(data)\n",
    "print \"Done with naive Model\"\n",
    "[P_GPU,S_g] = forward(data)\n",
    "#print \"Naive Result:\"\n",
    "#print P_naive\n",
    "#print \"GPU result:\"\n",
    "#print P_GPU\n",
    "print \"ERROR MADE: \" + str(np.sum(np.abs(P_naive-P_GPU)))\n",
    "print\n",
    "print \"Starting backward pass test:\"\n",
    "print \"----------------------------\"\n",
    "print \"Hidden Sample: \"\n",
    "V_naive = naiveModel.backwardBatch(S_n)\n",
    "V_gpu = backward(S_n)\n",
    "#print \"Naive Result (Backward)\"\n",
    "#print V_naive\n",
    "#print \"GPU Result (Backward)\"\n",
    "#print V_gpu\n",
    "print \"ERROR MADE: \" + str(np.sum(np.abs(V_naive-V_gpu)))\n",
    "print\n",
    "print \"Starting Gradient pass test:\"\n",
    "print \"----------------------------\"\n",
    "G_M_naive, G_b_naive, G_c_naive = naiveModel.expectedDerivative(P_naive, data)\n",
    "G_M_gpu,G_b_gpu,G_c_gpu = gradient(P_naive, data)\n",
    "print \"ERROR MADE (Motifs): \" + str(np.sum(np.abs(G_M_naive-G_M_gpu)))\n",
    "print \"ERROR MADE (Bias): \" + str(np.sum(np.abs(G_b_naive-G_b_gpu)))\n",
    "print \"ERROR MADE (c): \" + str(np.sum(np.abs(G_c_naive-G_c_gpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gibbs Sampling test:\n",
      "----------------------------\n",
      "ERROR MADE: 0.015492\n"
     ]
    }
   ],
   "source": [
    "print \"Starting Gibbs Sampling test:\"\n",
    "print \"----------------------------\"\n",
    "data = np.array([allSeqs[random.randrange(0,len(allSeqs))] for i in range(10)])\n",
    "precision = 1000\n",
    "V_naive_acc = np.zeros(data.shape)\n",
    "V_gpu_acc = np.zeros(data.shape)\n",
    "for i in range(precision):\n",
    "    V_naive = naiveModel.sampleVisibleLayer(naiveModel.backwardBatch(naiveModel.forwardBatch(data)[1]))\n",
    "    V_gpu = gibbs(data)\n",
    "    V_naive_acc += V_naive\n",
    "    V_gpu_acc += V_gpu\n",
    "\n",
    "V_naive_acc /= precision\n",
    "V_gpu_acc /= precision\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(V_naive_acc-V_gpu_acc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare training procedures of both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 2}\n",
      "None\n",
      "BatchSize: 1\n",
      "Num of iterations per epoch: 1\n",
      "Start compiling Theano training function...\n",
      "Compilation of Theano training function finished in 1.73778390884 seconds\n",
      "Start training the model...\n",
      "Convolution result forward:  __str__ = [[[[ 3.  0.  1.  1.  1.  1.]]\n",
      "\n",
      "  [[ 0.  3.  0.  1.  1.  1.]]]]\n",
      "prob max pooled layer:  __str__ = [[[[ 1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.  1.  1.  0.]]]]\n",
      "Pre sigmoid visible layer:  __str__ = [[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  1.  0.  1.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  1.  1.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  1.  1.  0.]]]]\n",
      "Visible Sample:  __str__ = [[[[ 1.  1.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  1.  0.  1.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  1.  1.]]]]\n",
      "Sample of Visible layer at CD0 __str__ = [[[[ 1.  1.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  1.  0.  1.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  1.  1.]]]]\n",
      "Hidden Probabilites:  __str__ = [[[[ 0.47  0.02  0.27  0.27  0.23  0.23]]\n",
      "\n",
      "  [[ 0.02  0.47  0.1   0.27  0.23  0.23]]]]\n",
      "Hidden Layer Probabilities:  __str__ = [[[[ 0.47  0.02  0.27  0.27  0.23  0.23]]\n",
      "\n",
      "  [[ 0.02  0.47  0.1   0.27  0.23  0.23]]]]\n",
      "Gradient for motifs (data):  __str__ = [[[[ 0.47  0.    0.  ]\n",
      "   [ 0.02  0.47  0.  ]\n",
      "   [ 0.73  0.75  1.46]\n",
      "   [ 0.27  0.27  0.02]]]\n",
      "\n",
      "\n",
      " [[[ 0.02  0.    0.  ]\n",
      "   [ 0.47  0.02  0.  ]\n",
      "   [ 0.56  1.19  0.85]\n",
      "   [ 0.27  0.1   0.47]]]]\n",
      "Convolution result forward:  __str__ = [[[[ 1.  3.  0.  2.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  2.  0.  3.  1.]]]]\n",
      "Hidden Probabilites:  __str__ = [[[[ 0.11  0.78  0.06  0.42  0.04  0.04]]\n",
      "\n",
      "  [[ 0.04  0.04  0.42  0.06  0.78  0.11]]]]\n",
      "Gradient for motifs (model):  __str__ = [[[[ 0.88  0.11  0.  ]\n",
      "   [ 0.1   1.19  0.16]\n",
      "   [ 0.45  0.1   1.19]\n",
      "   [ 0.    0.04  0.08]]]\n",
      "\n",
      "\n",
      " [[[ 0.08  0.04  0.  ]\n",
      "   [ 1.19  0.1   0.45]\n",
      "   [ 0.16  1.19  0.1 ]\n",
      "   [ 0.    0.11  0.88]]]]\n",
      "[Epoch 0] done!\n",
      "Training finished after: 0.00745391845703 seconds!\n",
      "Pre Sigmoid Hidden Layer:\n",
      "[[[[ 3.  0.  1.  1.  1.  1.]]\n",
      "\n",
      "  [[ 0.  3.  0.  1.  1.  1.]]]]\n",
      "Hidden Layer Probabilities:\n",
      "[[[[ 0.47  0.02  0.27  0.27  0.23  0.23]]\n",
      "\n",
      "  [[ 0.02  0.47  0.1   0.27  0.23  0.23]]]]\n",
      "Hidden Layer Sample:\n",
      "[[[[ 0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  1.  0.  0.  0.  0.]]]]\n",
      "Data gradient for motifs\n",
      "[[[[ 0.47  0.    0.  ]\n",
      "   [ 0.02  0.47  0.  ]\n",
      "   [ 0.73  0.75  1.46]\n",
      "   [ 0.27  0.27  0.02]]]\n",
      "\n",
      "\n",
      " [[[ 0.02  0.    0.  ]\n",
      "   [ 0.47  0.02  0.  ]\n",
      "   [ 0.56  1.19  0.85]\n",
      "   [ 0.27  0.1   0.47]]]]\n",
      "Visible Sample for CD 0\n",
      "[[[[ 0.  0.  0.  0.  0.  1.  1.  0.]\n",
      "   [ 1.  1.  0.  1.  0.  0.  0.  1.]\n",
      "   [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  0.  0.  0.  0.]]]]\n",
      "Pre Sigmoid Hidden Layer:\n",
      "[[[[ 1.  0.  2.  0.  0.  1.]]\n",
      "\n",
      "  [[ 2.  1.  0.  2.  0.  0.]]]]\n",
      "Model gradient for motifs:\n",
      "[[[[ 0.4   0.55  0.21]\n",
      "   [ 0.31  0.6   0.47]\n",
      "   [ 0.15  0.06  0.42]\n",
      "   [ 0.42  0.07  0.18]]]\n",
      "\n",
      "\n",
      " [[[ 0.15  0.3   0.56]\n",
      "   [ 1.1   0.55  0.33]\n",
      "   [ 0.15  0.42  0.06]\n",
      "   [ 0.06  0.18  0.5 ]]]]\n"
     ]
    }
   ],
   "source": [
    "#data = np.array([allSeqs[random.randrange(0,len(allSeqs))] for i in range(1)])\n",
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "print gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False\n",
    "gpuModel.trainMinibatch(data, 1, 1, 1)\n",
    "naiveModel.trainMinibatch(data, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU derivatives\n",
      "[[[[ 2.19 -1.42 -0.68]\n",
      "   [-1.24  2.53 -0.36]\n",
      "   [-0.78  1.4   3.47]\n",
      "   [ 1.71 -0.64 -0.56]]]\n",
      "\n",
      "\n",
      " [[[-0.27 -0.33 -0.46]\n",
      "   [ 0.75 -0.12 -0.31]\n",
      "   [-0.39  0.94  0.31]\n",
      "   [ 0.29 -0.11  0.85]]]]\n",
      "Naive Derivatives\n",
      "[[[[ 1.26 -1.48 -0.88]\n",
      "   [-1.63  1.79 -0.67]\n",
      "   [ 0.27  2.15  3.87]\n",
      "   [ 1.68 -0.9  -0.75]]]\n",
      "\n",
      "\n",
      " [[[-0.24 -0.24 -0.3 ]\n",
      "   [ 0.93 -0.2  -0.3 ]\n",
      "   [ 0.08  1.31  0.52]\n",
      "   [-0.13 -0.22  0.73]]]]\n",
      "7.62626819384\n"
     ]
    }
   ],
   "source": [
    "new_motifs_gpu = gpuModel.motifs.get_value()\n",
    "new_motifs_naive = naiveModel.kernels\n",
    "print \"GPU derivatives\"\n",
    "print new_motifs_gpu\n",
    "print \"Naive Derivatives\"\n",
    "print new_motifs_naive\n",
    "print np.sum(np.abs(new_motifs_gpu-new_motifs_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test the sampling methods of both implementations\n",
    "In order to do that, sampling from both implementations has to be done multiple times to ensure the correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test of sampling for prob max pooling\n",
      "----------------------------\n",
      "ERROR MADE: 0.00442577702703\n"
     ]
    }
   ],
   "source": [
    "precision = 10000\n",
    "naiveModel.debug = False\n",
    "\n",
    "print \"Start test of sampling for prob max pooling\"\n",
    "print \"----------------------------\"\n",
    "[P_naive,S_n] = naiveModel.forwardBatch(data)\n",
    "\n",
    "S_naive = np.zeros((data.shape[0], naiveModel.numberOfKernels, 1, data.shape[3]-naiveModel.kernelLength+1))\n",
    "S_GPU = np.zeros(P_naive.shape)\n",
    "for i in range(precision):\n",
    "    [P_n,S_n] = naiveModel.forwardBatch(data)\n",
    "    [P_g,S_g] = forward(data)\n",
    "    S_naive += S_n\n",
    "    S_GPU += S_g\n",
    "S_naive /= precision\n",
    "S_GPU /= precision\n",
    "\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(S_naive-S_GPU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test of sampling for Visible Layer\n",
      "----------------------------\n",
      "V Probs:\n",
      "ERROR MADE: 0.0\n"
     ]
    }
   ],
   "source": [
    "V_P = T.tensor4('Reconstruction Probs')\n",
    "V = gpuModel.sampleVisibleLayer(V_P)\n",
    "sampleV = theano.function([V_P], V, allow_input_downcast=True)\n",
    "\n",
    "precision = 1000\n",
    "print \"Start test of sampling for Visible Layer\"\n",
    "print \"----------------------------\"\n",
    "print \"V Probs:\"\n",
    "#print V_naive\n",
    "\n",
    "Sample_naive = np.zeros((data.shape[0], naiveModel.numberOfKernels//2, 4, data.shape[3]))\n",
    "Sample_gpu = np.zeros(Sample_naive.shape)\n",
    "for i in range(precision):\n",
    "    V_n = naiveModel.sampleVisibleLayer(V_naive)\n",
    "    V_g = sampleV(V_naive)\n",
    "    Sample_naive += V_n\n",
    "    Sample_gpu += V_g\n",
    "Sample_naive /= precision\n",
    "Sample_gpu /= precision\n",
    "\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(Sample_naive-Sample_gpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
