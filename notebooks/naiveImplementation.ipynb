{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Implementation vs GPU code\n",
    "This is for debugging purposes only, to see if the GPU implementation is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '22906' (I am process '28315')\n",
      "WARNING:theano.gof.compilelock:Overriding existing lock by dead process '22906' (I am process '28315')\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "# the underlying convRBM implementation\n",
    "sys.path.append(os.path.abspath('../code'))\n",
    "from convRBM import CRBM\n",
    "import getData as dataRead\n",
    "\n",
    "# biopython stuff\n",
    "#import Bio.SeqIO as sio\n",
    "#import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "#from Bio import motifs\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the naive cRBM\n",
    "All formulas are the same as in the writing section of our git repo.\n",
    "We implement all the basic functions for upward pass, downward pass, prob max pooling, derivative calculation, gibbs sampling and a training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveCRBM:\n",
    "\n",
    "    def __init__ (self, motifLength=1, numMotifs=1, learningRate=0.1, poolingFactor=1):\n",
    "        self.numberOfKernels = numMotifs\n",
    "        self.kernelLength = motifLength\n",
    "        self.poolingFactor = poolingFactor\n",
    "        self.learningRate = learningRate\n",
    "        self.setParamsToZero = True\n",
    "        self.debug = True\n",
    "        self.updateWeights = True\n",
    "        if self.setParamsToZero:\n",
    "            self.kernels = np.zeros((self.numberOfKernels, 1, 4, self.kernelLength))\n",
    "            self.bias = np.zeros(self.numberOfKernels)\n",
    "            self.c = np.zeros(4)\n",
    "        else:\n",
    "            self.kernels = np.random.rand(self.numberOfKernels, 1, 4, self.kernelLength)\n",
    "            self.bias = np.random.rand(self.numberOfKernels)\n",
    "            self.c = np.random.rand(4)\n",
    "    \n",
    "    def setCustomKernels (self, kernels):\n",
    "        self.numberOfKernels = kernels.shape[0]\n",
    "        self.kernelLength = kernels.shape[3]\n",
    "        self.kernels = kernels.astype(float)\n",
    "        if self.setParamsToZero:\n",
    "            self.bias = np.zeros(self.numberOfKernels)\n",
    "        else:\n",
    "            self.bias = np.random.rand(self.numberOfKernels)\n",
    "\n",
    "    def initializeMotifs (self):\n",
    "        pass\n",
    "        \n",
    "    def complement (self, kernelSlice):\n",
    "        return kernelSlice[::-1]\n",
    "\n",
    "    def forwardBatch (self, data):\n",
    "        N_h = data.shape[3]-self.kernelLength+1\n",
    "        H = np.zeros((data.shape[0], self.numberOfKernels, 1, N_h))\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n in range(N_h):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        # calculate the x_i, that is the cross-correlation\n",
    "                        x = data[sample,0,:,n+m].T.dot(self.kernels[k,0,:,m]) + self.bias[k]\n",
    "                        #cKernel = self.complement(self.kernels[k,0,:,self.kernelLength-m-1])\n",
    "                        #x_prime = data[sample,0,:,n+m].T.dot(cKernel) + self.bias[k]\n",
    "                        H[sample, k, 0, n] += x # + x_prime\n",
    "        \n",
    "        if self.debug:\n",
    "            print \"Pre Sigmoid Hidden Layer:\"\n",
    "            print H\n",
    "        # perform prob max pooling\n",
    "        P = np.zeros(H.shape)\n",
    "        S = np.zeros(H.shape)\n",
    "        H_exp = np.exp(H)\n",
    "        numBins = N_h / self.poolingFactor\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k_pos in range(0, self.numberOfKernels, 1):\n",
    "                for unit in range(numBins):\n",
    "                    #print \"Doing unit: \" + str(unit)\n",
    "                    # calculate sum within unit\n",
    "                    sumInUnit = 0\n",
    "                    for cell in range(self.poolingFactor):\n",
    "                        curPos = unit*self.poolingFactor+cell\n",
    "                        sumInUnit += H_exp[sample,k_pos,0,curPos]# + H_exp[sample,k_pos+1,0,curPos]\n",
    "                        \n",
    "                    # now, calculate the single positions in P\n",
    "                    arr = []\n",
    "                    for cell in range(self.poolingFactor):\n",
    "                        curPos = unit*self.poolingFactor+cell\n",
    "                        P[sample,k_pos,0,curPos] = H_exp[sample,k_pos,0,curPos] / (sumInUnit + 1)\n",
    "                        #P[sample,k_pos+1,0,curPos] = H_exp[sample,k_pos+1,0,curPos] / (sumInUnit + 1)\n",
    "                        arr.append(P[sample,k_pos,0,curPos])\n",
    "                        #arr.append(P[sample,k_pos+1,0,curPos])\n",
    "                    \n",
    "                    # finally, do the sampling step\n",
    "                    arr.append(1 / (sumInUnit+1))\n",
    "                    s = np.random.multinomial(n=1, pvals=np.array(arr),size=1)\n",
    "                    am = np.argmax(s)\n",
    "                    if am < self.poolingFactor:#*2:\n",
    "                        strand = am % 2\n",
    "                        pos = unit * self.poolingFactor + am #(am // 2)\n",
    "                        #print \"Strand: \" + str(strand) + \" Pos: \" + str(pos)\n",
    "                        S[sample,k_pos,0,pos] = 1\n",
    "        return [P,S]\n",
    "\n",
    "\n",
    "    def backwardBatch (self, H):\n",
    "        \n",
    "        # calculate full convolution (not valid, therefore padding is applied with zeros)\n",
    "        N_v = H.shape[3] + self.kernelLength - 1\n",
    "        pad = self.kernelLength-1\n",
    "        V = np.zeros((H.shape[0],1,4,N_v))\n",
    "        Y = np.zeros(V.shape)\n",
    "        H_pad = np.pad(H,[(0,0),(0,0),(0,0),(pad, pad)], 'constant',constant_values=(0,0))\n",
    "        for sample in range(H.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n in range(N_v):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        Y[sample,0,:,n] += self.kernels[k,0,:,m] * H_pad[sample,k,0,pad+n-m]\n",
    "                        \n",
    "        # calculate softmax on convolved data\n",
    "        P_V = self.softmax(Y)\n",
    "        return P_V\n",
    "        \n",
    "    def sampleVisibleLayer(self, P_V):\n",
    "        # sample the visible layer from probabilities\n",
    "        V = np.zeros(P_V.shape)\n",
    "        for sample in range(P_V.shape[0]):\n",
    "            for col in range(P_V.shape[3]):\n",
    "                V[sample,0,:,col] = np.random.multinomial(n=1,pvals=P_V[sample,0,:,col],size=1)\n",
    "        return V\n",
    "\n",
    "    def expectedDerivative (self, H, data):\n",
    "        G = np.zeros(self.kernels.shape)\n",
    "        for sample in range(data.shape[0]):\n",
    "            for k in range(self.numberOfKernels):\n",
    "                for n_h in range(H.shape[3]):\n",
    "                    for m in range(self.kernelLength):\n",
    "                        G[k,0,:,m] += data[sample,0,:,n_h+m] * H[sample,k,0,n_h]\n",
    "\n",
    "        der_bias = np.mean(np.sum(H, axis=3), axis=0).reshape(-1)\n",
    "        der_c = np.mean(np.sum(data, axis=3), axis=0).reshape(-1)\n",
    "        return [G, der_bias, der_c]\n",
    "    \n",
    "    def train_model (self, D, numOfCDs):\n",
    "        # calculate the data gradient for weights (motifs) and bias\n",
    "        [H_data, S_data] = self.forwardBatch(D)\n",
    "        if self.debug:\n",
    "            print \"Hidden Layer Probabilities:\"\n",
    "            print H_data\n",
    "            print \"Hidden Layer Sample:\"\n",
    "            print S_data\n",
    "\n",
    "        # calculate data gradients\n",
    "        [G_motif_data, G_bias_data, G_c_data] = self.expectedDerivative(H_data, D)\n",
    "\n",
    "        if self.debug:\n",
    "            print \"Data gradient for motifs\"\n",
    "            print G_motif_data\n",
    "\n",
    "        # calculate model probs\n",
    "        S_H = S_data\n",
    "        for i in range(numOfCDs):\n",
    "            V_model = self.backwardBatch(S_H)\n",
    "            S_V = self.sampleVisibleLayer(V_model)\n",
    "            if self.debug:\n",
    "                print \"Visible Sample for CD \" + str(i)\n",
    "                print S_V\n",
    "            [H_model, S_H] = self.forwardBatch(S_V)\n",
    "        \n",
    "        # compute the model gradients\n",
    "        [G_motif_model, G_bias_model, G_c_model] = self.expectedDerivative(H_model, S_V)\n",
    "        \n",
    "        if self.debug:\n",
    "            print \"Model gradient for motifs:\"\n",
    "            print G_motif_model\n",
    "        \n",
    "        # update the parameters\n",
    "        new_kernels = self.learningRate * (G_motif_data - G_motif_model)\n",
    "        new_bias = self.learningRate * (G_bias_data - G_bias_model)\n",
    "        new_c = self.learningRate * (G_c_data - G_c_model)\n",
    "\n",
    "        if self.updateWeights:\n",
    "            self.kernels += new_kernels\n",
    "            self.bias += new_bias\n",
    "            self.c += new_c\n",
    "\n",
    "        return (new_kernels, new_bias, new_c)\n",
    "\n",
    "        \n",
    "    def trainMinibatch (self, trainData, epochs, batchSize, numOfCDs):\n",
    "        iterations = trainData.shape[0] / batchSize\n",
    "        for epoch in range(epochs):\n",
    "            for batchIdx in range(iterations):\n",
    "                self.train_model(trainData[batchIdx*batchSize:(batchIdx+1)*batchSize], numOfCDs)\n",
    "        \n",
    "    def softmax (self, x):\n",
    "        return np.exp(x) / np.exp(x).sum(axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct toy data to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: [[[[1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]]]]\n",
      "Data shape: (1, 1, 4, 8)\n",
      "[[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  1.  1.  1.  1.]\n",
      "   [ 0.  0.  0.  1.  0.  0.  0.  0.]]]]\n",
      "(2, 1, 4, 3)\n",
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'batch_size': 1,\n",
      " 'cd_k': 1,\n",
      " 'epochs': 1,\n",
      " 'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 1}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "kernel1 = np.tile(np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]]), [1,1,1])\n",
    "kernel1_ = np.tile(np.flipud(np.fliplr(kernel1[0])),[1,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0],[0,0,0],[1,1,1],[0,0,0]]), [1,1,1])\n",
    "kernel2_ = np.tile(np.flipud(np.fliplr(kernel2[0])), [1,1,1])\n",
    "kernel3 = np.random.rand(1,4,3)\n",
    "kernel3_ = np.tile(np.flipud(np.fliplr(kernel3[0])), [1,1,1])\n",
    "kernel = np.array([kernel1, kernel1_])#, kernel2, kernel2_])#, kernel3, kernel3_])\n",
    "#kernel = np.array([kernel3, kernel3_])\n",
    "print \"Kernel: \" + str(kernel)\n",
    "\n",
    "# initialize the data\n",
    "randSeq1 = dataRead.getOneHotSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = dataRead.getOneHotSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1], dtype=np.float32)\n",
    "print \"Data shape: \" + str(data.shape)\n",
    "print data\n",
    "print kernel.shape\n",
    "\n",
    "#initialize the learner and set custom kernels\n",
    "hyper_params = {'number_of_motifs':1,\n",
    "                'motif_length':3,\n",
    "                'learning_rate':0.1,\n",
    "                'pooling_factor':1,\n",
    "                'epochs':1,\n",
    "                'cd_k':1,\n",
    "                'batch_size':1\n",
    "}\n",
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "print gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip sequence containing N\n",
      "(1, 1, 4, 150)\n"
     ]
    }
   ],
   "source": [
    "seqReader = dataRead.SeqReader()\n",
    "allSeqs = seqReader.readSequencesFromFile('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa')\n",
    "realData = np.array([allSeqs[random.randrange(0, len(allSeqs))]])\n",
    "print realData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform test of both, the GPU and Naive variant of the code\n",
    "Test scenarios are the following:\n",
    "* **Upward pass**\n",
    "* **Downward pass**\n",
    "* **Calculation of Derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.cmodule): ModuleCache.refresh() Found key without dll in cache, deleting it. /home/sasse/.theano/compiledir_Linux-3.10.29.mx64.54-x86_64-with-glibc2.2.5--2.7.10-64/tmpfHpLbX/key.pkl\n",
      "WARNING:theano.gof.cmodule:ModuleCache.refresh() Found key without dll in cache, deleting it. /home/sasse/.theano/compiledir_Linux-3.10.29.mx64.54-x86_64-with-glibc2.2.5--2.7.10-64/tmpfHpLbX/key.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiliing theano functions...\n",
      "Starting forward pass test:\n",
      "----------------------------\n",
      "Done with naive Model\n",
      "Naive Result:\n",
      "[[[[ 0.91  0.05  0.42  0.42  0.42  0.42]]\n",
      "\n",
      "  [[ 0.05  0.91  0.21  0.58  0.42  0.42]]]]\n",
      "GPU result:\n",
      "[[[[ 0.91  0.05  0.42  0.42  0.42  0.42]]\n",
      "\n",
      "  [[ 0.05  0.91  0.21  0.58  0.42  0.42]]]]\n",
      "ERROR MADE: 1.99438660073e-07\n",
      "H shape (sample): (1, 2, 1, 6)\n",
      "Starting backward pass test:\n",
      "----------------------------\n",
      "Hidden Sample: \n",
      "ERROR MADE: 2.29919155464e-07\n",
      "\n",
      "Starting Gradient pass test:\n",
      "----------------------------\n",
      "ERROR MADE (Motifs): 2.75671482086e-07\n",
      "ERROR MADE (Bias): 8.76799512994e-08\n",
      "ERROR MADE (c): 0.0\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "\n",
    "# create theano functions\n",
    "# forward\n",
    "print \"Compiliing theano functions...\"\n",
    "D = T.tensor4('data')\n",
    "[P_H, H] = gpuModel.computeHgivenV(D)\n",
    "forward = theano.function([D], [P_H,H], allow_input_downcast=True)\n",
    "\n",
    "# backward\n",
    "H = T.tensor4('Hidden')\n",
    "[P_V, V] = gpuModel.computeVgivenH(H)\n",
    "backward = theano.function([H], [P_V,V], allow_input_downcast=True)\n",
    "\n",
    "# gradient\n",
    "H = T.tensor4('Hidden Probabilities')\n",
    "D = T.tensor4('Data')\n",
    "G_m,G_b,G_c = gpuModel.collectUpdateStatistics(H,D)\n",
    "gradient = theano.function([H,D], [G_m,G_b,G_c], allow_input_downcast=True)\n",
    "\n",
    "# gibbs sampler (up, down, sample)\n",
    "D = T.tensor4('data')\n",
    "[P_H,H] = gpuModel.computeHgivenV(D)\n",
    "[P_V,V] = gpuModel.computeVgivenH(H)\n",
    "gibbs = theano.function([D], V, allow_input_downcast=True)\n",
    "\n",
    "print \"Starting forward pass test:\"\n",
    "print \"----------------------------\"\n",
    "[P_naive,S_n] = naiveModel.forwardBatch(data)\n",
    "print \"Done with naive Model\"\n",
    "[P_GPU,S_g] = forward(data)\n",
    "print \"Naive Result:\"\n",
    "print P_naive\n",
    "print \"GPU result:\"\n",
    "print P_GPU\n",
    "print \"ERROR MADE: \" + str(np.sum(np.abs(P_naive-P_GPU)))\n",
    "print \"H shape (sample): \" + str(S_n.shape)\n",
    "print \"Starting backward pass test:\"\n",
    "print \"----------------------------\"\n",
    "print \"Hidden Sample: \"\n",
    "V_naive = naiveModel.backwardBatch(S_n)\n",
    "[V_gpu,S_V_gpu] = backward(S_n)\n",
    "#print \"Naive Result (Backward)\"\n",
    "#print V_naive\n",
    "#print \"GPU Result (Backward)\"\n",
    "#print V_gpu\n",
    "print \"ERROR MADE: \" + str(np.sum(np.abs(V_naive-V_gpu)))\n",
    "print\n",
    "print \"Starting Gradient pass test:\"\n",
    "print \"----------------------------\"\n",
    "G_M_naive, G_b_naive, G_c_naive = naiveModel.expectedDerivative(P_naive, data)\n",
    "G_M_gpu,G_b_gpu,G_c_gpu = gradient(P_naive, data)\n",
    "print \"ERROR MADE (Motifs): \" + str(np.sum(np.abs(G_M_naive-G_M_gpu)))\n",
    "print \"ERROR MADE (Bias): \" + str(np.sum(np.abs(G_b_naive-G_b_gpu)))\n",
    "print \"ERROR MADE (c): \" + str(np.sum(np.abs(G_c_naive-G_c_gpu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test the gibbs sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gibbs Sampling test:\n",
      "----------------------------\n",
      "100 iterations done\n",
      "ERROR MADE: 0.0453333333333\n"
     ]
    }
   ],
   "source": [
    "print \"Starting Gibbs Sampling test:\"\n",
    "print \"----------------------------\"\n",
    "#data = np.array([allSeqs[random.randrange(0,len(allSeqs))] for i in range(1)])\n",
    "precision = 100\n",
    "V_naive_acc = np.zeros(realData.shape)\n",
    "V_gpu_acc = np.zeros(realData.shape)\n",
    "for i in range(precision):\n",
    "    V_naive = naiveModel.sampleVisibleLayer(naiveModel.backwardBatch(naiveModel.forwardBatch(realData)[1]))\n",
    "    V_gpu = gibbs(realData)\n",
    "    V_naive_acc += V_naive\n",
    "    V_gpu_acc += V_gpu\n",
    "    if i % 100 == 0:\n",
    "        print \"100 iterations done\"\n",
    "\n",
    "V_naive_acc /= precision\n",
    "V_gpu_acc /= precision\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(V_naive_acc-V_gpu_acc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare training procedures of both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'batch_size': 1,\n",
      " 'cd_k': 1,\n",
      " 'epochs': 1,\n",
      " 'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 1}\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done\n",
      "100 iterations done"
     ]
    }
   ],
   "source": [
    "#data = np.array([allSeqs[random.randrange(0,len(allSeqs))] for i in range(1)])\n",
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False\n",
    "naiveModel.updateWeights = False\n",
    "\n",
    "# compile theano function\n",
    "D = T.tensor4('data')\n",
    "updates = gpuModel.updateWeightsOnMinibatch(D, 1)\n",
    "der_m = updates[0][1]-updates[0][0]\n",
    "der_bias = updates[1][1]-updates[1][0]\n",
    "der_c = updates[2][1]-updates[2][0]\n",
    "train = theano.function([D], [der_m, der_bias, der_c], allow_input_downcast=True)\n",
    "\n",
    "precision = 10000\n",
    "der_m_naive = np.zeros(kernel.shape)\n",
    "der_m_gpu = np.zeros(kernel.shape)\n",
    "der_bias_naive = np.zeros(naiveModel.bias.shape)\n",
    "der_bias_gpu = np.zeros(gpuModel.bias.get_value().shape)\n",
    "der_c_naive = np.zeros(naiveModel.c.shape)\n",
    "der_c_gpu = np.zeros(gpuModel.c.get_value().shape)\n",
    "\n",
    "for i in range(precision):\n",
    "    # naive\n",
    "    [der_m_naive_l, der_bias_naive_l, der_c_naive_l] = naiveModel.train_model(realData, 1)\n",
    "    der_m_naive += der_m_naive_l\n",
    "    der_bias_naive += der_bias_naive_l\n",
    "    der_c_naive += der_c_naive_l\n",
    "    # gpu\n",
    "    [der_m_gpu_l, der_bias_gpu_l, der_c_gpu_l] = train(realData)\n",
    "    der_m_gpu += der_m_gpu_l\n",
    "    der_bias_gpu += der_bias_gpu_l\n",
    "    der_c_gpu += der_c_gpu_l\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print \"100 iterations done\"\n",
    "\n",
    "der_m_naive /= precision\n",
    "der_bias_naive /= precision\n",
    "der_c_naive /= precision\n",
    "\n",
    "der_m_gpu /= precision\n",
    "der_bias_gpu /= precision\n",
    "der_c_gpu /= precision\n",
    "\n",
    "print \"ERROR MADE (motifs): \" + str(np.mean(np.abs(der_m_naive - der_m_gpu)))\n",
    "print \"ERROR MADE (bias): \" + str(np.mean(np.abs(der_bias_naive - der_bias_gpu)))\n",
    "print \"ERROR MADE (c): \" + str(np.mean(np.abs(der_c_naive - der_c_gpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New motifs set. # Motifs: 1 K-mer-Length: 3\n",
      "{'batch_size': 1,\n",
      " 'cd_k': 1,\n",
      " 'epochs': 1,\n",
      " 'learning_rate': 0.1,\n",
      " 'motif_length': 3,\n",
      " 'number_of_motifs': 1,\n",
      " 'pooling_factor': 1}\n",
      "DONE WITH NAIVE---------\n",
      "BatchSize: 1\n",
      "Num of iterations per epoch: 1\n",
      "Start compiling Theano training function...\n",
      "Compilation of Theano training function finished in 1.29015398026 seconds\n",
      "Start training the model...\n",
      "[Epoch 0] done!\n",
      "Training finished after: 0.000234127044678 seconds!\n",
      "ERROR MADE: (motifs)0.0758070537269\n"
     ]
    }
   ],
   "source": [
    "naiveModel = NaiveCRBM(motifLength=hyper_params['motif_length'],\n",
    "                       numMotifs=hyper_params['number_of_motifs'],\n",
    "                       learningRate=hyper_params['learning_rate'],\n",
    "                       poolingFactor=hyper_params['pooling_factor'])\n",
    "\n",
    "gpuModel = CRBM(hyper_params)\n",
    "gpuModel.setToZero = True\n",
    "# set parameters\n",
    "naiveModel.setCustomKernels(kernel)\n",
    "gpuModel.setCustomKernels(kernel)\n",
    "gpuModel.batchSize = 1\n",
    "gpuModel.printHyperParams()\n",
    "gpuModel.debug = False\n",
    "naiveModel.debug = False\n",
    "\n",
    "naiveModel.trainMinibatch(data, hyper_params['epochs'], hyper_params['batch_size'], hyper_params['cd_k'])\n",
    "print \"DONE WITH NAIVE---------\"\n",
    "gpuModel.trainModel(data)\n",
    "\n",
    "new_motifs_gpu = gpuModel.motifs.get_value()\n",
    "new_motifs_naive = naiveModel.kernels\n",
    "\n",
    "print \"ERROR MADE: (motifs)\" + str(np.mean(np.abs(new_motifs_gpu-new_motifs_naive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test the sampling methods of both implementations\n",
    "In order to do that, sampling from both implementations has to be done multiple times to ensure the correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test of sampling for prob max pooling\n",
      "----------------------------\n",
      "ERROR MADE: 0.00447570945946\n"
     ]
    }
   ],
   "source": [
    "precision = 10000\n",
    "naiveModel.debug = False\n",
    "\n",
    "print \"Start test of sampling for prob max pooling\"\n",
    "print \"----------------------------\"\n",
    "[P_naive,S_n] = naiveModel.forwardBatch(data)\n",
    "\n",
    "S_naive = np.zeros((data.shape[0], naiveModel.numberOfKernels, 1, data.shape[3]-naiveModel.kernelLength+1))\n",
    "S_GPU = np.zeros(P_naive.shape)\n",
    "for i in range(precision):\n",
    "    [P_n,S_n] = naiveModel.forwardBatch(data)\n",
    "    [P_g,S_g] = forward(data)\n",
    "    S_naive += S_n\n",
    "    S_GPU += S_g\n",
    "S_naive /= precision\n",
    "S_GPU /= precision\n",
    "\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(S_naive-S_GPU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test of sampling for Visible Layer\n",
      "----------------------------\n",
      "V Probs:\n",
      "ERROR MADE: 0.0\n"
     ]
    }
   ],
   "source": [
    "V_P = T.tensor4('Reconstruction Probs')\n",
    "V = gpuModel.sampleVisibleLayer(V_P)\n",
    "sampleV = theano.function([V_P], V, allow_input_downcast=True)\n",
    "\n",
    "precision = 1000\n",
    "print \"Start test of sampling for Visible Layer\"\n",
    "print \"----------------------------\"\n",
    "print \"V Probs:\"\n",
    "#print V_naive\n",
    "\n",
    "Sample_naive = np.zeros((data.shape[0], naiveModel.numberOfKernels//2, 4, data.shape[3]))\n",
    "Sample_gpu = np.zeros(Sample_naive.shape)\n",
    "for i in range(precision):\n",
    "    V_n = naiveModel.sampleVisibleLayer(V_naive)\n",
    "    V_g = sampleV(V_naive)\n",
    "    Sample_naive += V_n\n",
    "    Sample_gpu += V_g\n",
    "Sample_naive /= precision\n",
    "Sample_gpu /= precision\n",
    "\n",
    "print \"ERROR MADE: \" + str(np.mean(np.abs(Sample_naive-Sample_gpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91  0.63  0.87  0.18]\n",
      " [ 0.91  0.2   0.6   0.91]\n",
      " [ 0.95  0.02  0.    0.93]\n",
      " [ 0.68  0.48  0.31  0.71]\n",
      " [ 0.11  0.72  0.49  0.3 ]\n",
      " [ 0.61  0.81  0.71  0.37]\n",
      " [ 0.18  0.77  0.44  0.64]\n",
      " [ 0.53  0.4   0.78  0.26]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9e7bc5d77ae6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial (numpy/random/mtrand/mtrand.c:29042)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(8,4)\n",
    "print x\n",
    "np.random.multinomial(n=1,pvals=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1/7.]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
