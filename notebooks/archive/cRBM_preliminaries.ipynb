{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional RBM (cRBM)\n",
    "\n",
    "This notebook takes care of implementing the basic functionality for cRBMs.\n",
    "Or maybe it's just for the preliminaries, that is some simple stuff before it actually comes to the Boltzmann Machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading the data and converting it to various forms of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: libcublas.so.7.0: cannot open shared object file: No such file or directory\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n",
      "WARNING:theano.sandbox.cuda:CUDA is installed, but device gpu is not available  (error: cuda unavilable)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import numpy as np\n",
    "import Bio.SeqIO as sio\n",
    "import Bio.motifs.matrix as mat\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classes to read biological files (such as FASTA or JASPAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class reads sequences from fasta files.\n",
    "To use it, create an instance of that object and use\n",
    "the function readSequencesFromFile.\n",
    "\"\"\"\n",
    "class FASTAReader:\n",
    "    \n",
    "    def __init__(self, _path):\n",
    "        self.path = _path\n",
    "        \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        dhsSequences = []\n",
    "        for dhs in sio.parse(open(filename), 'fasta', IUPAC.unambiguous_dna):\n",
    "            dhsSequences.append(dhs.seq)\n",
    "        return dhsSequences\n",
    "    \n",
    "    \n",
    "class JASPARReader:\n",
    "    \n",
    "    def __init__ (self):\n",
    "        pass\n",
    "    \n",
    "    def readSequencesFromFile (self, filename):\n",
    "        matrices = []\n",
    "        for mat in motifs.parse(open(filename), 'jaspar'):\n",
    "            matrices.append(mat.pwm)\n",
    "        return matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in JASPAR matrices and FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matReader = JASPARReader()\n",
    "pwms = matReader.readSequencesFromFile('data/jaspar_matrices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply the two classes to calculate a forward pass in our algorithm\n",
    "seqReader = FASTAReader('.')\n",
    "allSeqs = seqReader.readSequencesFromFile('data/wgEncodeAwgDnaseUwAg10803UniPk.fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert FASTA sequences to matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "test_set = [allSeqs[random.randrange(0,len(allSeqs))] for i in range(1000)]\n",
    "print len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of test set in (in ms): 298.624038696\n"
     ]
    }
   ],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    m = len(seq.alphabet.letters)\n",
    "    n = len(seq)\n",
    "    result = np.zeros((2, m, n))\n",
    "    revSeq = seq.reverse_complement()\n",
    "    for i in range(len(seq)):\n",
    "        result[0,getIntToLetter(seq[i]),i] = 1\n",
    "        result[1,getIntToLetter(revSeq[i]),i] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "dataMat = np.array([getMatrixFromSeq(t) for t in test_set])\n",
    "print \"Conversion of test set in (in ms): \" + str((time.time()-start)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a: Borrowing Ian Goodfellow's implementation of the probabilistic max pooling layer\n",
    "This implementation is now part of the pylearn2 library which is licensed under the 3-claused BSD license.\n",
    "Source code is available here: https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/expr/probabilistic_max_pooling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.gof.op import get_debug_values\n",
    "\n",
    "def max_pool(z, pool_shape, top_down=None, theano_rng=None):\n",
    "    \"\"\"\n",
    "    Probabilistic max-pooling\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : theano 4-tensor\n",
    "        a theano 4-tensor representing input from below\n",
    "    pool_shape : tuple\n",
    "        tuple of ints. the shape of regions to be pooled\n",
    "    top_down : theano 4-tensor, optional\n",
    "        a theano 4-tensor representing input from above\n",
    "        if None, assumes top-down input is 0\n",
    "    theano_rng : MRG_RandomStreams, optional\n",
    "        Used for random numbers for sampling\n",
    "    Returns\n",
    "    -------\n",
    "    p : theano 4-tensor\n",
    "        the expected value of the pooling layer p\n",
    "    h : theano 4-tensor\n",
    "        the expected value of the detector layer h\n",
    "    p_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the pooling layer\n",
    "    h_samples : theano 4-tensor, only returned if theano_rng is not None\n",
    "        samples of the detector layer\n",
    "    Notes\n",
    "    ------\n",
    "    all 4-tensors are formatted with axes ('b', 'c', 0, 1).\n",
    "    This is for maximum speed when using theano's conv2d\n",
    "    to generate z and top_down, or when using it to infer conditionals of\n",
    "    other layers using the return values.\n",
    "    Detailed description:\n",
    "    Suppose you have a variable h that lives in a Conv2DSpace h_space and\n",
    "    you want to pool it down to a variable p that lives in a smaller\n",
    "    Conv2DSpace p.\n",
    "    This function does that, using non-overlapping pools.\n",
    "    Specifically, consider one channel of h. h must have a height that is a\n",
    "    multiple of pool_shape[0] and a width that is a multiple of pool_shape[1].\n",
    "    A channel of h can thus be broken down into non-overlapping rectangles\n",
    "    of shape pool_shape.\n",
    "    Now consider one rectangular pooled region within one channel of h.\n",
    "    I now use 'h' to refer just to this rectangle, and 'p' to refer to\n",
    "    just the one pooling unit associated with that rectangle.\n",
    "    We assume that the space that h and p live in is constrained such\n",
    "    that h and p are both binary and p = max(h). To reduce the state-space\n",
    "    in order to make probabilistic computations cheaper we also\n",
    "    constrain sum(h) <= 1.\n",
    "    Suppose h contains k different units. Suppose that the only term\n",
    "    in the model's energy function involving h is -(z*h).sum()\n",
    "    (elemwise multiplication) and the only term in\n",
    "    the model's energy function involving p is -(top_down*p).sum().\n",
    "    Then P(h[i] = 1) = softmax( [ z[1], z[2], ..., z[k], -top_down] )[i]\n",
    "    and P(p = 1) = 1-softmax( [z[1], z[2], ..., z[k], -top_down])[k]\n",
    "    This variation of the function assumes that z, top_down, and all\n",
    "    return values use Conv2D axes ('b', 'c', 0, 1).\n",
    "    This variation of the function implements the softmax using a\n",
    "    theano graph of exp, maximum, sub, and div operations.\n",
    "    Performance notes:\n",
    "    It might be possible to make a faster implementation with different\n",
    "    theano ops. rather than using set_subtensor, it might be possible\n",
    "    to use the stuff in theano.sandbox.neighbours. Probably not possible,\n",
    "    or at least nasty, because that code isn't written with multiple\n",
    "    channels in mind, and I don't think just a reshape can fix it.\n",
    "    Some work on this in galatea.cond.neighbs.py\n",
    "    At some point images2neighbs' gradient was broken so check that\n",
    "    it has been fixed before sinking too much time into this.\n",
    "    Stabilizing the softmax is also another source of slowness.\n",
    "    Here it is stabilized with several calls to maximum and sub.\n",
    "    It might also be possible to stabilize it with\n",
    "    T.maximum(-top_down,T.signal.downsample.max_pool(z)).\n",
    "    Don't know if that would be faster or slower.\n",
    "    Elsewhere in this file I implemented the softmax with a reshape\n",
    "    and call to Softmax / SoftmaxWithBias.\n",
    "    This is slower, even though Softmax is faster on the GPU than the\n",
    "    equivalent max/sub/exp/div graph. Maybe the reshape is too expensive.\n",
    "    Benchmarks show that most of the time is spent in GpuIncSubtensor\n",
    "    when running on gpu. So it is mostly that which needs a faster\n",
    "    implementation. One other way to implement this would be with\n",
    "    a linear.Conv2D.lmul_T, where the convolution stride is equal to\n",
    "    the pool width, and the thing to multiply with is the hparts stacked\n",
    "    along the channel axis. Unfortunately, conv2D doesn't work right\n",
    "    with stride > 2 and is pretty slow for stride 2. Conv3D is used to\n",
    "    mitigate some of this, but only has CPU code.\n",
    "    \"\"\"\n",
    "\n",
    "    z_name = z.name\n",
    "    if z_name is None:\n",
    "        z_name = 'anon_z'\n",
    "\n",
    "    batch_size, ch, zr, zc = z.shape\n",
    "\n",
    "    r, c = pool_shape\n",
    "\n",
    "    zpart = []\n",
    "\n",
    "    mx = None\n",
    "\n",
    "    if top_down is None:\n",
    "        t = 0.\n",
    "    else:\n",
    "        t = - top_down\n",
    "        t.name = 'neg_top_down'\n",
    "\n",
    "    for i in xrange(r):\n",
    "        zpart.append([])\n",
    "        for j in xrange(c):\n",
    "            cur_part = z[:, :, i:zr:r, j:zc:c]\n",
    "            if z_name is not None:\n",
    "                cur_part.name = z_name + '[%d,%d]' % (i, j)\n",
    "            zpart[i].append(cur_part)\n",
    "            if mx is None:\n",
    "                mx = T.maximum(t, cur_part)\n",
    "                if cur_part.name is not None:\n",
    "                    mx.name = 'max(-top_down,' + cur_part.name + ')'\n",
    "            else:\n",
    "                max_name = None\n",
    "                if cur_part.name is not None:\n",
    "                    mx_name = 'max(' + cur_part.name + ',' + mx.name + ')'\n",
    "                mx = T.maximum(mx, cur_part)\n",
    "                mx.name = mx_name\n",
    "    mx.name = 'local_max(' + z_name + ')'\n",
    "\n",
    "    pt = []\n",
    "\n",
    "    for i in xrange(r):\n",
    "        pt.append([])\n",
    "        for j in xrange(c):\n",
    "            z_ij = zpart[i][j]\n",
    "            safe = z_ij - mx\n",
    "            safe.name = 'safe_z(%s)' % z_ij.name\n",
    "            cur_pt = T.exp(safe)\n",
    "            cur_pt.name = 'pt(%s)' % z_ij.name\n",
    "            pt[-1].append(cur_pt)\n",
    "\n",
    "    off_pt = T.exp(t - mx)\n",
    "    off_pt.name = 'p_tilde_off(%s)' % z_name\n",
    "    denom = off_pt\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            denom = denom + pt[i][j]\n",
    "    denom.name = 'denom(%s)' % z_name\n",
    "\n",
    "    off_prob = off_pt / denom\n",
    "    p = 1. - off_prob\n",
    "    p.name = 'p(%s)' % z_name\n",
    "\n",
    "    hpart = []\n",
    "    for i in xrange(r):\n",
    "        hpart.append([pt_ij / denom for pt_ij in pt[i]])\n",
    "\n",
    "    h = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "    for i in xrange(r):\n",
    "        for j in xrange(c):\n",
    "            h.name = 'h_interm'\n",
    "            h = T.set_subtensor(h[:, :, i:zr:r, j:zc:c], hpart[i][j])\n",
    "\n",
    "    h.name = 'h(%s)' % z_name\n",
    "\n",
    "    if theano_rng is None:\n",
    "        return p, h\n",
    "    else:\n",
    "        events = []\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                events.append(hpart[i][j])\n",
    "        events.append(off_prob)\n",
    "\n",
    "        events = [event.dimshuffle(0, 1, 2, 3, 'x') for event in events]\n",
    "\n",
    "        events = tuple(events)\n",
    "\n",
    "        stacked_events = T.concatenate(events, axis=4)\n",
    "\n",
    "        rows = zr // pool_shape[0]\n",
    "        cols = zc // pool_shape[1]\n",
    "        outcomes = pool_shape[0] * pool_shape[1] + 1\n",
    "        assert stacked_events.ndim == 5\n",
    "        for se, bs, r, c, chv in get_debug_values(stacked_events, batch_size,\n",
    "                                                  rows, cols, ch):\n",
    "            assert se.shape[0] == bs\n",
    "            assert se.shape[1] == r\n",
    "            assert se.shape[2] == c\n",
    "            assert se.shape[3] == chv\n",
    "            assert se.shape[4] == outcomes\n",
    "        reshaped_events = stacked_events.reshape((\n",
    "            batch_size * rows * cols * ch, outcomes))\n",
    "\n",
    "        multinomial = theano_rng.multinomial(pvals=reshaped_events,\n",
    "                                             dtype=p.dtype)\n",
    "\n",
    "        reshaped_multinomial = multinomial.reshape((batch_size, ch, rows,\n",
    "                                                    cols, outcomes))\n",
    "\n",
    "        h_sample = T.alloc(0., batch_size, ch, zr, zc)\n",
    "\n",
    "        idx = 0\n",
    "        for i in xrange(r):\n",
    "            for j in xrange(c):\n",
    "                h_sample = T.set_subtensor(h_sample[:, :, i:zr:r, j:zc:c],\n",
    "                                           reshaped_multinomial[:, :, :, :,\n",
    "                                           idx])\n",
    "                idx += 1\n",
    "\n",
    "        p_sample = 1 - reshaped_multinomial[:, :, :, :, -1]\n",
    "\n",
    "        return p, h, p_sample, h_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano.tensor.nnet.conv as conv\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "def _getLetterToInt (num):\n",
    "    if num == 0:\n",
    "        return 'A'\n",
    "    elif num == 1:\n",
    "        return 'C'\n",
    "    elif num == 2:\n",
    "        return 'G'\n",
    "    elif num == 3:\n",
    "        return 'T'\n",
    "    else:\n",
    "        print 'ERROR: Num ' + str(num) + \" not a valid char in DNA alphabet\"\n",
    "        return -1\n",
    "\n",
    "def _convertPWM2Array (pwm):\n",
    "    result = np.zeros((4, len(pwm['A'])))\n",
    "    for letter in range(len(pwm)):\n",
    "        result[letter] = pwm[_getLetterToInt(letter)]\n",
    "    return result\n",
    "  \n",
    "def makeIt3d (seq, numOfKernels):\n",
    "    x = np.tile(seq, [numOfKernels, 1, 1])\n",
    "    print x.shape\n",
    "    return x\n",
    "\n",
    "# data: 4D matrix with dims = (N_batch x 2 x 4 x N_v)\n",
    "# kernel: 4D matrix with dims = (K x 2 x 4 x number of k-mers)\n",
    "def forwardBatch (data, kernel, bias):\n",
    "    # create 4D tensor for theano (BatchSize x K x 2*numOfLetters x lenOfSeqs)\n",
    "    D = T.tensor4('data')\n",
    "    K = T.tensor4('kernels')\n",
    "    out = conv.conv2d(D,K)\n",
    "    out = out[:,:,::-1,::-1]\n",
    "    f = theano.function([D,K], out, allow_input_downcast=True)\n",
    "\n",
    "    bMod = bias[np.newaxis,:,np.newaxis,np.newaxis] # add dims to the bias until it works\n",
    "\n",
    "    return f(data, kernel) + bMod\n",
    "\n",
    "\n",
    "\n",
    "def backwardBatch (hiddenActivation, kernel, c):\n",
    "    # theano convolution call\n",
    "    H = T.tensor4('hidden')\n",
    "    K = T.tensor4('kernels')\n",
    "    K_star = K.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "    C = conv.conv2d(H, K_star, border_mode='full')\n",
    "    out = T.sum(C, axis=1) # sum over all K\n",
    "    res = out + c\n",
    "    \n",
    "    # add fourth dimension (the strands) that was lost during forward pass (max pooling)\n",
    "    res = np.tile(res[:,np.newaxis,:,:], [1,2,1,1])\n",
    "    return res\n",
    "\n",
    "\n",
    "def gradient (_H, V_0):\n",
    "    H = T.tensor4('hidden')\n",
    "    S = T.tensor4('sample')\n",
    "    H_reshaped = H.dimshuffle(1, 0, 2, 3)\n",
    "    out = conv.conv2d(S, H_reshaped)\n",
    "    f = theano.function([H,S], out, allow_input_downcast=True)\n",
    "    \n",
    "    return f(np.tile(np.mean(_H, axis=0), [2,1,1,1]), np.tile(V_0, [1,1,1,1]))\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + T.exp(x))\n",
    "\n",
    "def batchTraining (data, epochs, batchSize, numOfCDs):\n",
    "    itPerEpoch = data.shape[0] / batchSize\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(itPerEpoch):\n",
    "            D_batch = data[batch*batchSize:(batch+1)*batchSize]\n",
    "            H = self.probMaxPooling(self.forwardBatch(D_batch))\n",
    "            G_data = self.gradient(H, D_batch) # gradient for W for the data\n",
    "            bias_data = np.mean(np.sum(H, axis=3), axis=0) # gradient for bias\n",
    "            c_data = np.mean(np.sum(np.sum(np.sum(D_batch, axis=3), axis=2), axis=1), axis=0)\n",
    "            print \"C_data: \" + str(c_data)\n",
    "            for cd in range(numOfCDs):\n",
    "                S = self.sampleFromMatrix(H)\n",
    "                V = self.softmax(self.backwardBatch(S))\n",
    "                S_v = self.sampleFromMatrix(V)\n",
    "                H = self.probMaxPooling(self.forwardBatch(S_v))\n",
    "\n",
    "            G_model = self.gradient(H, D_batch)\n",
    "            bias_model = np.sum(np.sum(H, axis=3), axis=0)\n",
    "            c_model = np.mean(np.sum(np.sum(np.sum(V, axis=3), axis=2), axis=1), axis=0)\n",
    "            print \"C_model: \" + str(c_model)\n",
    "            #print \"Gradient of model: \" + str((G_data-G_model).shape)\n",
    "            #print \"For data: \" + str((batch*batchSize, (batch+1)*batchSize))\n",
    "            #print G_data-G_model\n",
    "\n",
    "            self.motifs = self.motifs + self.learningRate * (G_data - G_model)\n",
    "            self.bias = self.bias + self.learningRate * (bias_data - bias_model).sum(axis=1)\n",
    "            self.c = self.c + self.learningRate * (c_data - c_model)\n",
    "\n",
    "        print \"Epoch done: \" + str(epoch)\n",
    "        \n",
    "        \n",
    "def softmax_4d(softmax_input):\n",
    "    si = softmax_input.reshape((softmax_input.shape[0], softmax_input.shape[1], -1))\n",
    "    shp = (si.shape[0], 1, si.shape[2])\n",
    "    exp = T.exp(si - si.max(axis=1).reshape(shp))\n",
    "    softmax_expression = (exp / exp.sum(axis=1).reshape(shp) ).reshape(softmax_input.shape)\n",
    "    return softmax_expression\n",
    "\n",
    "\n",
    "def allBatch (data, kernel, bias, c, cd, rng):\n",
    "    \n",
    "    # do forward convolution (get H)\n",
    "    V = T.tensor4('data')\n",
    "    K = T.tensor4('kernels')\n",
    "    bMod = bias[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "\n",
    "    H = T.nnet.sigmoid(conv.conv2d(V, K) + bMod)\n",
    "    \n",
    "    # compute the derivatives of the data\n",
    "    H_reshaped = np.tile(T.mean(H, axis=0), [2,1,1,1]).dimshuffle(1, 0, 2, 3)\n",
    "    GData_w = conv.conv2d(H, np.tile(V, [1,1,1,1]))\n",
    "    GData_bias = T.mean(T.sum(H, axis=3), axis=0)\n",
    "    \n",
    "    for conDiv in range(cd):\n",
    "        S = rng.binomial(size=H.shape)\n",
    "        \n",
    "        # now we have the sample, perform backward pass\n",
    "        K_star = K.dimshuffle(1, 0, 2, 3)[:,:,::-1,::-1]\n",
    "        V_ = conv.conv2d(S, K_star, border_mode='full')\n",
    "        V_ = 1. / (1. + T.exp(V_)) # sigmoid\n",
    "        pre_V = T.sum(V_ + c, axis=1) # sum over all K\n",
    "        V = softmax_4d(np.tile(V[:,np.newaxis,:,:], [1,2,1,1]))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        rng = RandomStreams(np.random.RandomState(1234).randint(2 ** 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (2, 2, 4, 8)\n",
      "[[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  1.  1.  1.  1.]\n",
      "   [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "   [ 1.  1.  1.  1.  0.  1.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "   [ 0.  0.  0.  0.  0.  0.  0.  1.]]]\n",
      "\n",
      "\n",
      " [[[ 1.  0.  0.  0.  1.  0.  0.  0.]\n",
      "   [ 0.  1.  0.  0.  0.  1.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  0.  0.  1.  0.]\n",
      "   [ 0.  0.  0.  1.  0.  0.  0.  1.]]\n",
      "\n",
      "  [[ 1.  0.  0.  0.  1.  0.  0.  0.]\n",
      "   [ 0.  1.  0.  0.  0.  1.  0.  0.]\n",
      "   [ 0.  0.  1.  0.  0.  0.  1.  0.]\n",
      "   [ 0.  0.  0.  1.  0.  0.  0.  1.]]]]\n",
      "Kernel: (2, 2, 4, 3)\n",
      "[[[[1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]\n",
      "   [0 0 1]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [1 1 1]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [1 1 1]\n",
      "   [0 0 0]]]]\n",
      "Result from forward: (2, 2, 1, 6)\n",
      "[[[[ 4.  1.  2.  1.  4.  1.]]\n",
      "\n",
      "  [[ 1.  1.  2.  2.  4.  4.]]]\n",
      "\n",
      "\n",
      " [[[ 6.  0.  0.  0.  6.  0.]]\n",
      "\n",
      "  [[ 2.  2.  2.  0.  2.  2.]]]]\n",
      "Expectation of H -> (2, 2, 1, 6)\n",
      "[[[[ 0.93623954  0.04661262  0.66524094  0.24472848  0.93623954  0.04661262]]\n",
      "\n",
      "  [[ 0.42231882  0.42231882  0.46831051  0.46831051  0.4954626   0.4954626 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.99506688  0.00246652  0.33333334  0.33333334  0.99506688  0.00246652]]\n",
      "\n",
      "  [[ 0.46831051  0.46831051  0.78698605  0.10650698  0.46831051  0.46831051]]]]\n",
      "Sampled from H -> (2, 2, 1, 6)\n",
      "[[[[ 1.  0.  1.  0.  1.  0.]]\n",
      "\n",
      "  [[ 0.  1.  1.  0.  0.  1.]]]\n",
      "\n",
      "\n",
      " [[[ 1.  0.  1.  0.  1.  0.]]\n",
      "\n",
      "  [[ 0.  1.  1.  0.  0.  1.]]]]\n"
     ]
    }
   ],
   "source": [
    "kernel1 = np.tile(np.array([[1,0,0],[0,1,0],[0,0,1],[0,0,0]]), [2,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0],[0,0,0],[1,1,1],[0,0,0]]), [2,1,1])\n",
    "#kernel1 = np.tile(np.eye(4), [2,1,1])\n",
    "#kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1])\n",
    "kernel = np.array([kernel1, kernel2])\n",
    "filter_shape = kernel.shape\n",
    "\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2])\n",
    "print \"Data: \" + str(data.shape)\n",
    "print data\n",
    "print \"Kernel: \" + str(kernel.shape)\n",
    "print kernel\n",
    "D = T.tensor4(name='data')\n",
    "K = T.tensor4(name='kernel')\n",
    "C = conv.conv2d(D, K, filter_shape=[filter_shape[k] for k in [1,0,2,3]])\n",
    "out = C[:,:,::-1,::-1]\n",
    "b = theano.function([D,K],out, allow_input_downcast=True)\n",
    "tmp = b(data,kernel)\n",
    "print \"Result from forward: \" + str(tmp.shape)\n",
    "print tmp\n",
    "\n",
    "\n",
    "D = T.tensor4('data')\n",
    "K = T.tensor4('kernel')\n",
    "C = conv.conv2d(D, K, filter_shape=[filter_shape[k] for k in [1,0,2,3]])\n",
    "out = C[:,:,::-1,::-1]\n",
    "res = max_pool(z=out, pool_shape=(1,2), top_down=None, theano_rng=rng)\n",
    "f = theano.function([D,K], res, allow_input_downcast=True)\n",
    "\n",
    "P = f(data, kernel)\n",
    "\n",
    "#print \"Expectation of P -> \" + str(P[0].shape)\n",
    "#print P[0]\n",
    "print \"Expectation of H -> \" + str(P[1].shape)\n",
    "print P[1]\n",
    "#print \"Sampled from P -> \" + str(P[2].shape)\n",
    "#print P[2]\n",
    "print \"Sampled from H -> \" + str(P[3].shape)\n",
    "print P[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing incredibly fast theano convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape.0\n",
      "(2, 2, 4, 8)\n"
     ]
    },
    {
     "ename": "AsTensorError",
     "evalue": "('Cannot convert [[[[Subtensor{::, ::, ::, ::}.0]]\\n\\n  [[Subtensor{::, ::, ::, ::}.0]]]] to TensorType', <type 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAsTensorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c2589df42775>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackwardBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-f5da342ac8ac>\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbatchTraining\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumOfCDs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \"\"\"\n\u001b[0;32m    506\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'return_list'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_test_value\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'off'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/tensor/elemwise.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[0musing\u001b[0m \u001b[0mDimShuffle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         \"\"\"\n\u001b[1;32m--> 527\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mas_tensor_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m         shadow = self.scalar_op.make_node(\n\u001b[0;32m    529\u001b[0m                 *[get_scalar_type(dtype=i.type.dtype).make_variable()\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-11-21/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mas_tensor_variable\u001b[1;34m(x, name, ndim)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mstr_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAsTensorError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot convert %s to TensorType\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;31m# this has a different name, because _as_tensor_variable is the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAsTensorError\u001b[0m: ('Cannot convert [[[[Subtensor{::, ::, ::, ::}.0]]\\n\\n  [[Subtensor{::, ::, ::, ::}.0]]]] to TensorType', <type 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "kernel1 = np.tile(np.eye(4), [2,1,1])\n",
    "kernel2 = np.tile(np.array([[0,0,0,0],[0,0,0,0],[1,1,1,1],[0,0,0,0]]), [2,1,1])\n",
    "kernel = np.array([kernel1, kernel2])\n",
    "randSeq1 = getMatrixFromSeq(Seq(\"ACGTGGGG\", IUPAC.unambiguous_dna))\n",
    "randSeq2 = getMatrixFromSeq(Seq(\"ACGTACGT\", IUPAC.unambiguous_dna))\n",
    "data = np.array([randSeq1, randSeq2])\n",
    "res = sigmoid(forwardBatch(data, kernel, np.zeros(2)))\n",
    "print res.shape\n",
    "print data.shape\n",
    "vis = sigmoid(backwardBatch(res, kernel, 0))\n",
    "grad = gradient(res, data)\n",
    "print grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2, 4, 4)\n",
      "(1000, 2, 4, 150)\n",
      "(1000, 10, 4, 4)\n",
      "Time for processing (for, back, grad) of 1000 sequences: 0.197737932205\n"
     ]
    }
   ],
   "source": [
    "kernel = np.tile(np.eye(4), [10, 2,1,1])\n",
    "print kernel.shape\n",
    "print dataMat.shape\n",
    "start = time.time()\n",
    "res = forwardBatch(dataMat, kernel)\n",
    "vis = backwardBatch(res, kernel)\n",
    "grad = gradient(res, dataMat)\n",
    "print grad.shape\n",
    "print \"Time for processing (for, back, grad) of \" + str(dataMat.shape[0]) + \" sequences: \" + str((time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation of our convRBM so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class implements a cRBM for sequence analysis.\n",
    "It does perform efficient forward and backward pass of any given DNA sequence.\n",
    "It implements softmax and sigmoid functions as activation and performs probabilistic max pooling\n",
    "after the convolution step.\n",
    "So this class is basically a two layer network with a convolution layer and a pooling layer on top\n",
    "of that.\n",
    "The learning procedure uses contrastive divergence (CD) with a variable amount of steps.\n",
    "\"\"\"\n",
    "\n",
    "class ConvRBM:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the cRBM. The parameters here are global params that should not change\n",
    "    during the execution of training or testing and characterize the network.\n",
    "    \n",
    "    Parameters:\n",
    "    _motifLength:    How long are the motifs (position weight matrices PWM). This\n",
    "                     This is equivalent to ask what the number of k-mers is.\n",
    "                     The current approach only deals with one fixed motif length.\n",
    "                     \n",
    "    _numMotifs:      How many motifs are applied to the sequence, that is how many\n",
    "                     hidden units does the network have. Each hidden unit consists\n",
    "                     of a vector of size (sequenceLength-motifLength+1)\n",
    "                     \n",
    "    _poolingFactor:  How many units from the hidden layer are pooled together.\n",
    "                     Note that the number has to divide evenly to the length of\n",
    "                     the hidden units, that is:\n",
    "                     mod(sequenceLength-motifLength+1, poolingFactor) == 0\n",
    "                     (1 = equivalent to sigmoid activation)\n",
    "                     \n",
    "    _alphabet:       Biopython uses alphabets for sequences to do sanity checks.\n",
    "                     However, all of the code is written for DNA sequences and even\n",
    "                     though in theory there should be no difference between that\n",
    "                     and other alphabets, Biopython may have trouble with the convolution.\n",
    "    \"\"\"\n",
    "    def __init__ (self, _motifLength, _numMotifs, _learningRate=0.1, _poolingFactor=1, _alphabet=IUPAC.unambiguous_dna):\n",
    "        # parameters for the motifs\n",
    "        self.motifLength = _motifLength\n",
    "        self.numMotifs = _numMotifs\n",
    "        self.motifs = []\n",
    "        self.alphabet = _alphabet\n",
    "        self.poolingFactor = _poolingFactor\n",
    "        \n",
    "        # cRBM parameters\n",
    "        self.bias = np.random.rand(self.numMotifs)\n",
    "        self.c = random.random()\n",
    "        self.learningRate = _learningRate\n",
    "        \n",
    "        # infrastructural parameters\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "    \"\"\"\n",
    "    This function initializes the motifs (or PWMs) of the cRBM. Maybe this function will\n",
    "    be removed in future versions and initialization will be performed by the\n",
    "    c'tor.\n",
    "    \"\"\"\n",
    "    def initializePWMs (self):\n",
    "        # set up PWMs\n",
    "        for m in range(self.numMotifs):\n",
    "            self.motifs.append(self._createRandomMotif(self.motifLength, self.alphabet))\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Calculate the forward pass for any given sequence, that is P(H | V).\n",
    "    This method applies convolution of all filters to the sequence, using the Biopython package.\n",
    "    It also looks on both strands for matches and returns the hidden activation layer.\n",
    "    It performs also the probabilistic max pooling on the convoluted data.\n",
    "    \n",
    "    Parameters:\n",
    "    seq:             The DNA sequence to calculate the forward pass on. The sequence is of\n",
    "                     type seq.\n",
    "                     \n",
    "    Return:\n",
    "    The function returns the hidden activation layer as numpy matrix.\n",
    "    That matrix has dimensionality 2 * (len(seq) - motifLength + 1) x K where K is the number\n",
    "    of kernels (motifs/PWMs) that are applied.\n",
    "    Since the algorithm looks on both strands, the factor 2 is present.\n",
    "    \"\"\"\n",
    "    def forwardPass (self, seq):\n",
    "        \n",
    "        # check that we actually have some motifs to do convolution on\n",
    "        if self.motifs == []:\n",
    "            print 'Error: No motifs created so far. Try executing initializePWMs before!'\n",
    "            return\n",
    "        if (len(seq)-self.motifLength+1) % self.poolingFactor != 0:\n",
    "            print 'Dimension mismatch: cannot create pooling layer because it would not fit!'\n",
    "\n",
    "        # perform convolution of motif and sequence (that is, apply the motif to the sequence)\n",
    "        hiddenActivation = np.zeros((2*self.numMotifs, len(seq)-self.motifLength+1))\n",
    "        motifCount = 0\n",
    "        for motif in self.motifs:\n",
    "            pssm = motif.log_odds()\n",
    "            \n",
    "            # apply convolution on both strands\n",
    "            hiddenActivation[motifCount*2,:] = pssm.calculate(seq) + self.bias[motifCount]\n",
    "            hiddenActivation[motifCount*2+1,:] = pssm.calculate(seq.reverse_complement()) + self.bias[motifCount]\n",
    "            hiddenActivation[motifCount*2:motifCount*2+2,:] = self._probMaxPooling(hiddenActivation[motifCount*2:motifCount*2+2,:])\n",
    "            motifCount += 1\n",
    "\n",
    "        return hiddenActivation\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the backward pass on the data, that is P(V | H).\n",
    "    It does so by performing convolution of the hidden layer and the data for each letter\n",
    "    in the alphabet, respecting both strands.\n",
    "    The de-convolutions for all kernels are added and the final sequence is obtained\n",
    "    using the softmax over all four letters.\n",
    "    \n",
    "    Parameters:\n",
    "    hidden layer:   The layer that was previously computed by the forward pass.\n",
    "    \n",
    "    Returns:        The reconstructed sequence. \n",
    "    \"\"\"\n",
    "    def backwardPass (self, hiddenActivation):\n",
    "        \n",
    "        # apply convolution on all of the filters\n",
    "        restoredLength = hiddenActivation.shape[1] + self.motifLength - 1\n",
    "        numOfLetters = len(self.alphabet.letters)\n",
    "        reConv = np.zeros((numOfLetters, restoredLength))\n",
    "\n",
    "        #start = time.time()\n",
    "        reConv = np.zeros((numOfLetters, restoredLength))\n",
    "        for k in range(len(self.motifs)):\n",
    "            # apply convolution on each of the channels (A, C, G, T) seperately\n",
    "            matrix = self._convertPWM2Array(self.motifs[k])\n",
    "            for i in range(numOfLetters):\n",
    "                conv1 = np.convolve(hiddenActivation[k], matrix[i])\n",
    "                conv2 = np.convolve(hiddenActivation[k+1], matrix[i])\n",
    "                reConv[i,:] = reConv[i,:] + conv1 + conv2 + self.c\n",
    "\n",
    "        #convTime = time.time()\n",
    "        # perform softmax and select index of the most promising one (results in visibleActivation)\n",
    "        visibleActivation = np.zeros((1, restoredLength))\n",
    "        \n",
    "        # calculate exp for whole matrix\n",
    "        reConv = np.exp(reConv)\n",
    "        \n",
    "        # calculate the sum (over all four letters for whole sequence)\n",
    "        sums = np.sum(reConv, 0)\n",
    "\n",
    "        # divide by the sum\n",
    "        for i in range(numOfLetters):\n",
    "            reConv[i,:] = reConv[i,:] / sums[i]\n",
    "            \n",
    "        # and the maximum is our letter...\n",
    "        visibleActivation = np.argmax(reConv, 0)\n",
    "        \n",
    "        #print \"Done with Softmax in: \" + str((time.time()-convTime)*1000)\n",
    "        # convert the resulting sequence to actual letters (A, C, G, T instead of 0, 1, 2, 3)\n",
    "        return self._getDNASeqFromNumericals(visibleActivation)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    The training algorithm for cRBMs. This uses the forward and backward pass\n",
    "    to collect the statistics for them.\n",
    "    \"\"\"\n",
    "    def miniBatchTraining (self, sequences, batchSize, cd_value):\n",
    "        \n",
    "        # first, some vars that we need all the time\n",
    "        sequenceLength = len(sequences[0])\n",
    "        hiddenUnitLength = sequenceLength - self.motifLength + 1\n",
    "        \n",
    "        # first, compute expected value of the data\n",
    "        # that means computing forward pass for all datapoints and computing the gradient\n",
    "        hiddenActivation = np.zeros((2*self.numMotifs, hiddenUnitLength))\n",
    "        derivatives = np.zeros((self.motifLength, self.numMotifs))\n",
    "        for seq in sequences:\n",
    "            hiddenActivation = self.forwardPass(seq)\n",
    "            \n",
    "            # calculate the gradients of the data\n",
    "            # In order to do that, we have to convolve the DNA and hidden layer\n",
    "            # This can be done by expressing DNA as four different convolutions (for each letter)\n",
    "            # We would need to represent the DNA as matrix of 4 x lengthOfDHS\n",
    "            # Example:\n",
    "            # -------\n",
    "            # ACGTGGGG\n",
    "            # 10000000\n",
    "            # 01000000\n",
    "            # 00101111\n",
    "            # 00010000\n",
    "            # Then, we can apply 4 convolutions to the sequence\n",
    "            for k in range(self.numMotifs):\n",
    "                derivatives[k,:] += np.convolve( hiddenActivation[k,:])\n",
    "            \n",
    "        print derivatives\n",
    "        # Now, sample from this distribution and collect the model's statistics\n",
    "        # Do that by sampling from the data statistics, recompute the model's statistics\n",
    "        # and so on until reaching the stationary distribution (in case of CD, do it once)\n",
    "        hidden_model = hiddenActivation\n",
    "        for it in range(cd_value):\n",
    "            sample = self._sampleFromHidden(hidden_model)\n",
    "            visible = self.backwardPass(sample)\n",
    "            hidden_model = self.forwardPass(visible)\n",
    "\n",
    "        print hidden_model.shape\n",
    "        \n",
    "        return self.learningRate * (hiddenActivation - hidden_model)\n",
    "            \n",
    "    \n",
    "    def _probMaxPooling (self, h_k):\n",
    "\n",
    "        # first of all some easy definitions\n",
    "        l = h_k.shape[1]\n",
    "        numOfGroups = l/self.poolingFactor\n",
    "        P = np.zeros((2, l))\n",
    "\n",
    "        # exponent of everything\n",
    "        ex = np.exp(h_k)\n",
    "        \n",
    "        # reshape s.t. each group forms one row\n",
    "        newDim = (numOfGroups, -1)\n",
    "        reordered = np.append(ex[0].reshape(newDim), ex[1].reshape(newDim), 1)\n",
    "        #print \"Shape of reordered: \" + str(reordered.shape)\n",
    "        # calculate denominators (sum of all rows)\n",
    "        denoms = np.sum(reordered, 1) + 1 # denoms for all groups (add 1 to have log. unit)\n",
    "        res = np.argmax(reordered.T / denoms, 0)\n",
    "\n",
    "        # calculate the actual values of the pooling layer P\n",
    "        for group in range(numOfGroups):\n",
    "            if reordered[group,res[group]] > 1: # check if really any element from P should be active\n",
    "                # we don't care about strands so just set res = res/2 for the index\n",
    "                idx = group * self.poolingFactor + int(res[group]/2)\n",
    "                P[res[group] % 2, idx] = reordered[group,res[group]] / denoms[group]\n",
    "        return P\n",
    "        \n",
    "    def _createRandomMotif (self, motifLength, alphabet):\n",
    "        counts = {}\n",
    "        for letter in alphabet.letters:\n",
    "            counts[letter] = [random.randint(0,100) for x in xrange(motifLength)]\n",
    "        return mat.PositionWeightMatrix(alphabet, counts)\n",
    "        \n",
    "    def _sampleFromHidden (self, hiddenActivation):\n",
    "        boolean_mat = hiddenActivation > self.rng.random_sample(hiddenActivation.shape)\n",
    "        return boolean_mat.astype(int)\n",
    "        \n",
    "        \n",
    "    def _sigmoid (self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _softmaxActivation (self, col, idx):\n",
    "        p_all = np.sum(np.exp(col))\n",
    "        p_x = np.exp(col[idx])\n",
    "        return p_x / p_all\n",
    "\n",
    "    def _getLetterToInt (self, num):\n",
    "        if num == 0:\n",
    "            return 'A'\n",
    "        elif num == 1:\n",
    "            return 'C'\n",
    "        elif num == 2:\n",
    "            return 'G'\n",
    "        elif num == 3:\n",
    "            return 'T'\n",
    "        else:\n",
    "            print 'ERROR: Num ' + str(num) + \" not a valid char in DNA alphabet\"\n",
    "            return -1\n",
    "\n",
    "    def _getDNASeqFromNumericals (self, seq):\n",
    "        dna_seq = []\n",
    "        for num in range(seq.shape[0]):\n",
    "            dna_seq.append(self._getLetterToInt(seq[num]))\n",
    "        return Seq(\"\".join(dna_seq), alphabet=self.alphabet)\n",
    "\n",
    "    def _convertPWM2Array (self, pwm):\n",
    "        result = np.zeros((len(self.alphabet.letters), self.motifLength))\n",
    "        for letter in range(len(pwm)):\n",
    "            result[letter] = pwm[self._getLetterToInt(letter)]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "convolve() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-670528c95fe1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# take some sequences and learn the 2 motifs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdata_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminiBatchTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Time for minibatch training (in sec): \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-36b810ee581e>\u001b[0m in \u001b[0;36mminiBatchTraining\u001b[1;34m(self, sequences, batchSize, cd_value)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;31m# Then, we can apply 4 convolutions to the sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumMotifs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0mderivatives\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mhiddenActivation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mderivatives\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: convolve() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "# initialize cRBM\n",
    "L = ConvRBM(4,2)\n",
    "L.initializePWMs()\n",
    "\n",
    "# create test set of sequences\n",
    "testSet = [allSeqs[random.randrange(0, len(allSeqs))] for x in range(100)]\n",
    "# take some sequences and learn the 2 motifs\n",
    "start = time.time()\n",
    "data_stats = L.miniBatchTraining(testSet, 1, 1)\n",
    "print \"Time for minibatch training (in sec): \" + str(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code from someone else that did it with theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T_activation_v_OLD(self, h):\n",
    "    # vbias is per visible dimension\n",
    "    # The first two dimensions of the weights tensor have to be swapped here,\n",
    "    # because they represent the number of input and output feature maps.\n",
    "    W_shuffled = self.W.dimshuffle(1, 0, 2, 3)\n",
    "    filter_shape = [self.filter_shape[k] for k in [1, 0, 2, 3]]\n",
    "    return conv.conv2d(h, W_shuffled, border_mode='full', image_shape=self.hiddens_shape, filter_shape=filter_shape) + self.vbias.dimshuffle('x', 'x', 0, 'x')\n",
    "\n",
    "\n",
    "def T_activation_v(self, h):\n",
    "    # this version of T_activation_v swaps 'dim' and 'hmaps'dimensions,\n",
    "    # because this seems to be a LOT faster.\n",
    "\n",
    "    # It seems to be a little less precise than the original, butotherwise\n",
    "    # the output is identical.\n",
    "\n",
    "    W_shuffled = self.W.dimshuffle(2, 1, 0, 3)\n",
    "\n",
    "    # The hmaps dimension needs to be flipped, because it waspreviously not convolved over,\n",
    "    # and now it is. Note that a valid convolution with filtersize =inputsize is NOT\n",
    "    # equivalent with a product - this is only the case if you FLIPthe filter.\n",
    "    # Note that the dim dimension need not be flipped, because this isconvolved over\n",
    "    # when computing T_activation_h (so it is already inherentlyflipped).\n",
    "    W_shuffled = W_shuffled[:,:,::-1,:]\n",
    "\n",
    "    h_shuffled = h.dimshuffle(0, 2, 1, 3)\n",
    "\n",
    "    # this is a full convolution in the time direction and a valid onein the other,\n",
    "    # hence the need to pad the input manually.\n",
    "    zero_padding = T.zeros_like(h_shuffled)[:,:,:,0:self.filter_width-1]\n",
    "    h_padded = T.concatenate([zero_padding, h_shuffled, zero_padding],axis=3)\n",
    "\n",
    "    filter_shape = [self.filter_shape[k] for k in [2, 1, 0, 3]]\n",
    "    image_shape = [self.hiddens_shape[k] for k in [0, 2, 1, 3]]\n",
    "    image_shape[3] += 2*(self.filter_width-1)\n",
    "    tmp = conv.conv2d(h_padded, W_shuffled, border_mode='valid',image_shape=image_shape, filter_shape=filter_shape)\n",
    "    return tmp.dimshuffle(0, 2, 1, 3) + self.vbias.dimshuffle('x','x', 0, 'x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test how to improve representation of DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small test matrix converstion took (in ms): 0.546216964722\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "ERROR. LETTER N DOES NOT EXIST!\n",
      "Conversion from DNA to matrix took: 24.7349979877\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  1.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.\n",
      "   0.  1.  1.  1.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  1.  1.\n",
      "   1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.\n",
      "   0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  1.  1.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  1.  0.  0.  1.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0.\n",
      "   0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  1.  0.  1.  1.  1.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.\n",
      "   0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.\n",
      "   0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   1.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  1.  1.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.\n",
      "   0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.\n",
      "   0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  1.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def getIntToLetter (letter):\n",
    "    if letter == 'A' or letter == 'a':\n",
    "        return 0\n",
    "    elif letter == 'C' or letter == 'c':\n",
    "        return 1\n",
    "    elif letter == 'G' or letter == 'g':\n",
    "        return 2\n",
    "    elif letter == 'T' or letter == 't':\n",
    "        return 3\n",
    "    else:\n",
    "        print \"ERROR. LETTER \" + letter + \" DOES NOT EXIST!\"\n",
    "        return -1\n",
    "\n",
    "def getMatrixFromSeq (seq):\n",
    "    numOfLetters = len(seq.alphabet.letters)\n",
    "    result = np.zeros((numOfLetters, len(seq)))\n",
    "    for i in range(len(seq)):\n",
    "        result[getIntToLetter(seq[i]),i] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "start1 = time.time()\n",
    "randSeq = Seq(\"ACGTGGGG\", L.alphabet)\n",
    "randSeq = allSeqs[random.randrange(0, len(allSeqs))]\n",
    "randSeq = randSeq.upper()\n",
    "m = getMatrixFromSeq(randSeq)\n",
    "print \"Small test matrix converstion took (in ms): \" + str((time.time()-start1)*1000)\n",
    "start = time.time()\n",
    "for seq in allSeqs:\n",
    "    getMatrixFromSeq(seq)\n",
    "print \"Conversion from DNA to matrix took: \" + str(time.time()-start)\n",
    "print m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a simple test sequence to verify that forward and backward pass work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer:\n",
      "[[ 0.          0.          0.          0.          0.99966465]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.99966465  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.99966465]]\n",
      "Reconstruction:\n",
      "ACGTGGGG\n"
     ]
    }
   ],
   "source": [
    "learner = ConvRBM(4, 2)\n",
    "learner.bias = np.zeros(2)\n",
    "learner.c = 0\n",
    "# create PWM (that will look for a sequence of 4 Gs)\n",
    "counts = {}\n",
    "for letter in learner.alphabet.letters:\n",
    "    if letter != 'G':\n",
    "        counts[letter] = [0 for x in xrange(learner.motifLength)]\n",
    "    else:\n",
    "        counts[letter] = [1 for x in xrange(learner.motifLength)]\n",
    "kernel = mat.PositionWeightMatrix(learner.alphabet, counts)\n",
    "learner.motifs.append(kernel)\n",
    "\n",
    "# create second kernel that will look for ACGT\n",
    "counts = {}\n",
    "for letter in range(len(learner.alphabet.letters)):\n",
    "    counts[learner._getLetterToInt(letter)] = [int(x == letter) for x in xrange(learner.motifLength)]\n",
    "\n",
    "kernel = mat.PositionWeightMatrix(learner.alphabet, counts)\n",
    "learner.motifs.append(kernel)\n",
    "\n",
    "# set the cRBMs other params to 0\n",
    "learner.bias = np.zeros(learner.numMotifs)\n",
    "learner.c = 0\n",
    "\n",
    "# now test forward pass on a simple sequence\n",
    "testSeq = Seq(\"ACGTGGGG\", learner.alphabet)\n",
    "h = learner.forwardPass(testSeq)\n",
    "print \"Hidden Layer:\"\n",
    "print h\n",
    "\n",
    "maxPooled = learner._probMaxPooling(h[:2])\n",
    "reconstructed = learner.backwardPass(h)\n",
    "print \"Reconstruction:\"\n",
    "print reconstructed\n",
    "\n",
    "# Should be able to completely reconstruct the sequence because we gave two kernels\n",
    "# which both can be hold responsible for a portion of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for forward: 4.54807281494\n",
      "Time for backward: 1.15513801575\n"
     ]
    }
   ],
   "source": [
    "x = ConvRBM(4, 10, 1)\n",
    "x.initializePWMs()\n",
    "\n",
    "testSeq = allSeqs[7190]\n",
    "startForward = time.time()\n",
    "h = x.forwardPass(testSeq)\n",
    "print \"Time for forward: \" + str((time.time()-startForward)*1000)\n",
    "startBackward = time.time()\n",
    "x.backwardPass(h)\n",
    "print \"Time for backward: \" + str((time.time()-startBackward)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the JASPAR data and see whether the sequences can be reproduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of JASPAR matrices: 593\n",
      "Average motif length (k-mer length): 10.7993254637\n",
      "\n",
      "Number of motifs with length 6 : 37\n",
      "Verfication: 6.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2c36f1528093>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# count the differences between the two sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdifferences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtestSeq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mdifferences\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "l = 6\n",
    "numMats = len(pwms)\n",
    "avgLength = np.mean([len(x[0]) for x in pwms])\n",
    "matsOfSpecificLength = [x for x in pwms if len(x[0]) == l]\n",
    "avgRedLength = np.mean([len(x[0]) for x in matsOfSpecificLength])\n",
    "print \"Total number of JASPAR matrices: \" + str(numMats)\n",
    "print \"Average motif length (k-mer length): \" + str(avgLength)\n",
    "print\n",
    "print \"Number of motifs with length \" + str(l) + \" : \" + str(len(matsOfSpecificLength))\n",
    "print \"Verfication: \" + str(avgRedLength)\n",
    "\n",
    "cRBM = ConvRBM(11, len(matsOfSpecificLength))\n",
    "cRBM.bias = np.zeros(len(matsOfSpecificLength))\n",
    "cRBM.c = 0\n",
    "# insert our pwms\n",
    "cRBM.motifs = matsOfSpecificLength\n",
    "\n",
    "# perform forward and backward pass\n",
    "correct = []\n",
    "errors = []\n",
    "times = []\n",
    "for i in range(1000):\n",
    "    testSeq = allSeqs[random.randrange(0, len(allSeqs))]\n",
    "    start = time.time()\n",
    "    hiddenActivation = learner.forwardPass(testSeq)\n",
    "    restored = learner.backwardPass(hiddenActivation)\n",
    "    times.append(time.time()-start)\n",
    "    \n",
    "    # count the differences between the two sequences\n",
    "    differences = 0\n",
    "    for elem in range(len(string)):\n",
    "        if string[elem] != testSeq[elem]:\n",
    "            differences += 1\n",
    "\n",
    "    #print \"Correct: \" + str(len(string)-differences)\n",
    "    #print \"Errors: \" + str(differences)\n",
    "    correct.append(len(string)-differences)\n",
    "    errors.append(differences)\n",
    "    \n",
    "print \"average correct: \" + str(np.mean(correct))\n",
    "print \"average error: \" + str(np.mean(errors))\n",
    "print \"var of error: \" + str(np.var(errors))\n",
    "print \"average time for forward and backward pass (in ms): \" + str(np.mean(times)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the forward and backward pass do anything meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16580311  0.28767123  0.36480687  0.28813559  0.03977273  0.38655462]\n",
      " [ 0.19689119  0.31506849  0.33476395  0.16610169  0.26136364  0.31092437]\n",
      " [ 0.20207254  0.32876712  0.08583691  0.29152542  0.47159091  0.11344538]\n",
      " [ 0.43523316  0.06849315  0.21459227  0.25423729  0.22727273  0.18907563]]\n",
      "Restored:\n",
      "['A', 'A', 'T', 'G', 'A', 'T', 'G', 'T', 'G', 'C', 'A', 'G', 'C', 'A', 'A', 'T', 'G', 'A', 'T', 'G']\n",
      "\n",
      "Original:\n",
      "TTTATCCTGCAGCTCGCCTG\n",
      "Now the real implementation:\n",
      "Restored:\n",
      "AAAAAGAAGAAGAAAGAAAA\n"
     ]
    }
   ],
   "source": [
    "learner = ConvRBM(6, 1)\n",
    "learner.initializePWMs()\n",
    "\n",
    "hiddenActivation = learner.forwardPass(allSeqs[246])\n",
    "restoredLength = hiddenActivation.shape[1] + learner.motifLength - 1\n",
    "reConv = np.zeros((len(learner.alphabet.letters), restoredLength))\n",
    "matrix = learner._convertPWM2Array(learner.motifs[0])\n",
    "print matrix\n",
    "for i in range(len(learner.alphabet.letters)):\n",
    "    reConv[i,:] = np.convolve(hiddenActivation[0], matrix[i])\n",
    "    \n",
    "#print reConv\n",
    "\n",
    "def softmaxActivation(col, idx):\n",
    "    p_all = np.sum(np.exp(col))\n",
    "    p_x = np.exp(col[idx])\n",
    "    return p_x / p_all\n",
    "\n",
    "visibleActivation = np.zeros((1, 150))\n",
    "for i in range(150):\n",
    "    visibleActivation[0,i] = np.argmax([softmaxActivation(reConv[:,i], x) for x in range(4)])\n",
    "    \n",
    "def convertNumericalToLetter(seq):\n",
    "    dna_seq = []\n",
    "    for num in range(seq.shape[1]):\n",
    "        dna_seq.append(learner._getLetterToInt(seq[0,num]))\n",
    "    return dna_seq\n",
    "\n",
    "print \"Restored:\"\n",
    "print convertNumericalToLetter(visibleActivation)[:20]\n",
    "print\n",
    "print \"Original:\"\n",
    "print allSeqs[246][:20]\n",
    "\n",
    "print \"Now the real implementation:\"\n",
    "print \"Restored:\"\n",
    "print learner.backwardPass(hiddenActivation)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the forward pass on the whole set of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0.0\n",
      "5000 -> 0.0\n",
      "10000 -> 0.0\n",
      "15000 -> 0.0\n",
      "20000 -> 0.0\n",
      "25000 -> 0.842827207671\n",
      "30000 -> 0.0\n",
      "35000 -> 0.0\n",
      "40000 -> 0.0\n",
      "45000 -> 0.0\n",
      "50000 -> 0.561771898779\n",
      "55000 -> 0.0\n",
      "60000 -> 0.0\n",
      "65000 -> 0.0\n",
      "70000 -> 0.0\n",
      "75000 -> 0.0\n",
      "80000 -> 0.58698077005\n",
      "85000 -> 0.0\n",
      "90000 -> 0.0\n",
      "95000 -> 0.0\n",
      "100000 -> 0.0\n",
      "105000 -> 0.0\n",
      "110000 -> 0.692016137551\n",
      "115000 -> 0.0\n",
      "120000 -> 0.0\n",
      "125000 -> 0.0\n",
      "130000 -> 0.0\n",
      "135000 -> 0.0\n",
      "140000 -> 0.0\n",
      "145000 -> 0.0\n",
      "150000 -> 0.0\n",
      "155000 -> 0.465888195173\n",
      "160000 -> 0.0\n",
      "165000 -> 0.0\n",
      "170000 -> 0.0\n",
      "\n",
      "\n",
      "Number of filters: 1\n",
      "Number of DHSs: 171275\n",
      "Average Length of Sequences: 150.0\n",
      "Execution Time: 62.1381750107\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 0\n",
    "lengthes = []\n",
    "someScores = []\n",
    "for seq in allSeqs:\n",
    "    convoluted = learner.forwardPass(seq)\n",
    "    lengthes.append(len(seq))\n",
    "    if i % 5000 == 0:\n",
    "        someScores.append(convoluted[0][random.randint(0, len(convoluted))])\n",
    "        print str(i) + \" -> \" + str(someScores[-1])\n",
    "    i += 1\n",
    "    \n",
    "print\n",
    "print\n",
    "print \"Number of filters: \" + str(learner.numMotifs)\n",
    "print \"Number of DHSs: \" + str(i)\n",
    "print \"Average Length of Sequences: \" + str(np.array(lengthes).mean())\n",
    "print \"Execution Time: \" + str(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test both passes on all sequences using parallelization (just CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE CPUs: 4\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5c582c38f323>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"AVAILABLE CPUs: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0msizePerCPU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallSeqs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0msublists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_repopulate_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         self._worker_handler = threading.Thread(\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36m_repopulate_pool\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Process'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PoolWorker'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'added worker'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0m_current_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/package/ddbtools/python2/2015-09-11/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'random'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "from multiprocessing.pool import Pool\n",
    "\n",
    "def calculatePassesForSeqs(seqs):\n",
    "    print \"Started thread with \" + str(len(seqs)) + \" Sequences!\"\n",
    "    lengthes = []\n",
    "    for seq in seqs:\n",
    "        hiddenActivation = learner.forwardPass(seq)\n",
    "        reconstruction = learner.backwardPass(hiddenActivation)\n",
    "        lengthes.append(len(seq))\n",
    "    return np.mean(lengthes)\n",
    "\n",
    "\n",
    "cpu_count = 4\n",
    "print \"AVAILABLE CPUs: \" + str(cpu_count)\n",
    "sizePerCPU = len(allSeqs) / cpu_count\n",
    "p = Pool(processes = cpu_count)\n",
    "sublists = []\n",
    "for i in range(cpu_count):\n",
    "    if not i == cpu_count-1:\n",
    "        sublists.append(allSeqs[i*sizePerCPU:(i+1)*sizePerCPU])\n",
    "    else:\n",
    "        sublists.append(allSeqs[i*sizePerCPU:])\n",
    "start = time.time()\n",
    "result = p.map(calculatePassesForSeqs, sublists)\n",
    "print result\n",
    "print\n",
    "print\n",
    "print \"Number of filters: \" + str(learner.numMotifs)\n",
    "print \"Number of DHSs: \" + str(len(allSeqs))\n",
    "print \"Execution Time: \" + str(time.time()-start)\n",
    "\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-482-3fd54ce79a77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests to learn how to do things with Biopython and Theano\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all DHS sequences have the same length by default?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 171275\n",
      "Number of seqs with length != 150: 0\n"
     ]
    }
   ],
   "source": [
    "fasta_seqs = sio.parse(open('../data/wgEncodeAwgDnaseUwAg10803UniPk.fa'), 'fasta')\n",
    "count = 0\n",
    "countNotSameLength = 0\n",
    "for seq in fasta_seqs:\n",
    "    if len(seq) != 150:\n",
    "        print 'not length 150'\n",
    "        countNotSameLength += 1\n",
    "    count += 1\n",
    "\n",
    "print 'Number of sequences: ' + str(count)\n",
    "print 'Number of seqs with length != 150: ' + str(countNotSameLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we generate random motif matrices (PWMs or PSSMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Bio.NeuralNetwork.Gene.Schema as schema\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import IUPAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTGCGAT\n",
      "CGTGACCA\n",
      "GTACACCGA\n"
     ]
    }
   ],
   "source": [
    "alphabet = IUPAC.unambiguous_dna\n",
    "generator = schema.RandomMotifGenerator(alphabet, 6, 10)\n",
    "for i in range(3):\n",
    "    x = generator.random_motif()\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet.letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Bio.motifs.matrix as mat\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createRandomMotif (motifLength, alphabet):\n",
    "    counts = {}\n",
    "    for letter in alphabet.letters:\n",
    "        counts[letter] = [random.randint(0,100) for x in xrange(motifLength)]\n",
    "    return mat.PositionWeightMatrix(alphabet, counts)\n",
    "#x = mat.PositionWeightMatrix(alphabet, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0      1      2      3      4      5      6      7      8      9\n",
      "A:   0.24   0.18   0.19   0.25   0.41   0.47   0.09   0.07   0.21   0.29\n",
      "C:   0.28   0.26   0.16   0.42   0.13   0.24   0.39   0.49   0.16   0.38\n",
      "G:   0.20   0.24   0.22   0.00   0.14   0.15   0.46   0.43   0.63   0.18\n",
      "T:   0.29   0.32   0.43   0.32   0.32   0.14   0.06   0.02   0.00   0.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = createRandomMotif(10, alphabet)\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.24,  0.18,  0.19,  0.25,  0.41,  0.47,  0.09,  0.07,  0.21,\n",
       "         0.29],\n",
       "       [ 0.28,  0.26,  0.16,  0.42,  0.13,  0.24,  0.39,  0.49,  0.16,\n",
       "         0.38],\n",
       "       [ 0.2 ,  0.24,  0.22,  0.  ,  0.14,  0.15,  0.46,  0.43,  0.63,\n",
       "         0.18],\n",
       "       [ 0.29,  0.32,  0.43,  0.32,  0.32,  0.14,  0.06,  0.02,  0.  ,\n",
       "         0.14]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((len(alphabet.letters), 10))\n",
    "\n",
    "def getLetterToInt (num):\n",
    "    if num == 0:\n",
    "        return 'A'\n",
    "    elif num == 1:\n",
    "        return 'C'\n",
    "    elif num == 2:\n",
    "        return 'G'\n",
    "    elif num == 3:\n",
    "        return 'T'\n",
    "    else:\n",
    "        print 'ERROR: Num ' + str(num) + \" not a valid char in DNA alphabet\"\n",
    "        return -1\n",
    "\n",
    "\n",
    "for letter in range(len(x)):\n",
    "    print letter\n",
    "    y[letter] = x[getLetterToInt(letter)]\n",
    "    letterCount += 1\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the elements of a PWM interpretable as probabilites?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 1.0\n",
      "1 -> 1.0\n",
      "2 -> 1.0\n",
      "3 -> 1.0\n",
      "4 -> 1.0\n",
      "5 -> 1.0\n",
      "6 -> 1.0\n",
      "7 -> 1.0\n",
      "8 -> 1.0\n",
      "9 -> 1.0\n"
     ]
    }
   ],
   "source": [
    "# verify that we're dealing with probabilities by summing up over all letters for each position\n",
    "for pos in range(x.length):\n",
    "    c = 0\n",
    "    for letter in alphabet.letters:\n",
    "        c += x[letter][pos]\n",
    "    print str(pos) + \" -> \" + str(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
